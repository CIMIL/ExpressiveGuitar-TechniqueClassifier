{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt, pickle, glob, re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_history_plots(resfolder, smooth_factor = None):\n",
    "    foldFolders = sorted(glob.glob(os.path.join(resfolder,'Fold*')))\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(10,5))\n",
    "    fig2, ax2 = plt.subplots(figsize=(10,5))\n",
    "    fig3, ax3 = plt.subplots(figsize=(10,5))\n",
    "    fig4, ax4 = plt.subplots(figsize=(10,5))\n",
    "    fig5, ax5 = plt.subplots(figsize=(10,5))\n",
    "    fig6, ax6 = plt.subplots(figsize=(10,5))\n",
    "\n",
    "\n",
    "    def smoothen(data, kernel_size):\n",
    "        if kernel_size is None:\n",
    "            return data\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "        data_convolved = np.convolve(data, kernel, mode='valid')\n",
    "        return data_convolved\n",
    "\n",
    "    linewidth = 2\n",
    "\n",
    "    for folder in foldFolders:\n",
    "        foldnumber = re.findall(r'Fold_(\\d+)', folder)\n",
    "        assert len(foldnumber) == 1\n",
    "        foldnumber = foldnumber[0]\n",
    "        with open(folder + '/history_fold_'+foldnumber+'.pickle', 'rb') as f:\n",
    "            history = pickle.load(f)\n",
    "\n",
    "        lbfolder = os.path.basename(folder)\n",
    "\n",
    "        toplot_ax1 = smoothen(history['val_loss'],smooth_factor)\n",
    "        ax1.plot(toplot_ax1, label=lbfolder, linewidth=linewidth)\n",
    "        ax1.set_title('Validation Loss')\n",
    "        ax1.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "        ax1.set_xlabel('Training Epoch')\n",
    "        fig1.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "        toplot_ax2 = smoothen(history['val_accuracy'],smooth_factor)\n",
    "        ax2.plot(toplot_ax2, label=lbfolder, linewidth=linewidth)\n",
    "        ax2.set_title('Validation Accuracy')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_xlabel('Training Epoch')\n",
    "        fig2.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "        toplot_ax3 = history['loss']\n",
    "        ax3.plot(toplot_ax3, label=lbfolder, linewidth=linewidth)\n",
    "        ax3.set_title('Training Loss')\n",
    "        ax3.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "        ax3.set_xlabel('Training Epoch')\n",
    "        fig3.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "        toplot_ax4 = history['accuracy']\n",
    "        ax4.plot(toplot_ax4, label=lbfolder, linewidth=linewidth)\n",
    "        ax4.set_title('Training Accuracy')\n",
    "        ax4.set_ylabel('Accuracy')\n",
    "        ax4.set_xlabel('Training Epoch')\n",
    "        fig4.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "        # Combined Trainig and Validation Loss\n",
    "        toplot_ax5 = smoothen(history['loss'],smooth_factor)\n",
    "        ax5.plot(toplot_ax5, label=lbfolder + ' Training', linewidth=linewidth)\n",
    "        toplot_ax5 = smoothen(history['val_loss'],smooth_factor)\n",
    "        ax5.plot(toplot_ax5, label=lbfolder + ' Validation', linewidth=linewidth)\n",
    "        ax5.set_title('Training and Validation Loss')\n",
    "        ax5.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "        ax5.set_xlabel('Training Epoch')\n",
    "        fig5.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "        # Combined Trainig and Validation Accuracy\n",
    "        toplot_ax6 = smoothen(history['accuracy'],smooth_factor)\n",
    "        ax6.plot(toplot_ax6, label=lbfolder + ' Training', linewidth=linewidth)\n",
    "        toplot_ax6 = smoothen(history['val_accuracy'],smooth_factor)\n",
    "        ax6.plot(toplot_ax6, label=lbfolder + ' Validation', linewidth=linewidth)\n",
    "        ax6.set_title('Training and Validation Accuracy')\n",
    "        ax6.set_ylabel('Accuracy')\n",
    "        ax6.set_xlabel('Training Epoch')\n",
    "        fig6.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "    fig1.savefig(os.path.join(resfolder,'Validation_Loss.png'))\n",
    "    fig2.savefig(os.path.join(resfolder,'Validation_Accuracy.png'))\n",
    "    fig3.savefig(os.path.join(resfolder,'Training_Loss.png'))\n",
    "    fig4.savefig(os.path.join(resfolder,'Training_Accuracy.png'))\n",
    "    fig5.savefig(os.path.join(resfolder,'Training_and_Validation_Loss.png'))\n",
    "    fig6.savefig(os.path.join(resfolder,'Training_and_Validation_Accuracy.png'))\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createscript(resfolder):\n",
    "    with open(os.path.join(resfolder,'make_history_plots.py'), 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "import numpy as np, matplotlib.pyplot as plt, pickle, glob, re\n",
    "\n",
    "foldFolders = sorted(glob.glob('Fold*'))\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(10,5))\n",
    "fig2, ax2 = plt.subplots(figsize=(10,5))\n",
    "fig3, ax3 = plt.subplots(figsize=(10,5))\n",
    "fig4, ax4 = plt.subplots(figsize=(10,5))\n",
    "fig5, ax5 = plt.subplots(figsize=(10,5))\n",
    "fig6, ax6 = plt.subplots(figsize=(10,5))\n",
    "\n",
    "def smoothen(data, kernel_size):\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "    data_convolved = np.convolve(data, kernel, mode='valid')\n",
    "    return data_convolved\n",
    "\n",
    "smooth_factor = 5\n",
    "linewidth = 2\n",
    "\n",
    "for folder in foldFolders:\n",
    "    foldnumber = re.findall(r'Fold_(\\d+)', folder)\n",
    "    assert len(foldnumber) == 1\n",
    "    foldnumber = foldnumber[0]\n",
    "    with open(folder + '/history_fold_'+foldnumber+'.pickle', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "\n",
    "    toplot_ax1 = smoothen(history['val_loss'],smooth_factor)\n",
    "    ax1.plot(toplot_ax1, label=folder, linewidth=linewidth)\n",
    "    ax1.set_title('Validation Loss')\n",
    "    ax1.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "    ax1.set_xlabel('Training Epoch')\n",
    "    fig1.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "    toplot_ax2 = smoothen(history['val_accuracy'],smooth_factor)\n",
    "    ax2.plot(toplot_ax2, label=folder, linewidth=linewidth)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_xlabel('Training Epoch')\n",
    "    fig2.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "    toplot_ax3 = history['loss']\n",
    "    ax3.plot(toplot_ax3, label=folder, linewidth=linewidth)\n",
    "    ax3.set_title('Training Loss')\n",
    "    ax3.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "    ax3.set_xlabel('Training Epoch')\n",
    "    fig3.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "    toplot_ax4 = history['accuracy']\n",
    "    ax4.plot(toplot_ax4, label=folder, linewidth=linewidth)\n",
    "    ax4.set_title('Training Accuracy')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.set_xlabel('Training Epoch')\n",
    "    fig4.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "    # Combined Trainig and Validation Loss\n",
    "    toplot_ax5 = smoothen(history['loss'],smooth_factor)\n",
    "    ax5.plot(toplot_ax5, label=folder + ' Training', linewidth=linewidth)\n",
    "    toplot_ax5 = smoothen(history['val_loss'],smooth_factor)\n",
    "    ax5.plot(toplot_ax5, label=folder + ' Validation', linewidth=linewidth)\n",
    "    ax5.set_title('Training and Validation Loss')\n",
    "    ax5.set_ylabel('Sparse Categorical Crossentropy Loss')\n",
    "    ax5.set_xlabel('Training Epoch')\n",
    "    fig5.legend(bbox_to_anchor=(0.3, 0.8), fancybox=True, shadow=True)\n",
    "\n",
    "    # Combined Trainig and Validation Accuracy\n",
    "    toplot_ax6 = smoothen(history['accuracy'],smooth_factor)\n",
    "    ax6.plot(toplot_ax6, label=folder + ' Training', linewidth=linewidth)\n",
    "    toplot_ax6 = smoothen(history['val_accuracy'],smooth_factor)\n",
    "    ax6.plot(toplot_ax6, label=folder + ' Validation', linewidth=linewidth)\n",
    "    ax6.set_title('Training and Validation Accuracy')\n",
    "    ax6.set_ylabel('Accuracy')\n",
    "    ax6.set_xlabel('Training Epoch')\n",
    "    fig6.legend(bbox_to_anchor=(0.3, 0.5), fancybox=True, shadow=True)\n",
    "\n",
    "\n",
    "fig1.savefig('Validation_Loss.png')\n",
    "fig2.savefig('Validation_Accuracy.png')\n",
    "fig3.savefig('Training_Loss.png')\n",
    "fig4.savefig('Training_Accuracy.png')\n",
    "fig5.savefig('Training_and_Validation_Loss.png')\n",
    "fig6.savefig('Training_and_Validation_Accuracy.png')\n",
    "plt.close('all')\n",
    "\"\"\")\n",
    "    return os.path.abspath(os.path.join(resfolder,'make_history_plots.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6064_acc_0.7914_CrossValidatedRun_20230221-234525/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6304_acc_0.8056_CrossValidatedRun_20230222-094853/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6322_acc_0.7950_CrossValidatedRun_20230222-084306/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6342_acc_0.7963_CrossValidatedRun_20230221-152643/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6458_acc_0.7917_CrossValidatedRun_20230221-145929/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6461_acc_0.8014_CrossValidatedRun_20230222-020252/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6487_acc_0.8064_CrossValidatedRun_20230222-001000/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6506_acc_0.7972_CrossValidatedRun_20230222-053239/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6543_acc_0.8017_CrossValidatedRun_20230222-043251/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6548_acc_0.8057_CrossValidatedRun_20230221-204301/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6548_acc_0.8111_CrossValidatedRun_20230222-003445/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6582_acc_0.8083_CrossValidatedRun_20230221-053330/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6600_acc_0.7990_CrossValidatedRun_20230221-183423/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6632_acc_0.8100_CrossValidatedRun_20230221-163701/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6632_acc_0.8134_CrossValidatedRun_20230221-155257/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6644_acc_0.8120_CrossValidatedRun_20230221-023506/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6650_acc_0.8015_CrossValidatedRun_20230222-011836/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6666_acc_0.8090_CrossValidatedRun_20230221-194815/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6755_acc_0.8164_CrossValidatedRun_20230221-172155/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6841_acc_0.8174_CrossValidatedRun_20230220-190308/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6850_acc_0.8128_CrossValidatedRun_20230222-031711/\n",
      "Resfolder: ./output/_3456windowed/full/_3456windowedfull_c_maf1_0.6871_acc_0.8138_CrossValidatedRun_20230220-224857/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plotscript = createscript('./')\n",
    "\n",
    "sizefolders = sorted(glob.glob('./output/_*'))\n",
    "for sizefolder in sizefolders:\n",
    "    winlen = re.findall(r'_(\\d+)windowed', os.path.basename(sizefolder))\n",
    "    assert len(winlen) == 1\n",
    "    winlen = winlen[0]\n",
    "    problemfolders = sorted(glob.glob(sizefolder + '/*'))\n",
    "    for problemfolder in problemfolders:\n",
    "        assert os.path.basename(problemfolder) in ['full','perc']\n",
    "        # print('Problemfolder: ' + problemfolder)\n",
    "        resfolders = sorted(glob.glob(problemfolder + '/_'+winlen+'*/'))\n",
    "        # print('Resfolder: ' + str(resfolders))\n",
    "        for resfolder in resfolders:\n",
    "            if glob.glob(resfolder + '/*.png') == []:\n",
    "                print('Resfolder: ' + resfolder)\n",
    "                make_history_plots(resfolder, smooth_factor = 3)\n",
    "                targetLinkPath = os.path.join(resfolder,'make_history_plots.py')\n",
    "                if os.path.exists(targetLinkPath):\n",
    "                    os.remove(targetLinkPath)\n",
    "                os.symlink(plotscript, targetLinkPath)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Also, extract dict with classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_from_history(folder, report_name):\n",
    "    assert os.path.exists(folder), 'Folder does not exist: ' + folder\n",
    "    infopath = os.path.join(folder,'info.txt')\n",
    "    assert os.path.exists(infopath), 'Info file does not exist: ' + infopath\n",
    "\n",
    "    with open(infopath, 'r') as f:\n",
    "        prevline = ''\n",
    "        lineread = f.readline()\n",
    "        found_report = False\n",
    "        reportlines = []\n",
    "\n",
    "        # print('Looking for %s in %s' % (report_name, infopath))\n",
    "        while not (found_report and lineread.strip() == '' and prevline.strip() == ''):\n",
    "            if report_name in lineread.strip():\n",
    "                found_report = True\n",
    "                # print('Found %s in %s' % (report_name, infopath))\n",
    "            elif found_report:\n",
    "                reportlines.append(lineread.strip())\n",
    "            prevline = lineread\n",
    "            lineread = f.readline()\n",
    "            if not lineread:\n",
    "                break\n",
    "        # assert reportlines != [], 'Could not find %s in %s' % (report_name, infopath)\n",
    "        if reportlines == [] or len(reportlines) < 3:\n",
    "            print('Could not find %s in %s' % (report_name, infopath))\n",
    "            return None\n",
    "\n",
    "        # Drop header\n",
    "        header = reportlines[0].split()\n",
    "        reportlines = reportlines[1:]\n",
    "        # Drop empty lines\n",
    "        reportlines = [line for line in reportlines if line != '']\n",
    "        # Split\n",
    "        reportlines = [[l for l in line.split('  ') if l != ''] for line in reportlines]\n",
    "\n",
    "        report_dict = {}\n",
    "        for line in reportlines:\n",
    "            # print('line:\"'+str(line)+'\"')\n",
    "            if line[0] == 'accuracy':\n",
    "                report_dict['accuracy'] = float(line[1])\n",
    "            else:\n",
    "                assert len(line) == len(header)+1, 'Line does not have the right number of entries: %i != %i +1'%(len(line),len(header))\n",
    "\n",
    "                report_dict[line[0]] = {t:float(l) for t,l in zip(header,line[1:])}\n",
    "\n",
    "        return report_dict\n",
    "        # print(reportlines)\n",
    "        # print(report_dict)\n",
    "    \n",
    "\n",
    "\n",
    "plotscript = createscript('./')\n",
    "\n",
    "sizefolders = sorted(glob.glob('./output/_*'))\n",
    "for sizefolder in sizefolders:\n",
    "    winlen = re.findall(r'_(\\d+)windowed', os.path.basename(sizefolder))\n",
    "    assert len(winlen) == 1\n",
    "    winlen = winlen[0]\n",
    "    problemfolders = sorted(glob.glob(sizefolder + '/*'))\n",
    "    for problemfolder in problemfolders:\n",
    "        assert os.path.basename(problemfolder) in ['full','perc']\n",
    "        # print('Problemfolder: ' + problemfolder)\n",
    "        resfolders = sorted(glob.glob(problemfolder + '/_'+winlen+'*/'))\n",
    "        # print('Resfolder: ' + str(resfolders))\n",
    "        for resfolder in resfolders:\n",
    "            # for eldsf in glob.glob(resfolder + '/extracted*.txt'):\n",
    "            #     os.remove(eldsf)\n",
    "            if glob.glob(resfolder + '/extracted*.txt') == []:\n",
    "                print('Resfolder: ' + resfolder)\n",
    "                for toextract in ['avg_classification_report', 'avg_classification_report_for_quantized_model', 'avg_classification_report_for_fullquantized_model']:\n",
    "                    report = extract_from_history(resfolder, toextract)\n",
    "                    if report:\n",
    "                        with open(os.path.join(resfolder,'extracted_'+toextract+'.txt'), 'w') as f:\n",
    "                            f.write(str(report))\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
