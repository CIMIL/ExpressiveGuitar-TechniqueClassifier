{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_ONLY_ON_FULL_DATASET = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2ha_JDqV9I"
      },
      "source": [
        "## Import modules and mount drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPrPhmjHk-kc"
      },
      "outputs": [],
      "source": [
        "# Choose ClassificationTask task\n",
        "from enum import Enum\n",
        "class ClassificationTask(Enum):\n",
        "    FULL_8_CLASS_PROBLEM,BINARY_PERCUSSIVE_PITCHED,PERCUSSIVE_4_ONLY,PITCHED_4_ONLY,PERCUSSIVE_PLUS_PITCHED_CLASS,ONE_GUITARIST_FULL = ((1,'full'), (2,'binary'), (3,'perc'), (4,'pitch'), (5,'perc+pitch'), (6,'one-guit-full'))\n",
        "class FeatureSelection(Enum):\n",
        "    NONE,MANUAL_VARIABLES,MANUAL_LIST,AUTO_ANOVA,AUTO_RELIEF = (0, 1, 2, 3, 4)\n",
        "class FeatureWindowSize(Enum):\n",
        "    s4800_SAMPLES_100ms, s704_Samples_14ms, _704windowed, _2112windowed, _3456windowed, _4800windowed = (1,2,3,4,5,6)\n",
        "\n",
        "class WindowedInputMode(Enum):\n",
        "    _1D, _2D = (1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_task = ClassificationTask.FULL_8_CLASS_PROBLEM\n",
        "# classification_task = ClassificationTask.BINARY_PERCUSSIVE_PITCHED\n",
        "# classification_task = ClassificationTask.PERCUSSIVE_4_ONLY\n",
        "# classification_task = ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS\n",
        "# classification_task = ClassificationTask.ONE_GUITARIST_FULL\n",
        "\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_VARIABLES\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_LIST\n",
        "FEATURE_SELECTION = FeatureSelection.AUTO_ANOVA #ANOVA: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
        "# FEATURE_SELECTION = FeatureSelection.AUTO_RELIEF\n",
        "\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize.s4800_SAMPLES_100ms\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize.s704_Samples_14ms\n",
        "FEATURE_WINDOW_SIZE = FeatureWindowSize._704windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._2112windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._3456windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._4800windowed\n",
        "\n",
        "# WINDOWED_INPUT_MODE = WindowedInputMode._1D\n",
        "WINDOWED_INPUT_MODE = WindowedInputMode._2D\n",
        "\n",
        "# WindowedInputMode._2D implies that 'windowed' is in FEATURE_WINDOW_SIZE.name\n",
        "assert (not (WINDOWED_INPUT_MODE == WindowedInputMode._2D)) or ('windowed' in FEATURE_WINDOW_SIZE.name), \"WindowedInputMode._2D implies that 'windowed' is in FEATURE_WINDOW_SIZE.name\"\n",
        "\n",
        "\n",
        "# DROP_ADDITIONAL_CEPSTRUM_FROM_BIG_WINDOW = True\n",
        "\n",
        "# SCALER_TO_USE = 'StandardScaler'\n",
        "SCALER_TO_USE = 'MinMaxScaler'\n",
        "\n",
        "#import sklearn scalers\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "if SCALER_TO_USE == 'StandardScaler':\n",
        "    SCALER_TO_USE = StandardScaler()\n",
        "elif SCALER_TO_USE == 'MinMaxScaler':\n",
        "    SCALER_TO_USE = MinMaxScaler()\n",
        "\n",
        "TRAIN_FINAL_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIRE_GPU = False\n",
        "DO_SAVE_TENSORBOARD_LOGS = False \n",
        "DO_SAVE_FOLD_MODELS = False \n",
        "CUSTOM_PLAYER_K_FOLD = True         # Very important, this ditches the k-fold stratified random shuffle, and creates as many splits as the guitar players, separating natural groups\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS = True # If true, drop the data from files that have 'extra' in the filename, which otherwise make the dataset unbalanced\n",
        "# --> Quantize (Dynamic) and test the TF Lite model obtained (quicker but lower accuracy)\n",
        "DO_TEST_QUANTIZATION = True\n",
        "\n",
        "USE_TENSORBOARD = True\n",
        "\n",
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    USE_AUGMENTED_DATA = True\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "    \n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "else:\n",
        "    raise ValueError('Invalid feature window size')\n",
        "# USE_AUGMENTED_DATA = False\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG = False\n",
        "\n",
        "DO_NORMALIZE_DATA = True\n",
        "\n",
        "DO_NORMALIZE_FOR_FEATURE_SELECTION = True\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if USE_TENSORBOARD:\n",
        "    %load_ext tensorboard\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "from sys import executable as sys_executable\n",
        "from sys import argv as sys_argv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import strftime, time\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from bz2 import BZ2File # To open compressed data\n",
        "import re\n",
        "import shutil\n",
        "import imblearn\n",
        "from sklearn.metrics import confusion_matrix as sk_conf_matrix\n",
        "from sklearn.metrics import classification_report as sk_class_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay as sk_conf_matrix_disp\n",
        "\n",
        "print(\"Tensorflow version: \" + tf.version.VERSION)\n",
        "print('Imblearn version:',imblearn.__version__)\n",
        "\n",
        "global_random_state = 43\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "seed_everything(global_random_state)\n",
        "\n",
        "COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if COLAB:\n",
        "    print('Running on CoLab')\n",
        "    #Connect and mount the drive folder that contains the train dataset and the output folder\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "    HOMEBASE = os.path.join('/content','gdrive','MyDrive','dottorato','Publications','02-IEEE-RTEmbeddedTimbreClassification(submitted)','Classifier')\n",
        "    THISDIR = \"/content/\"\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    HOMEBASE = \".\"\n",
        "    THISDIR = \"./\"\n",
        "DATAFOLDER = os.path.join(HOMEBASE,\"data/phase3\")\n",
        "MODELFOLDER = os.path.join(HOMEBASE,\"output\")\n",
        "\n",
        "RELIEF_CACHE_FILEPATH = os.path.join(DATAFOLDER,'relief_cache.pickle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY_34JvX6uFT"
      },
      "outputs": [],
      "source": [
        "def is_notebook() -> bool:\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7qyEuAk6uFV"
      },
      "source": [
        "## Enforce GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BxHBUDQPXKS"
      },
      "outputs": [],
      "source": [
        "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "print(physical_devices)\n",
        "if REQUIRE_GPU:\n",
        "  assert len(tf.config.experimental.list_physical_devices('GPU')) >= 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZMbpx2eM2G"
      },
      "source": [
        "## Check Real avaliable GRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lSUK12K6uFW"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def pip_install(package):\n",
        "    subprocess.check_call([executable, \"-m\", \"pip\", \"install\", package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_8hnGH7eL_T"
      },
      "outputs": [],
      "source": [
        "CHECK_GRAM = False\n",
        "\n",
        "if CHECK_GRAM:\n",
        "    # memory footprint support libraries/code\n",
        "    os.symlink('/opt/bin/nvidia-smi','/usr/bin/nvidia-smi')\n",
        "    pip_install('gputil')\n",
        "    pip_install('psutil')\n",
        "    pip_install('humanize')\n",
        "    import psutil\n",
        "    import humanize\n",
        "    import os\n",
        "    import GPUtil as GPU\n",
        "    GPUs = GPU.getGPUs()\n",
        "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "    gpu = GPUs[0]\n",
        "    def printm():\n",
        "        process = psutil.Process(os.getpid())\n",
        "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "    printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2hdmJnSsEOM"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CozyQAQgznvK"
      },
      "outputs": [],
      "source": [
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    DATASET_FILENAME = 'onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221206-165551_SUPERLONGdataset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    DATASET_FILENAME = 'onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221201-182312_REALTIMEdataset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-141803_windowed704-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-163834_windowed2112-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230120-081628_windowed3456-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-105343_windowed4800-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "else:\n",
        "    raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "\n",
        "print('Loading dataset from file:',DATASET_FILENAME)\n",
        "\n",
        "\n",
        "if os.path.splitext(DATASET_FILENAME)[1] == '.bz2':\n",
        "    print(\"Reading dataset from compressed pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,DATASET_FILENAME)\n",
        "    startime = time()\n",
        "    ifile = BZ2File(DATASET_PATH,'rb')\n",
        "    featuredataset = pickle.load(ifile)\n",
        "    ifile.close()\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from compressed pickle' % (time()-startime))\n",
        "elif os.path.splitext(DATASET_FILENAME)[1] == '.pickle':\n",
        "    print(\"Reading dataset from pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,DATASET_FILENAME)\n",
        "    startime = time()\n",
        "    with open(DATASET_PATH,'rb') as pf:\n",
        "        featuredataset = pickle.load(pf)\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from regular pickle' % (time()-startime))\n",
        "else:\n",
        "    raise Exception(\"Extension %s not supported!\" % os.path.splitext(DATASET_FILENAME)[1])\n",
        "print('Dataset loaded!')\n",
        "# display(featuredataset)\n",
        "DATA_IS_WINDOWED = featuredataset.columns.str.match('0_').any()\n",
        "WINDOW_INDEXES = sorted(list(set([int(e.split('_')[0]) for e in featuredataset.columns[featuredataset.columns.str.match('\\d+_')].to_list()])))\n",
        "print('Data is WINDOWED!' if DATA_IS_WINDOWED else '', '%d windows' % (len(WINDOW_INDEXES)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "augmented_featuredataset_list = []\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_data_paths = glob(os.path.join(DATAFOLDER,'augmented_data','*.pickle'))\n",
        "    for augmented_data_path in augmented_data_paths:\n",
        "        print(\"Loading file %s\" % os.path.basename(augmented_data_path))\n",
        "        with open(augmented_data_path,'rb') as pf:\n",
        "            augmented_featuredataset_list.append(pickle.load(pf))\n",
        "    augmented_featuredataset = pd.concat(augmented_featuredataset_list, ignore_index=True)\n",
        "    print(\"Loaded %d augmented samples\" % len(augmented_featuredataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1rKcNy5RqZ"
      },
      "source": [
        "### Drop features that we have found to be problematic with feature selection and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhip8r7F4sSv"
      },
      "outputs": [],
      "source": [
        "def drop_unused_features_old(features_df: pd.DataFrame, is_windowed = False, inplace = False) -> pd.DataFrame:\n",
        "    if not inplace:\n",
        "        res_df = features_df.copy()\n",
        "        print('Copied',flush=True)\n",
        "    else:\n",
        "        res_df = features_df\n",
        "        print('Not Copied',flush=True)\n",
        "\n",
        "\n",
        "    if is_windowed:\n",
        "        for window_index in WINDOW_INDEXES:\n",
        "            print('Dropping bad columns for index %d'%(window_index),flush=True)\n",
        "\n",
        "            if '%s_attackTime_peaksamp'%window_index       not in res_df.columns.to_list() or\\\n",
        "               '%s_attackTime_attackStartIdx'%window_index not in res_df.columns.to_list() or\\\n",
        "               '%s_peakSample_index'%window_index          not in res_df.columns.to_list():\n",
        "                # raise Exception(\"The features dataframe does not contain the required columns!\")\n",
        "                # Show warning instead of exception\n",
        "                print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\"%('%s_attackTime_peaksamp'%window_index,'%s_attackTime_attackStartIdx'%window_index,'%s_peakSample_index'%window_index))\n",
        "            else:\n",
        "                res_df.drop(columns=['%s_attackTime_peaksamp' % window_index,\\\n",
        "                                        '%s_attackTime_attackStartIdx' % window_index,\\\n",
        "                                        '%s_peakSample_index' % window_index], inplace=True)\n",
        "    else:\n",
        "        if 'attackTime_peaksamp'       not in res_df.columns.to_list() or\\\n",
        "        'attackTime_attackStartIdx' not in res_df.columns.to_list() or\\\n",
        "        'peakSample_index'          not in res_df.columns.to_list():\n",
        "            # raise Exception(\"The features dataframe does not contain the required columns!\")\n",
        "            print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\" % ('attackTime_peaksamp' in res_df.columns.to_list(), 'attackTime_attackStartIdx' in res_df.columns.to_list(), 'peakSample_index' in res_df.columns.to_list()))\n",
        "        else:\n",
        "            res_df.drop(columns=['attackTime_peaksamp',\\\n",
        "                                    'attackTime_attackStartIdx',\\\n",
        "                                    'peakSample_index'], inplace=True)\n",
        "    return res_df\n",
        "\n",
        "def drop_unused_features(features_df: pd.DataFrame, is_windowed = False, inplace = False) -> pd.DataFrame:\n",
        "    if not inplace:\n",
        "        res_df = features_df.copy()\n",
        "        print('Copied',flush=True)\n",
        "    else:\n",
        "        res_df = features_df\n",
        "        print('Not Copied',flush=True)\n",
        "\n",
        "    todrop = []\n",
        "    if is_windowed:\n",
        "        for window_index in WINDOW_INDEXES:\n",
        "            if '%s_attackTime_peaksamp'%window_index not in res_df.columns.to_list() or '%s_attackTime_attackStartIdx'%window_index not in res_df.columns.to_list() or '%s_peakSample_index'%window_index not in res_df.columns.to_list():\n",
        "                print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\"%('%s_attackTime_peaksamp'%window_index,'%s_attackTime_attackStartIdx'%window_index,'%s_peakSample_index'%window_index))\n",
        "            else:\n",
        "                todrop.extend(['%s_attackTime_peaksamp' % window_index,'%s_attackTime_attackStartIdx' % window_index,'%s_peakSample_index' % window_index])\n",
        "    else:\n",
        "        if 'attackTime_peaksamp' not in res_df.columns.to_list() or 'attackTime_attackStartIdx' not in res_df.columns.to_list() or 'peakSample_index' not in res_df.columns.to_list():\n",
        "            print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\" % ('attackTime_peaksamp' in res_df.columns.to_list(), 'attackTime_attackStartIdx' in res_df.columns.to_list(), 'peakSample_index' in res_df.columns.to_list()))\n",
        "        else:\n",
        "            todrop.extend(['attackTime_peaksamp','attackTime_attackStartIdx','peakSample_index'])\n",
        "\n",
        "    res_df.drop(columns=todrop, inplace=True)\n",
        "    return res_df\n",
        "\n",
        "\n",
        "#measure time\n",
        "startime = time()\n",
        "featuredataset = drop_unused_features(featuredataset, is_windowed = DATA_IS_WINDOWED)\n",
        "print('It took %.1fs to drop unused features' % (time()-startime))\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_featuredataset = drop_unused_features(augmented_featuredataset, is_windowed = DATA_IS_WINDOWED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if DROP_ADDITIONAL_CEPSTRUM_FROM_BIG_WINDOW:\n",
        "\n",
        "#     # Largest cepstrum now:\n",
        "#     larg_ceps = max([int(e.split('_')[-1]) for e in featuredataset.columns.to_list() if 'cepstrum' in e])\n",
        "#     # Max xepstrum coeff. of smallest window (704 samples)\n",
        "#     smallest_ceps = 704//2+1\n",
        "\n",
        "#     featuredataset.drop(columns=[f'cepstrum_{v}' for v in range(smallest_ceps+1,larg_ceps + 1)], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xh-UknklQy"
      },
      "source": [
        "### If specified, drop extra percussive recorded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvUB_btbklQy"
      },
      "outputs": [],
      "source": [
        "# if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "#     assert featuredataset.shape == (EXPECTED_DATASED_SIZE, 504)\n",
        "if DROP_EXTRA_PERCUSSIVE_SOUNDS:\n",
        "    to_drop_count = np.count_nonzero(featuredataset.meta_audiofilePath.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count))\n",
        "        featuredataset = featuredataset[~featuredataset.meta_audiofilePath.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(featuredataset.shape)))\n",
        "    # if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    #     assert featuredataset.shape == (EXPECTED_DATASED_SIZE-2237, 504)\n",
        "\n",
        "\n",
        "if USE_AUGMENTED_DATA and DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG:\n",
        "    augmented_featuredataset_dr = augmented_featuredataset.copy()\n",
        "    to_drop_count_aug = np.count_nonzero(augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count_aug >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count_aug))\n",
        "        augmented_featuredataset_dr = augmented_featuredataset[~augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(augmented_featuredataset_dr.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HN56MAc5kjA"
      },
      "outputs": [],
      "source": [
        "# Extract separate DFs\n",
        "from typing import Tuple\n",
        "\n",
        "# Divide dataset into metadata, features and labels\n",
        "def divide_dataset(features_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    # print time of eacch operation / line\n",
        "    # startime = time()\n",
        "    metadata = features_df.filter(regex='^meta_',axis=1)\n",
        "    # print('It took %.1fs to extract metadata' % (time()-startime))\n",
        "    # startime = time()\n",
        "    labels = features_df.meta_expressive_technique_id\n",
        "    # print('It took %.1fs to extract labels' % (time()-startime))\n",
        "    # startime = time()\n",
        "    features = features_df.loc[:,[col for col in features_df.columns if col not in metadata.columns]]\n",
        "    # print('It took %.1fs to extract features' % (time()-startime))\n",
        "    # Convert to numeric formats where possible (somehow convert_dtypes doesn't work [https://stackoverflow.com/questions/65915048/pandas-convert-dtypes-not-working-on-numbers-marked-as-objects])\n",
        "    # startime = time()\n",
        "    metadata = metadata.apply(pd.to_numeric, errors='ignore')\n",
        "    labels = labels.apply(pd.to_numeric, errors='ignore')\n",
        "    features = features.apply(pd.to_numeric, errors='ignore')\n",
        "    # print('It took %.1fs to convert to numeric types' % (time()-startime))\n",
        "    return metadata, features, labels\n",
        "\n",
        "metadata, features, labels = divide_dataset(featuredataset)\n",
        "assert metadata.shape[1] == 9\n",
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    assert features.shape[1] == 495\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    assert features.shape[1] == 2543\n",
        "\n",
        "if USE_AUGMENTED_DATA:\n",
        "    metadata_aug, features_aug, labels_aug = divide_dataset(augmented_featuredataset_dr if DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG else augmented_featuredataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iiyTkcYKFi"
      },
      "outputs": [],
      "source": [
        "def get_classes_description(classftask: ClassificationTask):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        classes_desk = {0:\"Kick\",1:\"Snare 1\",2:\"Tom\",3:\"Snare 2\",4:\"Natural Harmonics\",5:\"Palm Mute\",6:\"Pick Near Bridge\",7:\"Pick Over the Soundhole\"}\n",
        "        shortnames = {0:\"Kick\",1:\"Snare1\",2:\"Tom\",3:\"Snare2\",4:\"NatHarm\",5:\"PalmMute\",6:\"BridgeP\",7:\"SoundholeP\"}\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        classes_desk = {0:\"Percussive\",1:\"Pitched\"}\n",
        "        shortnames = {0:\"Perc\",1:\"Pitch\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\"}\n",
        "        shortnames = {0:\"Kick\",1:\"Snare1\",2:\"Tom\",3:\"Snare2\"}\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        classes_desk = {0:\"Natural Harmonics\", 1:\"Palm Mute\", 2:\"Pick Near Bridge\", 3:\"Pick Over the Soundhole\"}\n",
        "        shortnames = {0:\"NatHarm\",1:\"PalmMute\",2:\"BridgeP\",3:\"SoundholeP\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\", 4:\"Pitched\"}\n",
        "        shortnames = {0:\"Kick\",1:\"Snare1\",2:\"Tom\",3:\"Snare2\",4:\"NatHarm\",5:\"Pitch\"}\n",
        "    elif classification_task == ClassificationTask.ONE_GUITARIST_FULL:\n",
        "        classes_desk = {0:\"Kick\",1:\"Snare 1\",2:\"Tom\",3:\"Snare 2\",4:\"Natural Harmonics\",5:\"Palm Mute\",6:\"Pick Near Bridge\",7:\"Pick Over the Soundhole\"}\n",
        "        shortnames = {0:\"Kick\",1:\"Snare1\",2:\"Tom\",3:\"Snare2\",4:\"NatHarm\",5:\"PalmMute\",6:\"BridgeP\",7:\"SoundholeP\"}\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "    classes = list(classes_desk.keys())\n",
        "    return classes,classes_desk,shortnames\n",
        "\n",
        "def filter_dataset(tofilt_features,tofilt_labels,tofilt_metadata,classftask: ClassificationTask, hardcoded_sizes_test = False):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        pass\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        tofilt_labels = tofilt_labels.replace([0,1,2,3],[0,0,0,0])\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[1,1,1,1])\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        filtered_idxs = tofilt_labels < 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 1620\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        filtered_idxs = tofilt_labels >= 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[0,1,2,3])\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237-1620\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        tofilt_labels = tofilt_labels.replace([5,6,7],[4,4,4])\n",
        "    elif classification_task == ClassificationTask.ONE_GUITARIST_FULL:\n",
        "\n",
        "        filtered_idxs = tofilt_features['']\n",
        "        # tofilt_features = tofilt_features[filtered_idxs]\n",
        "        # tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        # tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        # assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == 1620\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "\n",
        "\n",
        "    tofilt_features.reset_index(drop=True,inplace=True)\n",
        "    tofilt_labels.reset_index(drop=True,inplace=True)\n",
        "    tofilt_metadata.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    return tofilt_features, tofilt_labels, tofilt_metadata\n",
        "\n",
        "original_dataset_features = features.copy()\n",
        "dataset_labels = labels.copy()\n",
        "dataset_metadata = metadata.copy()\n",
        "\n",
        "CLASSES,CLASSES_DESC,SHORTDESC = get_classes_description(classification_task)\n",
        "original_dataset_features,dataset_labels,dataset_metadata = filter_dataset(original_dataset_features,dataset_labels,dataset_metadata,classification_task,hardcoded_sizes_test=True if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms else False)\n",
        "if USE_AUGMENTED_DATA:\n",
        "    features_aug,labels_aug,metadata_aug = filter_dataset(features_aug,labels_aug,metadata_aug,classification_task,hardcoded_sizes_test=False)\n",
        "    assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(labels_aug))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(labels_aug))).all()\n",
        "\n",
        "assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(dataset_labels))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(dataset_labels))).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_JEEbL-mcJ-"
      },
      "outputs": [],
      "source": [
        "toprint = [(original_dataset_features,'Main dataset')]\n",
        "if USE_AUGMENTED_DATA:\n",
        "    toprint.append((features_aug,'Augmented data'))\n",
        "\n",
        "for dat,name in toprint:\n",
        "    print('Dataset \"'+name+'\" read')\n",
        "    print(\"├╴Entries: \"+str(dat.shape[0]))\n",
        "    if DATA_IS_WINDOWED:\n",
        "        print('├╴Features per window: '+str(dat.shape[1]//len(WINDOW_INDEXES)))\n",
        "        print(\"└╴Windows: \"+str(len(WINDOW_INDEXES)))\n",
        "    else:\n",
        "        print('├╴Features: '+str(dat.shape[1]))\n",
        "\n",
        "original_feature_number = original_dataset_features.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2gv9fFFavMz"
      },
      "outputs": [],
      "source": [
        "# Compute has of the dataset files.\n",
        "# This are used to cache precomputed feature selection with ReliefF (Which is rather slow)\n",
        "import hashlib\n",
        " \n",
        "dataset_sha256_hash = hashlib.sha256()\n",
        "with open(DATASET_PATH,\"rb\") as fy:\n",
        "    for byte_block in iter(lambda: fy.read(4096),b\"\"):    # Read and update hash string value in blocks of 4K\n",
        "        dataset_sha256_hash.update(byte_block)\n",
        "dataset_sha256_hash = dataset_sha256_hash.hexdigest()\n",
        "\n",
        "print(dataset_sha256_hash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ND_A4d6uFT"
      },
      "source": [
        "## Parse Command Line arguments\n",
        "\n",
        "*_Important_*: If you are running this from a jupyter Notebook, change the run parameters at the end of the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVHOAmQ-6uFU"
      },
      "outputs": [],
      "source": [
        "args = None\n",
        "if not is_notebook() and not COLAB:\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Train the expressive guitar technique classifier.')\n",
        "\n",
        "    def featnum_type(x):\n",
        "        (MIN,MAX) = (1,495)\n",
        "        if x == 'all':\n",
        "            return x\n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Feature parameter must either 'all' or a number be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netdepth_type(x):\n",
        "        (MIN,MAX) = (0,20) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network depth must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netwidth_type(x):\n",
        "        (MIN,MAX) = (1,2000) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network width must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def dropout_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Dropout Rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def aggressiveness_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Oversampling aggressiveness value must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def lr_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Learning rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def batchsize_type(x):\n",
        "        (MIN,MAX) = (1,4096) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Batchsize must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def epochs_type(x):\n",
        "        (MIN,MAX) = (1,10000) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Epoch number must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def kfold_type(x):\n",
        "        (MIN,MAX) = (1,20) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"KFOLD size must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def c1d_type(x):\n",
        "        (MIN,MAX) = (0,5) \n",
        "        x = int(x)\n",
        "        assert int(x) == float(x), \"Parameter must be an integer\"\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Number of conv layers must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "\n",
        "    parser.add_argument('-f',  '--features',      default='all',     type=featnum_type,   help='Number of features to use for training [1-495] (default: 80)')\n",
        "    parser.add_argument('-d',  '--net-depth',     default=3,      type=netdepth_type,  help='Number of dense layers in the network [0-20] (default: 3)')\n",
        "    parser.add_argument('-w',  '--net-width',     default=100,    type=netwidth_type,  help='Number of layers in the FFNN [1-2000] (default: 100)')\n",
        "    parser.add_argument('-dr', '--dropout',       default=0.15,   type=dropout_type,   help='Dropout amount [0-1] (default: 0.15)')\n",
        "    parser.add_argument('-lr', '--learning-rate', default=0.0001, type=lr_type,        help='Learning rate [0-1] (default: 0.0001)')\n",
        "    parser.add_argument('-bs', '--batchsize',     default=256,    type=batchsize_type, help='Learning rate [1-4096] (default: 256)')\n",
        "    parser.add_argument('-e',  '--epochs',        default=1000,   type=epochs_type,    help='Learning rate [1-10000] (default: 1000)')\n",
        "    parser.add_argument('-k',  '--k-folds',       default=5,      type=kfold_type,     help='K of K-folds [1-20] (default: 5)')\n",
        "    parser.add_argument('-osagg', '--oversampling-aggressiveness',  default=0.2,   type=aggressiveness_type,   help='Oversampling aggressiveness [0-1] (default: 0.2)')\n",
        "    parser.add_argument('-c1d',   '--conv',     default=0,      type=c1d_type,     help='Number of conv1D layers at the beginning [1-5] (default: 0)')\n",
        "    parser.add_argument('-ck',    '--conv-kernels',     default='',      type=ascii,     help='Comma-separated list of kernel sizes for conv1D layers (es: 3,5,7)')\n",
        "    parser.add_argument('-cs',    '--conv-strides',     default='',      type=ascii,     help='Comma-separated list of strides for conv1D layers (es: 1,1,1)')\n",
        "    parser.add_argument('-cf',    '--conv-filters',     default='',      type=ascii,     help='Comma-separated list of filters for conv1D layers (es: 32,64,128)')\n",
        "    parser.add_argument('-cact','--conv-activations', default='',  type=ascii,     help='Comma-separated list of activations for conv1D layers (es: relu,relu,relu)')\n",
        "    parser.add_argument('-cp','--conv-padding', default='',  type=ascii,     help='Comma-separated list of padding method layers. Use \"same\" or \"valid\" (es: same,same,same)')\n",
        "    parser.add_argument('-pl','--pool-layers', default='',  type=ascii,     help='Comma-separated list of pool layers. Use \"N\" for none, \"M\" for max-pooling and \"A\" for average pooling  (es: M,N,M)')\n",
        "    parser.add_argument('-v', '--verbose',        action='store_true', help='increase output verbosity')\n",
        "    args = parser.parse_args()\n",
        "    args = vars(args)\n",
        "else:\n",
        "\n",
        "    \"\"\"\n",
        "    ████████    ████████████████████████████████████████████████████████████████████████████    ████████\n",
        "    ██╔══════╗    ╔══════════════════════════════════════════════════════════════════════════╗    ╔═══██══╗\n",
        "    ██║ ╔════╝    ╚══════════════════════════════════════════════════════════════════════════╝    ╚═══██╗ ║\n",
        "    ██║ ║     ██████╗  █████╗ ██████╗  █████╗ ███╗   ███╗███████╗████████╗███████╗██████╗ ███████╗    ██║ ║\n",
        "      ║ ║     ██╔══██╗██╔══██╗██╔══██╗██╔══██╗████╗ ████║██╔════╝╚══██╔══╝██╔════╝██╔══██╗██╔════╝      ║ ║\n",
        "      ╚═╝     ██████╔╝███████║██████╔╝███████║██╔████╔██║█████╗     ██║   █████╗  ██████╔╝███████╗      ╚═╝\n",
        "              ██╔═══╝ ██╔══██║██╔══██╗██╔══██║██║╚██╔╝██║██╔══╝     ██║   ██╔══╝  ██╔══██╗╚════██║         \n",
        "    ██        ██║     ██║  ██║██║  ██║██║  ██║██║ ╚═╝ ██║███████╗   ██║   ███████╗██║  ██║███████║    ██   \n",
        "    ██╔═╗     ╚═╝     ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚══════╝   ╚═╝   ╚══════╝╚═╝  ╚═╝╚══════╝    ██╔═╗\n",
        "    ██║ ║                                                                                             ██║ ║\n",
        "    ████████    ████████████████████████████████████████████████████████████████████████████    ████████║ ║\n",
        "      ║ ╚════╗    ╔══════════════════════════════════════════════════════════════════════════╗    ╔═════╝ ║\n",
        "      ╚══════╝    ╚══════════════════════════════════════════════════════════════════════════╝    ╚═══════╝\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "    \"\"\" +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    #   |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #   + ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ +                                 #\n",
        "    \"\"\" #↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓#\n",
        "    args = {'features':      200, \n",
        "            'net_depth':     2, \n",
        "            'net_width':     32, \n",
        "            'dropout':       0.5,\n",
        "            'learning_rate': 0.00008,\n",
        "            'batchsize':     512,\n",
        "            'epochs':        10,\n",
        "            'k_folds':       5,\n",
        "            'oversampling_aggressiveness':  1.0,\n",
        "            'conv':          1,\n",
        "            'conv_kernels': '5',\n",
        "            'conv_strides': '1',\n",
        "            'conv_filters': '4',\n",
        "            'conv_activations': 'relu',\n",
        "            'conv_padding': 'same',\n",
        "            'pool_layers': 'M',\n",
        "            'verbose':       False}\n",
        "    #↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑#\n",
        "    \"\"\" + ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ +                                 #\n",
        "    #   |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #   +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    \"\"\"#----------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "args['conv_kernels'] = args['conv_kernels'].strip(\"'\")\n",
        "args['conv_filters'] = args['conv_filters'].strip(\"'\")\n",
        "args['conv_activations'] = args['conv_activations'].strip(\"'\")\n",
        "args['conv_strides'] = args['conv_strides'].strip(\"'\")\n",
        "args['conv_padding'] = args['conv_padding'].strip(\"'\")\n",
        "args['pool_layers'] = args['pool_layers'].strip(\"'\")\n",
        "\n",
        "KERNEL_SIZES = [int(x) for x in args['conv_kernels'].split(',')] if args['conv'] > 1 else [int(args['conv_kernels'])] if args['conv'] == 1 else []\n",
        "print('KERNEL_SIZES: ', KERNEL_SIZES)\n",
        "FILTERS = [int(x) for x in args['conv_filters'].split(',')] if args['conv'] > 1 else [int(args['conv_filters'])] if args['conv'] == 1 else []\n",
        "print('FILTERS: ', FILTERS)\n",
        "CONV_ACTIVATIONS = args['conv_activations'].split(',')    if args['conv'] > 1 else [args['conv_activations']] if args['conv'] == 1 else []\n",
        "CONV_ACTIVATIONS = [e if e.lower() != 'none' else None for e in CONV_ACTIVATIONS]\n",
        "print('CONV_ACTIVATIONS: ', CONV_ACTIVATIONS)\n",
        "STRIDES = [int(x) for x in args['conv_strides'].split(',')] if args['conv'] > 1 else [int(args['conv_strides'])] if args['conv'] == 1 else []\n",
        "print('STRIDES: ', STRIDES)\n",
        "PADDING = args['conv_padding'].split(',') if args['conv'] > 1 else [args['conv_padding']] if args['conv'] == 1 else []\n",
        "print('PADDING: ', PADDING)\n",
        "POOL_LAYERS = args['pool_layers'].split(',') if args['conv'] > 1 else [args['pool_layers']] if args['conv'] == 1 else []\n",
        "#Pooling layers must be one of 'M', 'N', or 'A' \n",
        "assert all([e in ['M','N','A'] for e in POOL_LAYERS]), \"Pooling layers must be one of 'M', 'N', or 'A'\"\n",
        "print('POOL_LAYERS: ', POOL_LAYERS)\n",
        "\n",
        "POOL_SIZES = [2]*args['conv']\n",
        "\n",
        "assert len(KERNEL_SIZES)     == args['conv'], \"The number of kernel sizes must be equal to the number of conv layers ({} != {})\".format(len(KERNEL_SIZES), args['conv'])\n",
        "assert len(FILTERS)          == args['conv'], \"The number of filters must be equal to the number of conv layers ({} != {})\".format(len(FILTERS), args['conv'])\n",
        "assert len(CONV_ACTIVATIONS) == args['conv'], \"The number of activations must be equal to the number of conv layers ({} != {})\".format(len(CONV_ACTIVATIONS), args['conv'])\n",
        "assert len(STRIDES)          == args['conv'], \"The number of strides must be equal to the number of conv layers ({} != {})\".format(len(STRIDES), args['conv'])\n",
        "assert len(PADDING)          == args['conv'], \"The number of padding arguments must be equal to the number of conv layers ({} != {})\".format(len(PADDING), args['conv'])\n",
        "assert len(POOL_LAYERS)      == args['conv'], \"The number of pool layers must be equal to the number of conv layers ({} != {})\".format(len(POOL_LAYERS), args['conv'])\n",
        "# raise Exception('STOP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "# call garbage collector to free up memory\n",
        "gc.collect()\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjF3Cif5zr1p"
      },
      "source": [
        "## Subset features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--hCfGHKZK98"
      },
      "outputs": [],
      "source": [
        "def get_manual_selected_features(data,prefix = ''):\n",
        "    print (\"Subsetting features...\")\n",
        "    columns_to_keep = []\n",
        "    # if USE_ATTACKTIME_PEAKSAMP:\n",
        "    #     columns_to_keep.append(\"attackTime_peaksamp\")\n",
        "    # if USE_ATTACKTIME_ATTACKSTARTIDX:\n",
        "    #     columns_to_keep.append(\"attackTime_attackStartIdx\")\n",
        "    if USE_ATTACKTIME_VALUE:\n",
        "        columns_to_keep.append(prefix+\"attackTime_value\")\n",
        "    if USE_BARKSPECBRIGHTNESS:\n",
        "        columns_to_keep.append(prefix+\"barkSpecBrightness\")\n",
        "    if USE_PEAKSAMPLE_VALUE:\n",
        "        columns_to_keep.append(prefix+\"peakSample_value\")\n",
        "    # if USE_PEAKSAMPLE_INDEX:\n",
        "    #     columns_to_keep.append(\"peakSample_index\")\n",
        "    if USE_ZEROCROSSING:\n",
        "        columns_to_keep.append(prefix+\"zeroCrossing\")\n",
        "\n",
        "    assert USE_BARKSPEC <= 50 and USE_BARKSPEC >= 0 and USE_BFCC <= 49 and USE_BFCC >= 0 and USE_CEPSTRUM <= 353 and USE_CEPSTRUM >= 0 and USE_MFCC <= 37 and USE_MFCC >= 0\n",
        "\n",
        "    if USE_BARKSPEC > 0:\n",
        "        columns_to_keep += [prefix+'barkSpec_'+str(i+1) for i in range(USE_BARKSPEC)]\n",
        "    if USE_BFCC > 0:\n",
        "        columns_to_keep += [prefix+'bfcc_'+str(i+2) for i in range(USE_BFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "    if USE_CEPSTRUM > 0:\n",
        "        columns_to_keep += [prefix+'cepstrum_'+str(i+1) for i in range(USE_CEPSTRUM)]\n",
        "    if USE_MFCC > 0:\n",
        "        columns_to_keep += [prefix+'mfcc_'+str(i+2) for i in range(USE_MFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "\n",
        "    return columns_to_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agcUWSWVbokS"
      },
      "outputs": [],
      "source": [
        "# # To Compeltely reset RelieFF cache\n",
        "# with open(RELIEF_CACHE_FILEPATH, 'wb') as rcf:\n",
        "#     pickle.dump(set(), rcf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBDXJV0dnx2W"
      },
      "outputs": [],
      "source": [
        "# how_many_examples_per_class =10\n",
        "# subselection = list(range(0,how_many_examples_per_class))+\\\n",
        "#                list(range(600,600+how_many_examples_per_class))+\\\n",
        "#                list(range(1100,1100+how_many_examples_per_class))+\\\n",
        "#                list(range(1400,1400+how_many_examples_per_class))+\\\n",
        "#                list(range(1900,1900+how_many_examples_per_class))+\\\n",
        "#                list(range(3000,3000+how_many_examples_per_class))+\\\n",
        "#                list(range(9000,9000+how_many_examples_per_class))+\\\n",
        "#                list(range(14000,14000+how_many_examples_per_class))\n",
        "\n",
        "# testprova_dataset_features = original_dataset_features.iloc[subselection]\n",
        "# testprova_dataset_labels = dataset_labels.iloc[subselection]\n",
        "import os, platform, subprocess, re\n",
        "\n",
        "def get_processor_name():\n",
        "    if platform.system() == \"Windows\":\n",
        "        return platform.processor()\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n",
        "        command =\"sysctl -n machdep.cpu.brand_string\"\n",
        "        return subprocess.check_output(command).strip()\n",
        "    elif platform.system() == \"Linux\":\n",
        "        command = \"cat /proc/cpuinfo\"\n",
        "        all_info = subprocess.check_output(command, shell=True).decode().strip()\n",
        "        for line in all_info.split(\"\\n\"):\n",
        "            if \"model name\" in line:\n",
        "                return re.sub( \".*model name.*:\", \"\", line,1)\n",
        "    return \"\"\n",
        "\n",
        "class ReliefCacheElem(dict):\n",
        "\n",
        "    PRINT_HASH = False\n",
        "\n",
        "    def __init__(self,dataset_sha256,n_neighbors,relieff_top_features,relieff_feature_importances,time_of_computation):\n",
        "        self.dataset_sha256 = dataset_sha256\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.relieff_top_features = relieff_top_features\n",
        "        self.relieff_feature_importances = relieff_feature_importances\n",
        "        self.date = strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
        "\n",
        "        self.cpu_info = get_processor_name()\n",
        "        self.time_of_computation = time_of_computation\n",
        "\n",
        "    def __key(self):\n",
        "        return tuple([self.dataset_sha256,\n",
        "                     self.n_neighbors,\n",
        "                     str(self.relieff_top_features),\n",
        "                     str(self.relieff_feature_importances)])\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.__key())\n",
        "\n",
        "    def __str__(self):\n",
        "        res = '{date: '+self.date+', n_neighbors:'+str(self.n_neighbors)\n",
        "        \n",
        "        if self.PRINT_HASH:\n",
        "            res += 'dataset_sha256:'+str(self.dataset_sha256)+','\n",
        "\n",
        "        res += 'cpu_info:'+str(self.cpu_info)+','\n",
        "        res += 'time_of_computation:'+str(self.time_of_computation)+','\n",
        "        res += '}'\n",
        "        return res\n",
        "\n",
        "def relieff_selection(X:list,y:list,n_features,n_neighbors,relief_cache_filepath,verbose_ = True):\n",
        "    relief_data_X = X\n",
        "    relief_data_y = y\n",
        "    # First check if result is already cached\n",
        "    ## Load Cache\n",
        "    relief_cache = None\n",
        "\n",
        "    ##----------------------------------------------##\n",
        "    if not os.path.exists(relief_cache_filepath):\n",
        "        raise Exception(\"RELIEF CACHE NOT FOUND at '\"+relief_cache_filepath+\"'! Comment exception to create empty cache\")\n",
        "        with open(relief_cache_filepath, 'wb') as rcf:\n",
        "            pickle.dump(set(), rcf)\n",
        "    ##----------------------------------------------##\n",
        "\n",
        "    with open(relief_cache_filepath,'rb') as rcf:\n",
        "        relief_cache = pickle.load(rcf)\n",
        "        if verbose_: \n",
        "            print('Loaded Relief cache ('+str(len(relief_cache))+' solutions)')\n",
        "    # Check if present\n",
        "    for cache_elem in relief_cache:\n",
        "        if cache_elem.dataset_sha256 == dataset_sha256_hash and\\\n",
        "           cache_elem.n_neighbors == n_neighbors:\n",
        "            if verbose_:\n",
        "                print(\"Result found in cache!\")\n",
        "            return cache_elem.relieff_top_features[:n_features]\n",
        "    \n",
        "    # If not present, compute\n",
        "    if verbose_:\n",
        "        print(\"Result NOT found in cache, computing now... (might take a long while)\")\n",
        "    \n",
        "    from skrebate import ReliefF\n",
        "    r = ReliefF(n_neighbors=n_neighbors,verbose=verbose_)\n",
        "    \n",
        "    start_fit = time()\n",
        "    r.fit(X=relief_data_X,y=relief_data_y)\n",
        "    top_features = r.top_features_\n",
        "    feature_importances = r.feature_importances_\n",
        "    stop_fit = time()\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done. Now storing in cache...\")\n",
        "\n",
        "    savedata = ReliefCacheElem(\n",
        "        dataset_sha256 = dataset_sha256_hash,\n",
        "        n_neighbors = n_neighbors,\n",
        "        relieff_top_features = top_features,\n",
        "        relieff_feature_importances = feature_importances,\n",
        "        time_of_computation = stop_fit - start_fit)\n",
        "    relief_cache.add(savedata)\n",
        "    with open(relief_cache_filepath, 'wb') as rcf:\n",
        "        pickle.dump(relief_cache, rcf)\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done.\")\n",
        "\n",
        "    return top_features[:n_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oas8vnnMQ0uP"
      },
      "outputs": [],
      "source": [
        "if FEATURE_SELECTION == FeatureSelection.AUTO_RELIEF:\n",
        "    with open(RELIEF_CACHE_FILEPATH,'rb') as rcf:\n",
        "        relief_cache = pickle.load(rcf)\n",
        "        print(len(relief_cache),'cached relief runs:')\n",
        "\n",
        "        if len(relief_cache) != 0:\n",
        "            samedataset = [e for e in relief_cache if e.dataset_sha256 == dataset_sha256_hash]\n",
        "            print('('+str(len(samedataset))+'/'+str(len(relief_cache)), 'are from the same dataset)')\n",
        "            if len(samedataset) != len(relief_cache):\n",
        "                raise Exception('Some of the cached results are from a different dataset!')\n",
        "            for i,e in enumerate(relief_cache):\n",
        "                print(i,':',e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkiDFjjztbM"
      },
      "outputs": [],
      "source": [
        "if args['features'] == 'all':\n",
        "    FEATURE_SELECTION = FeatureSelection.NONE\n",
        "    AUTO_FEATURE_NUMBER = len(original_dataset_features.columns)\n",
        "    if DATA_IS_WINDOWED:\n",
        "        AUTO_FEATURE_NUMBER //= len(WINDOW_INDEXES)\n",
        "    print('AUTO_FEATURE_NUMBER:',AUTO_FEATURE_NUMBER)\n",
        "else:\n",
        "    AUTO_FEATURE_NUMBER = args['features']    # If FEATURE_SELECTION is AUTO_ANOVA or AUTO_RELIEF, select this number of features automatically\n",
        "\n",
        "\n",
        "assert AUTO_FEATURE_NUMBER > 0, 'Number of features must be > 0'\n",
        "assert AUTO_FEATURE_NUMBER <= len(original_dataset_features.columns), 'Number of features is bigger than the number of columns in dataset ({} > {})'.format(AUTO_FEATURE_NUMBER,len(original_dataset_features.columns))\n",
        "\n",
        "if FEATURE_SELECTION != FeatureSelection.NONE and DO_NORMALIZE_FOR_FEATURE_SELECTION:\n",
        "    print('Normalizing data for feature selection...')\n",
        "    feature_dataset_for_selection = original_dataset_features.to_numpy()\n",
        "    feature_dataset_for_selection = SCALER_TO_USE.fit_transform(feature_dataset_for_selection)\n",
        "    feature_dataset_for_selection_D = pd.DataFrame(columns=original_dataset_features.columns, data=feature_dataset_for_selection)\n",
        "    # print('example row' + str(feature_dataset_for_selection[0]))\n",
        "    (relief_data_X,relief_data_y) = (feature_dataset_for_selection,dataset_labels.values.ravel())\n",
        "    print('Done.')\n",
        "else:\n",
        "    feature_dataset_for_selection_D = original_dataset_features\n",
        "    feature_dataset_for_selection = original_dataset_features.to_numpy()\n",
        "    (relief_data_X,relief_data_y) = (original_dataset_features.values,dataset_labels.values.ravel())\n",
        "\n",
        "\n",
        "selected_features = []\n",
        "if FEATURE_SELECTION == FeatureSelection.NONE:\n",
        "    selected_features = original_dataset_features.columns\n",
        "elif FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES:\n",
        "    ''' Features '''\n",
        "    USE_ATTACKTIME_VALUE = True\n",
        "    USE_BARKSPECBRIGHTNESS = True\n",
        "    USE_PEAKSAMPLE_VALUE = True\n",
        "    USE_ZEROCROSSING = False\n",
        "\n",
        "    USE_BARKSPEC = 40 # Number in range [0-50]\n",
        "    USE_BFCC = 40     # Number in range [0-50]\n",
        "    USE_CEPSTRUM = 60 # Number in range [0-353]\n",
        "    USE_MFCC = 30     # Number in range [0-38]\n",
        "\n",
        "    if DATA_IS_WINDOWED:\n",
        "        for index in WINDOW_INDEXES:\n",
        "            selected_features.extend(get_manual_selected_features(original_dataset_features,prefix=index+'_'))\n",
        "    else:\n",
        "        selected_features = get_manual_selected_features(original_dataset_features)\n",
        "elif FEATURE_SELECTION == FeatureSelection.MANUAL_LIST:\n",
        "    selected_features = ['attackTime_value', 'barkSpecBrightness', 'barkSpec_1', 'barkSpec_2', 'barkSpec_3', 'barkSpec_4', 'barkSpec_5', 'barkSpec_6', 'barkSpec_7', 'barkSpec_8', 'barkSpec_9', 'barkSpec_10', 'barkSpec_11', 'barkSpec_12', 'barkSpec_13', 'barkSpec_14', 'barkSpec_15', 'barkSpec_16', 'barkSpec_17', 'barkSpec_18', 'barkSpec_19', 'barkSpec_20', 'barkSpec_21', 'barkSpec_22', 'barkSpec_23', 'barkSpec_24', 'barkSpec_25', 'barkSpec_26', 'barkSpec_27', 'barkSpec_28', 'barkSpec_29', 'barkSpec_30', 'barkSpec_31', 'barkSpec_32', 'barkSpec_33', 'barkSpec_34', 'barkSpec_35', 'barkSpec_36', 'barkSpec_37', 'barkSpec_38', 'barkSpec_39', 'barkSpec_40', 'barkSpec_41', 'barkSpec_42', 'barkSpec_43', 'barkSpec_44', 'barkSpec_45', 'barkSpec_46', 'barkSpec_47', 'barkSpec_48', 'barkSpec_49', 'barkSpec_50', 'bfcc_2', 'bfcc_3', 'bfcc_4', 'bfcc_5', 'bfcc_6', 'bfcc_7', 'bfcc_8', 'bfcc_9', 'bfcc_10', 'bfcc_11', 'bfcc_12', 'bfcc_13', 'bfcc_15', 'bfcc_16', 'bfcc_17', 'bfcc_18', 'bfcc_19', 'bfcc_20', 'bfcc_21', 'bfcc_25', 'bfcc_26', 'bfcc_27', 'bfcc_28', 'bfcc_29', 'bfcc_30', 'bfcc_31', 'bfcc_35', 'bfcc_36', 'bfcc_37', 'bfcc_39', 'bfcc_40', 'bfcc_42', 'bfcc_43', 'bfcc_44', 'bfcc_45', 'bfcc_46', 'bfcc_48', 'cepstrum_1', 'cepstrum_2', 'cepstrum_3', 'cepstrum_4', 'cepstrum_5', 'cepstrum_6', 'cepstrum_7', 'cepstrum_8', 'cepstrum_9', 'cepstrum_10', 'cepstrum_11', 'cepstrum_12', 'cepstrum_13', 'cepstrum_14', 'cepstrum_15', 'cepstrum_16', 'cepstrum_17', 'cepstrum_18', 'cepstrum_19', 'cepstrum_20', 'cepstrum_21', 'cepstrum_22', 'cepstrum_23', 'cepstrum_24', 'cepstrum_25', 'cepstrum_26', 'cepstrum_27', 'cepstrum_28', 'cepstrum_29', 'cepstrum_30', 'cepstrum_31', 'cepstrum_32', 'cepstrum_33', 'cepstrum_34', 'cepstrum_35', 'cepstrum_36', 'cepstrum_37', 'cepstrum_41', 'cepstrum_42', 'cepstrum_43', 'cepstrum_44', 'cepstrum_45', 'cepstrum_46', 'cepstrum_47', 'cepstrum_48', 'cepstrum_49', 'cepstrum_54', 'cepstrum_56', 'cepstrum_59', 'cepstrum_60', 'cepstrum_67', 'cepstrum_72', 'cepstrum_86', 'cepstrum_87', 'cepstrum_108', 'cepstrum_164', 'cepstrum_205', 'cepstrum_206', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13', 'mfcc_14', 'mfcc_15', 'mfcc_16', 'mfcc_17', 'mfcc_18', 'mfcc_19', 'mfcc_20', 'mfcc_21', 'mfcc_22', 'mfcc_23', 'mfcc_24', 'mfcc_25', 'mfcc_26', 'mfcc_32', 'mfcc_33', 'mfcc_34', 'mfcc_35', 'mfcc_36', 'peakSample_value', 'zeroCrossing']\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_ANOVA:\n",
        "    if original_dataset_features.shape[1] != original_feature_number:\n",
        "        raise ValueError(\"ERROR: please import dataset again since you are trying to subset an already processed feature set (\"+str(dataset_features.shape[1])+\"<\"+str(original_feature_number)+\")\")\n",
        "\n",
        "    # ANOVA feature selection for numeric input and categorical output (https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/#:~:text=Feature%20selection%20is%20the%20process,the%20performance%20of%20the%20model)\n",
        "    from sklearn.feature_selection import SelectKBest\n",
        "    from sklearn.feature_selection import f_classif\n",
        "    \n",
        "    if not DATA_IS_WINDOWED:\n",
        "        fs = SelectKBest(score_func=f_classif, k=AUTO_FEATURE_NUMBER) # Define feature selection\n",
        "        X_selected = fs.fit_transform(feature_dataset_for_selection, dataset_labels.to_numpy().ravel())                         # Apply feature selection\n",
        "        support = fs.get_support(indices=True)                      # Extract selected indexes\n",
        "        selected_features = original_dataset_features.columns[support].tolist()\n",
        "        print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "    else:\n",
        "        # For windowed data we select the best features for each window, rank the most used across all the windows and select the AUTO_FEATURE_NUMBER best ones\n",
        "        bestfeats_total_count = {}\n",
        "        for wi in WINDOW_INDEXES:\n",
        "            print(\"Selecting best features for window %d\"%wi)\n",
        "            fs = SelectKBest(score_func=f_classif, k=AUTO_FEATURE_NUMBER)\n",
        "            currentwindowdata = feature_dataset_for_selection_D[feature_dataset_for_selection_D.columns[feature_dataset_for_selection_D.columns.str.contains('%d_'%wi)]]\n",
        "            X_selected = fs.fit_transform(currentwindowdata.to_numpy(), dataset_labels.to_numpy().ravel())                         # Apply feature selection\n",
        "            current_window_best_features = currentwindowdata.columns[fs.get_support(indices=True) ].tolist()\n",
        "            # Increase occurences of best features but drop window index prefix\n",
        "            for f in current_window_best_features:\n",
        "                f = '_'.join(f.split('_')[1:])\n",
        "                if f in bestfeats_total_count:\n",
        "                    bestfeats_total_count[f] += 1\n",
        "                else:\n",
        "                    bestfeats_total_count[f] = 1\n",
        "        # sort best features by occurences\n",
        "        bestfeats_total_count = [k for k, v in sorted(bestfeats_total_count.items(), key=lambda item: item[1],reverse=True)]\n",
        "        #take first AUTO_FEATURE_NUMBER\n",
        "        bestfeats_total_count = bestfeats_total_count[:AUTO_FEATURE_NUMBER]\n",
        "        # add window index prefix and expand to all indexes\n",
        "        selected_features = []\n",
        "        for wi in WINDOW_INDEXES:\n",
        "            selected_features.extend([str(wi)+'_'+str(f) for f in bestfeats_total_count])\n",
        "        print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_RELIEF:\n",
        "    support = relieff_selection(relief_data_X,relief_data_y,AUTO_FEATURE_NUMBER,n_neighbors=5,relief_cache_filepath=RELIEF_CACHE_FILEPATH,verbose_= True)\n",
        "    selected_features = original_dataset_features.columns[support].tolist()\n",
        "    print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "else:\n",
        "    raise Exception(\"ERROR! This type of feature selection is not supported\")\n",
        "\n",
        "dataset_features = original_dataset_features.copy().loc[:,selected_features]\n",
        "# if USE_AUGMENTED_DATA:\n",
        "#     features_aug = features_aug.copy().loc[:,selected_features]\n",
        "\n",
        "if FEATURE_SELECTION == FeatureSelection.NONE:\n",
        "    print(\"No feature selection applied\")\n",
        "    assert AUTO_FEATURE_NUMBER * len(WINDOW_INDEXES) == dataset_features.shape[1], \"ERROR: the number of features selected is not correct (%d * %d != %d)\"%(AUTO_FEATURE_NUMBER,len(WINDOW_INDEXES),dataset_features.shape[1])\n",
        "    print('%d features are kept'%(AUTO_FEATURE_NUMBER),('for each of the %d windows'%len(WINDOW_INDEXES) if DATA_IS_WINDOWED else ''))\n",
        "else:\n",
        "    print(\"Features reduced \"+('manually' if (FEATURE_SELECTION == FeatureSelection.MANUAL_LIST or FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES) else 'automatically')+\" (\"+str(FEATURE_SELECTION)+\") from \"+str(original_feature_number)+\" to : \"+str(dataset_features.shape[1]))\n",
        "    print('%d features for each of the %d windows'%(AUTO_FEATURE_NUMBER,len(set([e.split('_')[0] for e in original_dataset_features.columns[original_dataset_features.columns.str.match('\\d_')].to_list()]))) if DATA_IS_WINDOWED else '')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset_features.shape[1]\n",
        "# len(WINDOW_INDEXES)\n",
        "# AUTO_FEATURE_NUMBER\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if WINDOWED_INPUT_MODE == WindowedInputMode._2D:\n",
        "#     if not DATA_IS_WINDOWED:\n",
        "#         raise Exception(\"ERROR! Cannot convert 1D input to 2D input if data is not windowed!\")\n",
        "#     # display(dataset_features.head())\n",
        "#     display(dataset_features.to_numpy().reshape((dataset_features.shape[0],len(WINDOW_INDEXES),AUTO_FEATURE_NUMBER)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # If specified, convert the 1d arrays of consequent feature vectors to 2d arrays of shape (n_windows, n_features)\n",
        "# if WINDOWED_INPUT_MODE == WindowedInputMode._2D:\n",
        "#     if not DATA_IS_WINDOWED:\n",
        "#         raise Exception(\"ERROR! Cannot convert 1D input to 2D input if data is not windowed!\")\n",
        "#     dataset_features = dataset_features.to_numpy().reshape((dataset_features.shape[0],len(WINDOW_INDEXES),AUTO_FEATURE_NUMBER))\n",
        "#     dataset_labels = dataset_labels.to_numpy().reshape((dataset_labels.shape[0],len(WINDOW_INDEXES),1))\n",
        "#     if USE_AUGMENTED_DATA:\n",
        "#         features_aug = features_aug.to_numpy().reshape((features_aug.shape[0],len(WINDOW_INDEXES),AUTO_FEATURE_NUMBER))\n",
        "#         labels_aug = labels_aug.to_numpy().reshape((labels_aug.shape[0],len(WINDOW_INDEXES),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxb5oNdswky3"
      },
      "source": [
        "## Evaluate class support\n",
        "(What percentage of dataset entries represent each class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOwPsK_h5r3q"
      },
      "outputs": [],
      "source": [
        "DO_PRINT_SUPPORT = False\n",
        "def printSupport (labels_ds):\n",
        "    binc = np.bincount(np.reshape(labels_ds,labels_ds.size))\n",
        "    for i in range(binc.size):\n",
        "        print(\"Class \" + str(i) + \" support: \" + str(\"{:.2f}\".format(binc[i]/sum(binc) * 100)) + \"%\")\n",
        "        \n",
        "if DO_PRINT_SUPPORT:\n",
        "    printSupport(dataset_labels.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w196aHu6YqnH"
      },
      "source": [
        "# Define model architecture,\n",
        "Loss, optimizer and compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Be14IPJTg_4"
      },
      "outputs": [],
      "source": [
        "def define_model_architecture(num_classes:int, _verbose = False):\n",
        "    tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "    sequential_structure = []\n",
        "    if WINDOWED_INPUT_MODE == WindowedInputMode._2D:\n",
        "        convLayer = tf.keras.layers.Conv2D\n",
        "        convLayer_type = 'conv2d'\n",
        "        maxpoolLayer = tf.keras.layers.MaxPooling2D\n",
        "        maxpoolLayer_type = 'maxpool'\n",
        "        avgpoolLayer = tf.keras.layers.AveragePooling2D\n",
        "        avgpoolLayer_type = 'avgpool'\n",
        "        input_shape = (len(WINDOW_INDEXES), AUTO_FEATURE_NUMBER, 1)\n",
        "    else:\n",
        "        convLayer = tf.keras.layers.Conv1D\n",
        "        convLayer_type = 'conv'\n",
        "        maxpoolLayer = tf.keras.layers.MaxPooling1D\n",
        "        maxpoolLayer_type = 'maxpool'\n",
        "        avgpoolLayer = tf.keras.layers.AveragePooling1D\n",
        "        avgpoolLayer_type = 'avgpool'\n",
        "        input_shape = (AUTO_FEATURE_NUMBER, 1)\n",
        "\n",
        "    count_for_layertype = {}\n",
        "    def getAndIncreaseOrAdd(_dict,_key):\n",
        "        if _key in _dict:\n",
        "            orig = _dict[_key]\n",
        "            _dict[_key] += 1\n",
        "            return orig\n",
        "        else:\n",
        "            _dict[_key] = 1\n",
        "            return 0\n",
        "\n",
        "    def getAndCount(layername):\n",
        "        return getAndIncreaseOrAdd(count_for_layertype,layername)\n",
        "\n",
        "    def getname(layername):\n",
        "        return '%s_%d'%(layername,getAndCount(layername))\n",
        "\n",
        "\n",
        "    if args['conv'] > 0:\n",
        "        for i in range(args['conv']):\n",
        "            if i == 0:\n",
        "                sequential_structure.append(convLayer(filters=FILTERS[i], kernel_size=KERNEL_SIZES[i], strides=STRIDES[i], padding=PADDING[i], activation=CONV_ACTIVATIONS[i], input_shape=input_shape,name=getname('conv') ))\n",
        "            else:\n",
        "                sequential_structure.append(convLayer(filters=FILTERS[i], kernel_size=KERNEL_SIZES[i], strides=STRIDES[i], padding=PADDING[i], activation=CONV_ACTIVATIONS[i],name=getname('conv')))\n",
        "\n",
        "            sequential_structure += [tf.keras.layers.BatchNormalization(name=getname('batchnorm'))]\n",
        "\n",
        "\n",
        "            if POOL_LAYERS[i] != 'N':\n",
        "                pname = getname(POOL_LAYERS[i]+'pool')\n",
        "                if POOL_LAYERS[i] == 'M':\n",
        "                    sequential_structure += [maxpoolLayer(pool_size=POOL_SIZES[i],name=pname)]\n",
        "                elif POOL_LAYERS[i] == 'A':\n",
        "                    sequential_structure += [avgpoolLayer(pool_size=POOL_SIZES[i],name=pname)]\n",
        "\n",
        "        sequential_structure += [tf.keras.layers.Flatten(name=getname('flatten')),\n",
        "                                 tf.keras.layers.Dropout(args['dropout'],name=getname('dropout'))]\n",
        "\n",
        "    for i in range(args['net_depth']):\n",
        "        sequential_structure += [tf.keras.layers.Dense(args['net_width'],activation='relu',\n",
        "                                                       kernel_initializer='he_uniform',\n",
        "                                                       name=getname('dense')),    #   X   |           |         |\n",
        "                                 tf.keras.layers.BatchNormalization(name=getname('batchnorm')),                      #       |     X     |         |\n",
        "                                 tf.keras.layers.Dropout(args['dropout'],name=getname('dropout'))                   #       |           |    X    |\n",
        "                                ]\n",
        "\n",
        "    sequential_structure += [tf.keras.layers.Dense(num_classes,name='OUT_'+getname('dense'))]               \n",
        "\n",
        "    model = tf.keras.models.Sequential(sequential_structure)\n",
        "\n",
        "    model._name = \"guitar_timbre_classifier_\" + strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_loss():\n",
        "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "PREVIEW_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os.path\n",
        "# import tempfile\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.python.keras import Model, Sequential\n",
        "\n",
        "\n",
        "# def count_flops(model):\n",
        "#     \"\"\" Count flops of a keras model\n",
        "#     # Args.\n",
        "#         model: Model,\n",
        "#     # Returns\n",
        "#         int, FLOPs of a model\n",
        "#     # Raises\n",
        "#         TypeError, if a model is not an instance of Sequence or Model\n",
        "#     \"\"\"\n",
        "\n",
        "#     if not isinstance(model, (Sequential, Model)):\n",
        "#         raise TypeError(\n",
        "#             'Model is expected to be an instance of Sequential or Model, '\n",
        "#             'but got %s' % type(model))\n",
        "\n",
        "#     output_op_names = [_out_tensor.op.name for _out_tensor in model.outputs]\n",
        "#     tf.compat.v1.keras.backend.get_session()\n",
        "#     frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "#         sess, sess.graph.as_graph_def(), output_op_names)\n",
        "\n",
        "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
        "#         graph_file = os.path.join(os.path.join(tmpdir, 'graph.pb'))\n",
        "#         with tf.gfile.GFile(graph_file, \"wb\") as f:\n",
        "#             f.write(frozen_graph_def.SerializeToString())\n",
        "\n",
        "#         with tf.gfile.GFile(graph_file, \"rb\") as f:\n",
        "#             graph_def = tf.GraphDef()\n",
        "#             graph_def.ParseFromString(f.read())\n",
        "\n",
        "#         with tf.Graph().as_default() as new_graph:\n",
        "#             tf.import_graph_def(graph_def, name='')\n",
        "#             tfprof_opts = tf.profiler.ProfileOptionBuilder.float_operation()\n",
        "#             flops = tf.profiler.profile(new_graph, options=tfprof_opts)\n",
        "#             writer = tf.summary.FileWriter('gg', graph=new_graph)\n",
        "#             writer.flush()\n",
        "\n",
        "#     return flops\n",
        "\n",
        "# testopsmodel = define_model_architecture(len(CLASSES))\n",
        "# print(count_flops(testopsmodel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testopsmodel = define_model_architecture(len(CLASSES))\n",
        "# flops = tf.compat.v1.profiler.profile(testopsmodel,\\\n",
        "#      options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n",
        "# print('FLOP = ', flops.total_float_ops)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kOMPQBTXVXb"
      },
      "outputs": [],
      "source": [
        "def compile_model(model,optimizer,loss_fn,_verbose = False):\n",
        "    opt = None\n",
        "    if optimizer[\"method\"] == \"sgd\":\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate = optimizer[\"learning_rate\"], momentum=optimizer[\"momentum\"])\n",
        "    elif optimizer[\"method\"] == \"adam\":\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = optimizer[\"learning_rate\"])\n",
        "    else:\n",
        "        raise Exception(\"Optimizer method not supported\")\n",
        "\n",
        "    def recall(y_true, y_pred, c):\n",
        "        y_true = K.flatten(y_true)\n",
        "        pred_c = K.cast(K.equal(K.argmax(y_pred, axis=-1), c), K.floatx())\n",
        "        true_c = K.cast(K.equal(y_true, c), K.floatx())\n",
        "        true_positives = K.sum(pred_c * true_c)\n",
        "        possible_postives = K.sum(true_c)\n",
        "        return true_positives / (possible_postives + K.epsilon())\n",
        "\n",
        "\n",
        "    def precision(y_true, y_pred, c):\n",
        "        y_true = K.flatten(y_true)\n",
        "        pred_c = K.cast(K.equal(K.argmax(y_pred, axis=-1), c), K.floatx())\n",
        "        true_c = K.cast(K.equal(y_true, c), K.floatx())\n",
        "        true_positives = K.sum(pred_c * true_c)\n",
        "        pred_positives = K.sum(pred_c)\n",
        "        return true_positives / (pred_positives + K.epsilon())\n",
        "\n",
        "    def recall_class(theclass:int):\n",
        "        funk = lambda y_true, y_pred: recall(y_true, y_pred, theclass)\n",
        "        funk.__name__ = 'recall_c' + str(theclass)\n",
        "        return funk\n",
        "\n",
        "    # model.compile(optimizer=opt,\n",
        "    #               loss=loss_fn,\n",
        "    #               metrics=['accuracy',\n",
        "    #                         recall_class(0),\n",
        "    #                         recall_class(1),\n",
        "    #                         recall_class(2),\n",
        "    #                         recall_class(3),\n",
        "    #                         recall_class(4),\n",
        "    #                         recall_class(5),\n",
        "    #                         recall_class(6),\n",
        "    #                         recall_class(7)])\n",
        "\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss_fn,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #----------------------------------------------------------#\n",
        "    # Why did we not add here a custom metric for F1-SCORE?    #\n",
        "    # In particular, we wanted ** Macro Average - F1-score **  #\n",
        "    # which is the (non-weighted) average of the F1-score for  #\n",
        "    # each class.                                              #\n",
        "    # Thus is a relevnt metric for our problem, since we have  #\n",
        "    # class imbalance.                                         #\n",
        "    #                                                          #\n",
        "    # The issue seems to be the way that tensorflow handles    #\n",
        "    # metric computation.                                      #\n",
        "    # The metric is computed for each batch, and then the      #\n",
        "    # average is computed.                                     #\n",
        "    # Also, custom metrics seems to require the use of tensor  #\n",
        "    # operations, and converting to numpy arrays does not seem #\n",
        "    # to be supported FOR COMPUTATIONS                         #\n",
        "    # https://stackoverflow.com/a/52659570                     #\n",
        "    # https://stackoverflow.com/a/52659570/10930862            #\n",
        "    #----------------------------------------------------------#\n",
        "    \n",
        "    if _verbose:\n",
        "        print(\"Model compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc66NhrugHsG"
      },
      "source": [
        "# Save Models and Info functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vbu_AA6dhUU"
      },
      "outputs": [],
      "source": [
        "def save_model_info(model,optimizer,final_cross_validation_results,folds,metrics,outpath, fold_zerobased = None, smote_strategy = None):\n",
        "    info_filename = '/info.txt' if fold_zerobased is None else '/info_fold_'+str(fold_zerobased+1)+'.txt'\n",
        "    assert not (final_cross_validation_results and (fold_zerobased is not None))\n",
        "\n",
        "    with open(outpath + info_filename, \"w\") as f:\n",
        "        if not is_notebook():\n",
        "            f.write('Execution command:\\n')\n",
        "            f.write(\" \".join(sys_argv[:])+'\\n')\n",
        "        else:\n",
        "            f.write('Trained with the jupyter notebook (not the script version)\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if DO_OVERSAMPLING:\n",
        "            f.write(\"Oversampling (SMOTE) with aggressiveness: \"+str(OVERSAMPLING_AGGRESSIVENESS)+'\\n')\n",
        "            if smote_strategy is not None:\n",
        "                f.write(\"SMOTE strategy: \"+str(smote_strategy)+'\\n')\n",
        "            else:\n",
        "                f.write(\"SMOTE strategy: \"+str(SMOTE_STRATEGY)+')\\n')\n",
        "        else:\n",
        "            f.write('NOT performing Oversampling'+'\\n')\n",
        "\n",
        "        if USE_AUGMENTED_DATA:\n",
        "            f.write('Using augmented data'+'\\n')\n",
        "            f.write('Augmented audio file paths: ' + ','.join([os.path.basename(x) for x in augmented_data_paths])+'\\n')\n",
        "            f.write('Loaded '+str(len(augmented_featuredataset))+' augmented samples'+'\\n')\n",
        "            f.write('Used '+str(len(features_aug))+' augmented samples after filtering the dataset'+'\\n')\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('Augmented data used in this fold: '+str(len(train_aug_indexes[fold_zerobased]))+'\\n')\n",
        "            else:\n",
        "                f.write('Augmented data used for all splits: \\n')\n",
        "                for split_idx in train_aug_indexes:\n",
        "                    f.write(str(len(train_aug_indexes[split_idx]))+'\\n')\n",
        "\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if fold_zerobased is not None:\n",
        "            f.write(\"FOLD [\"+str(fold_zerobased+1)+\"/\"+str(folds)+\"]\\n\\n\")\n",
        "        f.write(\"Summary:\\n\")\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write('Data read from file '+DATASET_FILENAME+'\\n')\n",
        "        if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "            f.write('Window size of 4800 samples (100ms)\\n')\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "            f.write('Window size of 704 samples (~14ms)\\n')\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "            f.write('Window size of 704 samples (~14ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "            f.write('Window size of 2112 samples (~44ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "            f.write('Window size of 3456 samples (~72ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "            f.write('Window size of 4800 samples (~100ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        else:\n",
        "            raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"+--| Features: \\n\")\n",
        "        f.write('Number of features selected: '+str(len(selected_features))+'\\n')\n",
        "        f.write('Selected features: '+str(selected_features)+'\\n')\n",
        "        f.write('Feature Selection method: '+str(FEATURE_SELECTION)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        if DO_NORMALIZE_DATA:\n",
        "            f.write(\"Data was normalized. Find the parameters at the end of the file, and the scaler in the pickle file 'scaler.pickle'\\n\")\n",
        "        else:\n",
        "            f.write(\"Data was NOT normalized\")\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write('Run arguments: '+str(args)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"Optimizer: \" + optimizer[\"method\"])\n",
        "        if optimizer[\"method\"] == \"sgd\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]) + \" momentum: \" + str(optimizer[\"momentum\"]))\n",
        "        elif optimizer[\"method\"] == \"adam\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]))\n",
        "        else:\n",
        "            assert(False) # If triggered check new optimizer and add case\n",
        "        f.write(\"\\n\\n\")\n",
        "        if final_cross_validation_results:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs for each fold (\"+str(folds)+\"-foldCrossValidation)\\n\")\n",
        "            f.write(\"Single results in the folds directories\\n\")\n",
        "            f.write('\\n\\n-------- Average results --------\\n\\n')\n",
        "        else:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs\\n\")\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('(K-Fold cross validation run (fold '+str(fold_zerobased+1)+'/' +str(folds)+ '))\\n')\n",
        "            else:\n",
        "                f.write('(Single run, NO k-fold cross validation)\\n')\n",
        "\n",
        "        for metric in metrics.keys():\n",
        "            value = metrics[metric] if fold_zerobased is None else metrics[metric][fold_zerobased]\n",
        "            f.write(str(metric) + \":\\n\" + str(value) + \"\\n\\n\")\n",
        "        f.close()\n",
        "\n",
        "    # Copy Tensorboard Logs\n",
        "    if fold_zerobased == None and DO_SAVE_TENSORBOARD_LOGS:\n",
        "        LOGPATH=outpath+\"/tensorboardlogs\"\n",
        "        shutil.copytree('./logs', LOGPATH)\n",
        "\n",
        "    if not COLAB and fold_zerobased == None:\n",
        "        # Copy script or notebook depending on the execution environment\n",
        "        script_path = None\n",
        "        if is_notebook():\n",
        "            script_path = 'expressive-technique-classifier-phase3.ipynb'\n",
        "            pass #TODO: make this work\n",
        "        else:\n",
        "            script_path = os.path.realpath(__file__)\n",
        "        shutil.copyfile(script_path, os.path.join(outpath, 'backup_'+os.path.basename(script_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0iA0sVlINRz"
      },
      "source": [
        "# Prepare Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTVv1XlUOgBh"
      },
      "outputs": [],
      "source": [
        "if os.path.exists('./logs'):\n",
        "    shutil.rmtree('./logs', ignore_errors=True) #Clear logs if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2NMIioKEYyP"
      },
      "outputs": [],
      "source": [
        "def start_tensorboard(tb_dir,logname):\n",
        "    log_dir = tb_dir\n",
        "    if logname is not None: \n",
        "        log_dir += logname\n",
        "    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "tb_dir = \"logs/fit/\"\n",
        "# %tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY1u63rqX7fn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(train_metric, validation_metric, title, xlabel, ylabel, filename=None, show = False):\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.plot(train_metric)\n",
        "    ax.plot(validation_metric)\n",
        "    ax.legend(['Training','Validation'])\n",
        "    # if show:\n",
        "    #     fig.show()\n",
        "    if filename is not None:\n",
        "        plt.savefig(filename+\".pdf\",bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qExZCK5ipvaa"
      },
      "source": [
        "F1-Score on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hBHvhH2punc"
      },
      "outputs": [],
      "source": [
        "def macroweighted_f1(y_true,y_pred):\n",
        "    f1scores = []\n",
        "    numSamples = []\n",
        "    for selclass in CLASSES:\n",
        "        classSelection = (y_true == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        numSamples.append(sum(classSelection))\n",
        "        classPrediction = (y_pred == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        true_positives = np.sum(np.logical_and(classSelection,(y_true == y_pred)))\n",
        "\n",
        "        precision = 1.0 * true_positives / np.sum(classPrediction)\n",
        "        recall = 1.0 * true_positives / np.sum(classSelection)\n",
        "        f1score = 2 /((1/precision)+(1/recall))\n",
        "        f1scores.append(f1score)\n",
        "    macroWeightedF1 = sum(np.array(f1scores) * np.array(numSamples)) / sum(numSamples)\n",
        "    return macroWeightedF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SQi2z0Hw2Km"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred,_verbose = False):\n",
        "    accuracy = np.sum(y_pred == y_true)/np.shape(y_true)[0]\n",
        "    f1mw = macroweighted_f1(y_true,y_pred)\n",
        "    confusion_matrix = sk_conf_matrix(y_true, y_pred)\n",
        "    \n",
        "    assert len(y_true) == len(y_pred), 'The \"y_true\" and \"y_pred\" arrays have a different length' \n",
        "\n",
        "    assert len(np.unique(y_true)) >= len(np.unique(y_pred)) \n",
        "    assert np.isin(np.unique(y_pred),np.unique(y_true)).all(), 'Some classes in y_pred are not in y_true ('+str(np.setdiff1d(y_pred,y_true))+')'\n",
        "    \n",
        "    classification_report = sk_class_report(y_true, y_pred, digits=6,target_names = CLASSES_DESC.values(),output_dict=True)\n",
        "    printable_classification_report = sk_class_report(y_true, y_pred, digits=4,target_names = CLASSES_DESC.values())\n",
        "\n",
        "    if _verbose:\n",
        "        print(\"Test Accuracy: \" + str(accuracy) + \"\\nTest macro_weighted_avg f1-score: \" + str(f1mw)+'\\n'+str(confusion_matrix)+'\\n'+str(printable_classification_report))\n",
        "\n",
        "    return accuracy, f1mw, confusion_matrix, classification_report, printable_classification_report\n",
        "\n",
        "\n",
        "def pretty_confusion_matrix(confusion_matrix,savepath,labels):\n",
        "    assert os.path.exists(os.path.dirname(savepath)), 'The path to save the confusion matrix does not exist'\n",
        "    cmplot = sk_conf_matrix_disp(confusion_matrix,display_labels=labels).plot()\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.savefig(savepath,bbox_inches='tight')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEGBYnUF1vXn"
      },
      "source": [
        "# Prepare TFLite conversion and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAvtjWCwxx1I"
      },
      "outputs": [],
      "source": [
        "# TFLite conversion function\n",
        "def convert2tflite(tf_model_dir,tflite_model_dir = None,model_name=\"model\",quantization=None,dataset=None):\n",
        "    assert (quantization==None or quantization==\"dynamic\" or quantization==\"float-fallback\" or quantization==\"full\")\n",
        "    # Convert the model saved in the previous step.\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "    if quantization is not None:\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        if quantization == \"full\" or quantization==\"float-fallback\":\n",
        "            assert dataset is not None\n",
        "            def representative_dataset_gen():\n",
        "                for data in tf.data.Dataset.from_tensor_slices((dataset)).batch(1).take(100):\n",
        "                    yield [tf.dtypes.cast(data, tf.float32)]\n",
        "            converter.representative_dataset = representative_dataset_gen\n",
        "        if quantization == \"full\":\n",
        "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "            converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "            converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "        if quantization == \"dynamic\":\n",
        "            assert dataset is None\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the TF Lite model.\n",
        "    if tflite_model_dir is None:\n",
        "        TF_MODEL_PATH = tf_model_dir + \"/\" + model_name + '.tflite'\n",
        "    else:\n",
        "        TF_MODEL_PATH = tflite_model_dir + \"/\" + model_name + '.tflite'\n",
        "\n",
        "    with tf.io.gfile.GFile(TF_MODEL_PATH, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "## USAGE\n",
        "# model_path = MODELFOLDER + \"/\" + RUN_NAME + \"/fold_1\"\n",
        "# convert2tflite(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6wGuSE91syp"
      },
      "outputs": [],
      "source": [
        "def test_tflite_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    tflite_interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    input_details = tflite_interpreter.get_input_details()[0]\n",
        "    output_details = tflite_interpreter.get_output_details()[0]\n",
        "    \n",
        "    if verbose_test:\n",
        "        print(\"+--------------------------------------------+\\n| Testing the TF lite model saved            |\\n+--------------------------------------------+\\n\")\n",
        "        print(\"[Model loaded]\\n\")\n",
        "        print(\"\\n== Input details ==\\nname:\"+ str(input_details['name']) + \"\\nshape:\"+str(input_details['shape']) +  \"\\ntype:\"+str(input_details['dtype']))\n",
        "        print(\"\\n== Output details ==\\nname:\"+str(output_details['name']) + \"\\nshape:\"+str(output_details['shape']) + \"\\ntype:\"+str(output_details['dtype']))\n",
        "        print(\"+--------------------------------------------+\\n| Testing on TEST set...                     |\\n+--------------------------------------------+\\n\")\n",
        "    \n",
        "    tflite_interpreter.allocate_tensors()\n",
        "    y_pred = list()\n",
        "    for i in range(X_test.shape[0]):\n",
        "        extracted_test_sample = np.array(X_test[i:i+1]).astype(np.float32)\n",
        "        \n",
        "        # Quantize inputs if necessary (full uint model)\n",
        "        if input_details['dtype'] is np.int8:\n",
        "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "            extracted_test_sample = (extracted_test_sample / input_scale + input_zero_point).astype(np.int8)\n",
        "\n",
        "        if first_layer_is_conv:\n",
        "            input_tensor = np.expand_dims(extracted_test_sample,axis=2).astype(input_details[\"dtype\"])\n",
        "        else:\n",
        "            input_tensor = extracted_test_sample\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Setting \"+str(input_tensor.shape)+\" \"+str(input_tensor.dtype)+\" as input\")\n",
        "\n",
        "        tflite_interpreter.set_tensor(input_details['index'], input_tensor)\n",
        "        tflite_interpreter.invoke()\n",
        "        prediction_vec = tflite_interpreter.get_tensor(output_details['index'])\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Getting \"+str(prediction_vec.shape)+\" \"+str(prediction_vec.dtype)+\" as output\")\n",
        "\n",
        "        if output_details['dtype'] is np.int8:\n",
        "            output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "            prediction_vec = (prediction_vec + output_zero_point) * output_scale\n",
        "\n",
        "        if verbose_test:\n",
        "            print(prediction_vec)\n",
        "        y_pred.append(np.argmax(prediction_vec))\n",
        "    return y_pred\n",
        "\n",
        "def test_regulartf_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    imported = tf.keras.models.load_model(model_path)\n",
        "    if first_layer_is_conv:\n",
        "        test_set = np.expand_dims(X_test,axis=2)\n",
        "    else:\n",
        "        test_set = X_test\n",
        "    _, accuracy = imported.evaluate(test_set,  y_test, verbose=2)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPVrRtLoxvu"
      },
      "source": [
        "# k-Fold Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISgdGWqRFkMt"
      },
      "outputs": [],
      "source": [
        "# --> Epochs / Batches\n",
        "print('Using training epochs: ', args['epochs'])\n",
        "print('Using batch size: ', args['batchsize'])\n",
        "\n",
        "# --> Early Stopping\n",
        "use_early_stopping = False\n",
        "\n",
        "# --> OVERSAMPLING ################################################\n",
        "DO_OVERSAMPLING = args['oversampling_aggressiveness'] > 0.0       #\n",
        "OVERSAMPLING_AGGRESSIVENESS = args['oversampling_aggressiveness'] #\n",
        "VERBOSE_OVERSAMPLING = True                                       #\n",
        "SMOTE_STRATEGY = {} # Do not set this variable\n",
        "###################################################################\n",
        "\n",
        "# --> KFOLD RUN #################################################\n",
        "K_SPLITS = args['k_folds']\n",
        "USE_CROSS_VALIDATION = K_SPLITS > 1 # Activate K-Fold Cross Validation only if K_SPLITS > 1\n",
        "VAL_SPLIT_SIZE = 0.1                                            # percentage of total entries going into the validation set\n",
        "# TODO: this is something to fix.\n",
        "# For the custom splitter that keeps guitarists separate, there\n",
        "# is the need to take the validation set from the test and not \n",
        "# the train set. To do this we now have two percentages.\n",
        "# TODO: change all code to always take the validation percentage\n",
        "# from the test set, so that there can be a single percentage \n",
        "# constant.\n",
        "VAL_SPLIT_SIZE_TESTPERC = 0.3                                   # percentage of test entries going into the validation set\n",
        "random_state = global_random_state                              # seed for pseudo random generator\n",
        "#################################################################\n",
        "\n",
        "# --> SINGLE RUN ################################################\n",
        "SAVE_MODEL_INFO = True                                          #\n",
        "test_split_size = 0.2                                           #\n",
        "#################################################################\n",
        "\n",
        "DO_TEST = False\n",
        "\n",
        "# optimizer = { \"method\" : \"sgd\", \"learning_rate\" : args['learning_rate'], \"momentum\" : 0.7 }\n",
        "optimizer = { \"method\" : \"adam\", \"learning_rate\" : args['learning_rate'] }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGkWC3hVoiz-"
      },
      "outputs": [],
      "source": [
        "def player_stats(metadata,exclude_extra_500 = False):\n",
        "    # metadata = metadata[metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    if exclude_extra_500:\n",
        "        metadata = metadata[~metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el) for el in metadata['meta_audiofilePath']]\n",
        "    players_meta_list = [el[0] for el in players_meta_list]\n",
        "    players = np.unique(players_meta_list).tolist()\n",
        "\n",
        "    print(len(players),'players in the dataset')\n",
        "\n",
        "    for pix, p in enumerate(players):\n",
        "        print(str(pix+1)+' - Player \"'+p+'\" \\thas '+str(players_meta_list.count(p))+' note entries',end='')\n",
        "\n",
        "        # p_records = [el for el in metadata if '_'+p+'_' in metadata['meta_audiofilePath']]\n",
        "\n",
        "        p_records = metadata[metadata['meta_audiofilePath'].str.contains('_'+p+'_')]\n",
        "        assert len(p_records) == players_meta_list.count(p)\n",
        "        count_for_each_tech = p_records.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "        print('  ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in count_for_each_tech.items()]))\n",
        "\n",
        "    tot_count_for_each_tech = metadata.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "    print('_'*170)\n",
        "    print('                                          Tot: ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in tot_count_for_each_tech.items()]))\n",
        "\n",
        "# player_stats(dataset_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxfTUtYgg_nO"
      },
      "outputs": [],
      "source": [
        "class CustomPlayerFold():\n",
        "    def __init__(self, metadata: pd.DataFrame, features: pd.DataFrame, labels: pd.DataFrame, _val_split_size: float):\n",
        "        self.val_split_size = _val_split_size\n",
        "        self.metadata = metadata.copy()\n",
        "        self.features = features.copy()\n",
        "        self.labels = labels.copy()\n",
        "\n",
        "        assert self.metadata.shape[0] == self.features.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.features.shape[0]) \n",
        "        assert self.metadata.shape[0] == self.labels.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.labels.shape[0]) \n",
        "        assert np.max(self.metadata.index) == len(self.metadata.index)-1, 'np.max(self.metadata.index) == len(self.metadata.index)-1 ('+str(np.max(self.metadata.index))+'!='+str(len(self.metadata.index)-1)+')'\n",
        "        assert np.max(self.features.index) == len(self.features.index)-1, 'np.max(self.features.index) == len(self.features.index)-1 ('+str(np.max(self.features.index))+'!='+str(len(self.features.index)-1)+')'\n",
        "        assert np.max(self.labels.index) == len(self.labels.index)-1, 'np.max(self.labels.index) == len(self.labels.index)-1 ('+str(np.max(self.labels.index))+'!='+str(len(self.labels.index)-1)+')'\n",
        "\n",
        "        players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el)[0] for el in metadata['meta_audiofilePath']]\n",
        "        self.players = np.unique(players_meta_list).tolist()\n",
        "        self.k_splits = len(self.players)\n",
        "\n",
        "        for idd, player in enumerate(self.players):\n",
        "            print('Fold %d/%d - TestGuitarist: \"%s\"'%(idd+1,len(self.players),player))\n",
        "\n",
        "        player_stats(metadata)\n",
        "\n",
        "    def get_k_splits(self,):\n",
        "        return self.k_splits\n",
        "\n",
        "    def split(self,_X,_y):\n",
        "        res = []\n",
        "\n",
        "        assert (_X == self.features.to_numpy()).all()\n",
        "        assert (_y == self.labels.to_numpy()).all()\n",
        "\n",
        "        for idd, player in enumerate(self.players):\n",
        "            print('Fold %d/%d - Player \"%s\"'%(idd+1,len(self.players),player))\n",
        "            p_records = self.metadata[self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "            not_p_records = self.metadata[~self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "\n",
        "            train_idx = not_p_records.index.values\n",
        "            test_idx = p_records.index.values\n",
        "\n",
        "            # print('test_idx',test_idx)\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "\n",
        "            stratify_labels = p_records['meta_expressive_technique_id'].tolist()\n",
        "            \n",
        "            # In this case, we want to take the validation split from the TEST split,\n",
        "            # unlike all the other cases where we take it from the test set.\n",
        "            # This is because we want the validation results to highlight GENERALIZATION.\n",
        "            # So we split the test and validation indexes here\n",
        "\n",
        "            # To keep things somewhat consistent, we take 'val_split_size', which is supposed to be a percentage of the train set, and convert it so that it is the same number of samples, when extracted from the test set\n",
        "            # new_valsplit_perc = self.val_split_size * len(train_idx) / len(test_idx)\n",
        "            # print('val_split_size:',self.val_split_size)\n",
        "            # print('len(train_idx):',len(train_idx))\n",
        "            # print('len(test_idx):', len(test_idx))\n",
        "            # assert new_valsplit_perc <= 1.0, 'new_valsplit_perc > 1.0 ('+str(new_valsplit_perc)+')'\n",
        "            # assert new_valsplit_perc >= 0.0, 'new_valsplit_perc < 0.0 ('+str(new_valsplit_perc)+')'\n",
        "\n",
        "            test_idx,val_idx = train_test_split(test_idx,test_size=self.val_split_size,random_state=random_state, shuffle=True, stratify = stratify_labels)\n",
        "\n",
        "            # print('len(val_idx)/(len(val_idx)+len(test_idx))',len(val_idx)/(len(val_idx)+len(test_idx)))\n",
        "            # print()\n",
        "\n",
        "            # print('len(val_idx)',len(val_idx))\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "            \n",
        "            # print()\n",
        "            # print()\n",
        "\n",
        "\n",
        "            assert type(train_idx) == type(val_idx) == type(test_idx)\n",
        "            assert np.all(train_idx < len(self.labels))\n",
        "            assert np.all(val_idx < len(self.labels))\n",
        "            assert np.all(test_idx < len(self.labels))\n",
        "            assert np.array_equal(np.unique(train_idx),sorted(train_idx))\n",
        "            assert np.array_equal(np.unique(test_idx),sorted(test_idx))\n",
        "            assert np.array_equal(np.unique(val_idx),sorted(val_idx))\n",
        "\n",
        "            res.append((train_idx,test_idx,val_idx))\n",
        "\n",
        "            players_intest = np.unique([re.findall(r'_([A-Z][a-z][a-z][A-Z][a-z][a-z][0-9]?)_',dat) for dat in self.metadata.iloc[test_idx]['meta_audiofilePath'].tolist()])\n",
        "            assert len(players_intest) == 1, 'There should be only one player in the test set. Instead, there are '+str(len(players_intest))+' players in the test set: '+str(players_intest)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1hUwlybmgEE"
      },
      "outputs": [],
      "source": [
        "def oversample(features: list, labels: list, aggressiveness = 1, verbose: bool = False):\n",
        "    if verbose:\n",
        "        print(\"Oversampling...\")\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        target_count = int(round(max(counts) * aggressiveness,0))\n",
        "        sampling_strategy = dict(zip(unique, counts))\n",
        "        print('Label count in fold:',dict(zip(unique, counts)))\n",
        "        sampling_strategy = {k:(v if v > target_count else target_count) for k,v in sampling_strategy.items()}\n",
        "        smote_strategy = sampling_strategy\n",
        "\n",
        "        print('sampling_strategy:',sampling_strategy)\n",
        "\n",
        "        ovs_features, ovs_labels = imblearn.over_sampling.SMOTE(sampling_strategy=sampling_strategy).fit_resample(features, labels)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Increased training samples from \" + str(features.shape[0]) + \" to \" + str(ovs_features.shape[0]))\n",
        "            printSupport(ovs_labels)\n",
        "    return ovs_features, ovs_labels, smote_strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataTo1DConv = lambda x: np.expand_dims(x,axis = -1)\n",
        "dataTo2DConv = lambda x: np.expand_dims(x.reshape((len(x),len(WINDOW_INDEXES),AUTO_FEATURE_NUMBER)),axis = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy9WiiwEpHXf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "prefix = \"CrossValidated\" if USE_CROSS_VALIDATION else \"Single\"\n",
        "RUN_NAME = prefix + \"Run_\"+strftime(\"%Y%m%d-%H%M%S\")\n",
        "OUTPUT_DIR = os.path.join(MODELFOLDER,RUN_NAME)\n",
        "os.makedirs(OUTPUT_DIR,exist_ok = True) \n",
        "\n",
        "\n",
        "if CUSTOM_PLAYER_K_FOLD:\n",
        "    print('Warning! Using custom K-Fold, which splits the dataset in the data from different players.')\n",
        "    cv = CustomPlayerFold(dataset_metadata,dataset_features,dataset_labels,_val_split_size=VAL_SPLIT_SIZE_TESTPERC)\n",
        "    K_SPLITS = cv.get_k_splits()\n",
        "    print('as a result, the number of folds has been OVERWRITTEN and is now ',K_SPLITS)\n",
        "\n",
        "else:\n",
        "    cv = StratifiedKFold(n_splits=K_SPLITS,shuffle=True,random_state=random_state)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uLTufCRmlpIW",
        "4pBrSDphxyxL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-fix",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "bbba7bb91c842039e47232619da588d5d39cf0384ecb01581f802e1efeb502cf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
