{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAJRnwSBQNbQ"
      },
      "source": [
        "# Expressive Guitar Technique classifier\n",
        "Ph.D. research project of [Domenico Stefani](work.domenicostefani.com)  \n",
        "This notebook loads a dataset of feature vectors extracted from **pitched** and **percussive** sounds recorded with many acoustic guitars.\n",
        "The techniques/classes recorded are:  \n",
        "\n",
        "0.    **Kick**      (Palm on lower body)\n",
        "1.    **Snare 1**   (All fingers on lower side)\n",
        "2.    **Tom**       (Thumb on higher body)\n",
        "3.    **Snare 2**   (Fingers on the muted strings, over the end\n",
        "of the fingerboard)\n",
        "___\n",
        "4.    **Natural Harmonics** (Stop strings from playing the dominant frequency, letting harmonics ring)\n",
        "5.    **Palm Mute** (Muting partially the strings with the palm\n",
        "of the pick hand)\n",
        "6.    **Pick Near Bridge** (Playing toward the bridge/saddle)\n",
        "7.    **Pick Over the Soundhole** (Playing over the sound hole)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2ha_JDqV9I"
      },
      "source": [
        "## Import modules and mount drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yPrPhmjHk-kc"
      },
      "outputs": [],
      "source": [
        "# Choose ClassificationTask task\n",
        "from enum import Enum\n",
        "class ClassificationTask(Enum):\n",
        "    FULL_8_CLASS_PROBLEM =          (1,'full')\n",
        "    BINARY_PERCUSSIVE_PITCHED =     (2,'binary')\n",
        "    PERCUSSIVE_4_ONLY =             (3,'perc')\n",
        "    PITCHED_4_ONLY =                (4,'pitch')\n",
        "    PERCUSSIVE_PLUS_PITCHED_CLASS = (5,'perc+pitch')\n",
        "\n",
        "classification_task = ClassificationTask.FULL_8_CLASS_PROBLEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VuprRjSbkuh3"
      },
      "outputs": [],
      "source": [
        "# Install module for the ReliefF feature selection\n",
        "# !pip install skrebate\n",
        "# !pip install tensorboard\n",
        "# !pip3 install pickle5\n",
        "# !pip3 install tensorflow==2.4.1\n",
        "# !pip3 install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dw8otQ7AQMaV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Tensorflow version: 2.4.1\n",
            "Imblearn version: 0.6.2\n",
            "Not running on CoLab\n"
          ]
        }
      ],
      "source": [
        "REQUIRE_GPU = False\n",
        "DO_SAVE_TENSORBOARD_LOGS = False \n",
        "DO_SAVE_FOLD_MODELS = False \n",
        "CUSTOM_PLAYER_K_FOLD = True         # Very important, this ditches the k-fold stratified random shuffle, and creates as many splits as the guitar players, separating natural groups\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS = True # If true, drop the data from files that have 'extra' in the filename, which otherwise make the dataset unbalanced\n",
        "\n",
        "\n",
        "USE_TENSORBOARD = True\n",
        "\n",
        "USE_AUGMENTED_DATA = True\n",
        "# USE_AUGMENTED_DATA = False\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if USE_TENSORBOARD:\n",
        "    %load_ext tensorboard\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "from sys import executable as sys_executable\n",
        "from sys import argv as sys_argv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import strftime, time\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from bz2 import BZ2File # To open compressed data\n",
        "import re\n",
        "import shutil\n",
        "import imblearn\n",
        "from sklearn.metrics import confusion_matrix as sk_conf_matrix\n",
        "from sklearn.metrics import classification_report as sk_class_report\n",
        "\n",
        "print(\"Tensorflow version: \" + tf.version.VERSION)\n",
        "print('Imblearn version:',imblearn.__version__)\n",
        "\n",
        "global_random_state = 42\n",
        "np.random.seed(global_random_state)\n",
        "tf.random.set_seed(global_random_state)\n",
        "\n",
        "COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if COLAB:\n",
        "    print('Running on CoLab')\n",
        "    #Connect and mount the drive folder that contains the train dataset and the output folder\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "    HOMEBASE = os.path.join('/content','gdrive','MyDrive','dottorato','Publications','02-IEEE-RTEmbeddedTimbreClassification(submitted)','Classifier')\n",
        "    THISDIR = \"/content/\"\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    HOMEBASE = \".\"\n",
        "    THISDIR = \"./\"\n",
        "DATAFOLDER = os.path.join(HOMEBASE,\"data/phase3\")\n",
        "MODELFOLDER = os.path.join(HOMEBASE,\"output\")\n",
        "\n",
        "RELIEF_CACHE_FILEPATH = os.path.join(DATAFOLDER,'relief_cache.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sY_34JvX6uFT"
      },
      "outputs": [],
      "source": [
        "def is_notebook() -> bool:\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ND_A4d6uFT"
      },
      "source": [
        "## Parse Command Line arguments\n",
        "\n",
        "*_Important_*: If you are running this from a jupyter Notebook, change the run parameters at the end of the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dVHOAmQ-6uFU"
      },
      "outputs": [],
      "source": [
        "args = None\n",
        "if not is_notebook() and not COLAB:\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Train the expressive guitar technique classifier.')\n",
        "\n",
        "    def featnum_type(x):\n",
        "        (MIN,MAX) = (1,495) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Feature number must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netdepth_type(x):\n",
        "        (MIN,MAX) = (1,20) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network depth must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netwidth_type(x):\n",
        "        (MIN,MAX) = (1,2000) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network width must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def dropout_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Dropout Rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def aggressiveness_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Oversampling aggressiveness value must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def lr_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Learning rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def batchsize_type(x):\n",
        "        (MIN,MAX) = (1,4096) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Batchsize must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def epochs_type(x):\n",
        "        (MIN,MAX) = (1,10000) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Batchsize must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def kfold_type(x):\n",
        "        (MIN,MAX) = (1,20) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Batchsize must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    parser.add_argument('-f',  '--features',      default=80,     type=featnum_type,   help='Number of features to use for training [1-495] (default: 80)')\n",
        "    parser.add_argument('-d',  '--net-depth',     default=3,      type=netdepth_type,  help='Number of layers in the FFNN [1-20] (default: 3)')\n",
        "    parser.add_argument('-w',  '--net-width',     default=100,    type=netwidth_type,  help='Number of layers in the FFNN [1-2000] (default: 100)')\n",
        "    parser.add_argument('-dr', '--dropout',       default=0.15,   type=dropout_type,   help='Dropout amount [0-1] (default: 0.15)')\n",
        "    parser.add_argument('-lr', '--learning-rate', default=0.0001, type=lr_type,        help='Learning rate [0-1] (default: 0.0001)')\n",
        "    parser.add_argument('-bs', '--batchsize',     default=256,    type=batchsize_type, help='Learning rate [1-4096] (default: 256)')\n",
        "    parser.add_argument('-e',  '--epochs',        default=1000,   type=epochs_type,    help='Learning rate [1-10000] (default: 1000)')\n",
        "    parser.add_argument('-k',  '--k-folds',       default=5,      type=kfold_type,     help='K of K-folds [1-20] (default: 5)')\n",
        "    parser.add_argument('-os', '--oversampling',  action='store_true', help='Perform oversampling')\n",
        "    parser.add_argument('-osagg', '--oversampling-aggressiveness',  default=0.2,   type=aggressiveness_type,   help='Oversampling aggressiveness [0-1] (default: 0.2)')\n",
        "\n",
        "    \n",
        "    parser.add_argument('-v', '--verbose',        action='store_true', help='increase output verbosity')\n",
        "    args = parser.parse_args()\n",
        "    args = vars(args)\n",
        "else:\n",
        "    \n",
        "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "    \"\"\"                               +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    #                                 |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #                                 + ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ +                                 #\n",
        "    \"\"\" #↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓#\n",
        "    args = {'features':      30, \n",
        "            'net_depth':     1, \n",
        "            'net_width':     10, \n",
        "            'dropout':       0.15,\n",
        "            'learning_rate': 0.00001,\n",
        "            'batchsize':     1024,\n",
        "            'epochs':        10,\n",
        "            'k_folds':       5,\n",
        "            'oversampling':  True,\n",
        "            'oversampling_aggressiveness':  0.0,\n",
        "            'verbose':       False}\n",
        "    #↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑#\n",
        "    \"\"\"                               + ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ +                                 #\n",
        "    #                                 |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #                                 +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    \"\"\"#----------------------------------------------------------------------------------------------------------------------------------------------------------------#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7qyEuAk6uFV"
      },
      "source": [
        "## Enforce GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9BxHBUDQPXKS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "print(physical_devices)\n",
        "if REQUIRE_GPU:\n",
        "  assert len(tf.config.experimental.list_physical_devices('GPU')) >= 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZMbpx2eM2G"
      },
      "source": [
        "## Check Real avaliable GRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8lSUK12K6uFW"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def pip_install(package):\n",
        "    subprocess.check_call([sys_executable, \"-m\", \"pip\", \"install\", package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R_8hnGH7eL_T"
      },
      "outputs": [],
      "source": [
        "CHECK_GRAM = False\n",
        "\n",
        "if CHECK_GRAM:\n",
        "    # memory footprint support libraries/code\n",
        "    os.symlink('/opt/bin/nvidia-smi','/usr/bin/nvidia-smi')\n",
        "    pip_install('gputil')\n",
        "    pip_install('psutil')\n",
        "    pip_install('humanize')\n",
        "    import psutil\n",
        "    import humanize\n",
        "    import os\n",
        "    import GPUtil as GPU\n",
        "    GPUs = GPU.getGPUs()\n",
        "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "    gpu = GPUs[0]\n",
        "    def printm():\n",
        "        process = psutil.Process(os.getpid())\n",
        "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "    printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2hdmJnSsEOM"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CozyQAQgznvK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset from pickle...\n",
            "Successfully Loaded!\n",
            "It took 1.2s to load from regular pickle\n"
          ]
        }
      ],
      "source": [
        "LOAD_DATA_FROM = 'pickle'\n",
        "# LOAD_DATA_FROM = 'compressedpickle'\n",
        "\n",
        "if LOAD_DATA_FROM == 'compressedpickle':\n",
        "    print(\"Reading dataset from compressed pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,'20220929_115154_onlycorrectdetections.bz2')\n",
        "    startime = time()\n",
        "    ifile = BZ2File(DATASET_PATH,'rb')\n",
        "    featuredataset = pickle.load(ifile)\n",
        "    ifile.close()\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from compressed pickle' % (time()-startime))\n",
        "elif LOAD_DATA_FROM == 'pickle':\n",
        "    print(\"Reading dataset from pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,'20220929_115154_onlycorrectdetections.pickle')\n",
        "    startime = time()\n",
        "    with open(DATASET_PATH,'rb') as pf:\n",
        "        featuredataset = pickle.load(pf)\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from regular pickle' % (time()-startime))\n",
        "\n",
        "# display(featuredataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading file metaplus_onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221125-144015_gainaug-phase3-aug1PROCESSED_FEATURES.pickle\n",
            "Loading file metaplus_onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221125-160225_gainaug-phase3-aug3PROCESSED_FEATURES.pickle\n",
            "Loading file metaplus_onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221125-163857_gainaug-phase3-aug4PROCESSED_FEATURES.pickle\n",
            "Loading file metaplus_onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221125-152410_gainaug-phase3-aug2PROCESSED_FEATURES.pickle\n",
            "Loaded 92293 augmented samples\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "augmented_featuredataset_list = []\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_data_paths = glob(os.path.join(DATAFOLDER,'augmented_data','*.pickle'))\n",
        "    for augmented_data_path in augmented_data_paths:\n",
        "        print(\"Loading file %s\" % os.path.basename(augmented_data_path))\n",
        "        with open(augmented_data_path,'rb') as pf:\n",
        "            augmented_featuredataset_list.append(pickle.load(pf))\n",
        "    augmented_featuredataset = pd.concat(augmented_featuredataset_list, ignore_index=True)\n",
        "    print(\"Loaded %d augmented samples\" % len(augmented_featuredataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1rKcNy5RqZ"
      },
      "source": [
        "### Drop features that we have found to be problematic with feature selection and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Uhip8r7F4sSv"
      },
      "outputs": [],
      "source": [
        "def drop_unused_features(features_df: pd.DataFrame, inplace = False) -> pd.DataFrame:\n",
        "    if not inplace:\n",
        "        res_df = features_df.copy()\n",
        "    else:\n",
        "        res_df = features_df\n",
        "    if 'attackTime_peaksamp'       not in res_df.columns.to_list() or\\\n",
        "       'attackTime_attackStartIdx' not in res_df.columns.to_list() or\\\n",
        "       'peakSample_index'          not in res_df.columns.to_list():\n",
        "       raise Exception(\"The features dataframe does not contain the required columns!\")\n",
        "\n",
        "    res_df.drop(columns=['attackTime_peaksamp',\\\n",
        "                                'attackTime_attackStartIdx',\\\n",
        "                                'peakSample_index'], inplace=True)\n",
        "    return res_df\n",
        "\n",
        "featuredataset = drop_unused_features(featuredataset)\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_featuredataset = drop_unused_features(augmented_featuredataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xh-UknklQy"
      },
      "source": [
        "### If specified, drop extra percussive recorded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WvUB_btbklQy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping 2237 additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.\n",
            "Dataset shape after dropping extra percussive recordings: (21053, 504)\n"
          ]
        }
      ],
      "source": [
        "assert featuredataset.shape == (23290, 504)\n",
        "if DROP_EXTRA_PERCUSSIVE_SOUNDS:\n",
        "    to_drop_count = np.count_nonzero(featuredataset.meta_audiofilePath.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count))\n",
        "        featuredataset = featuredataset[~featuredataset.meta_audiofilePath.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(featuredataset.shape)))\n",
        "    assert featuredataset.shape == (21053, 504)\n",
        "\n",
        "\n",
        "augmented_featuredataset_dr = augmented_featuredataset.copy()\n",
        "if USE_AUGMENTED_DATA and DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG:\n",
        "    to_drop_count_aug = np.count_nonzero(augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count_aug >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count_aug))\n",
        "        augmented_featuredataset_dr = augmented_featuredataset[~augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(augmented_featuredataset_dr.shape)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-HN56MAc5kjA"
      },
      "outputs": [],
      "source": [
        "# Extract separate DFs\n",
        "from typing import Tuple\n",
        "\n",
        "# Divide dataset into metadata, features and labels\n",
        "def divide_dataset(features_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    metadata = features_df.filter(regex='^meta_',axis=1)\n",
        "    labels = features_df.meta_expressive_technique_id\n",
        "    features = features_df.loc[:,[col for col in features_df.columns if col not in metadata.columns]]\n",
        "    # Convert to numeric formats where possible (somehow convert_dtypes doesn't work [https://stackoverflow.com/questions/65915048/pandas-convert-dtypes-not-working-on-numbers-marked-as-objects])\n",
        "    metadata = metadata.apply(pd.to_numeric, errors='ignore')\n",
        "    labels = labels.apply(pd.to_numeric, errors='ignore')\n",
        "    features = features.apply(pd.to_numeric, errors='ignore')\n",
        "    return metadata, features, labels\n",
        "\n",
        "metadata, features, labels = divide_dataset(featuredataset)\n",
        "assert metadata.shape[1] == 9\n",
        "assert features.shape[1] == 495\n",
        "\n",
        "if USE_AUGMENTED_DATA:\n",
        "    metadata_aug, features_aug, labels_aug = divide_dataset(augmented_featuredataset_dr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X1iiyTkcYKFi"
      },
      "outputs": [],
      "source": [
        "def get_classes_description(classftask: ClassificationTask):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        classes_desk = {0:\"Kick\",1:\"Snare 1\",2:\"Tom\",3:\"Snare 2\",4:\"Natural Harmonics\",5:\"Palm Mute\",6:\"Pick Near Bridge\",7:\"Pick Over the Soundhole\"}\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        classes_desk = {0:\"Percussive\",1:\"Pitched\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\"}\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        classes_desk = {0:\"Natural Harmonics\", 1:\"Palm Mute\", 2:\"Pick Near Bridge\", 3:\"Pick Over the Soundhole\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\", 4:\"Pitched\"}\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "    classes = list(classes_desk.keys())\n",
        "    return classes,classes_desk\n",
        "\n",
        "def filter_dataset(tofilt_features,tofilt_labels,tofilt_metadata,classftask: ClassificationTask, hardcoded_sizes_test = False):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        pass\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 21053\n",
        "        tofilt_labels = tofilt_labels.replace([0,1,2,3],[0,0,0,0])\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[1,1,1,1])\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 21053\n",
        "        filtered_idxs = tofilt_labels < 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 1620\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 21053\n",
        "        filtered_idxs = tofilt_labels >= 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[0,1,2,3])\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 19433\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 21053\n",
        "        tofilt_labels = tofilt_labels.replace([5,6,7],[4,4,4])\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "\n",
        "\n",
        "    tofilt_features.reset_index(drop=True,inplace=True)\n",
        "    tofilt_labels.reset_index(drop=True,inplace=True)\n",
        "    tofilt_metadata.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    return tofilt_features, tofilt_labels, tofilt_metadata\n",
        "\n",
        "original_dataset_features = features.copy()\n",
        "dataset_labels = labels.copy()\n",
        "dataset_metadata = metadata.copy()\n",
        "\n",
        "CLASSES,CLASSES_DESC = get_classes_description(classification_task)\n",
        "original_dataset_features,dataset_labels,dataset_metadata = filter_dataset(original_dataset_features,dataset_labels,dataset_metadata,classification_task,hardcoded_sizes_test=True)\n",
        "if USE_AUGMENTED_DATA:\n",
        "    features_aug,labels_aug,metadata_aug = filter_dataset(features_aug,labels_aug,metadata_aug,classification_task,hardcoded_sizes_test=False)\n",
        "    assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(labels_aug))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(labels_aug))).all()\n",
        "\n",
        "assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(dataset_labels))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(dataset_labels))).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "H_JEEbL-mcJ-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset \"Main datase\" read\n",
            "| Entries: 1620\n",
            "╰─ Features: 495\n",
            "Dataset \"Augmented data\" read\n",
            "| Entries: 15609\n",
            "╰─ Features: 495\n"
          ]
        }
      ],
      "source": [
        "if USE_AUGMENTED_DATA:\n",
        "    for dat,name in [(original_dataset_features,'Main datase'),(features_aug,'Augmented data')]:\n",
        "        print('Dataset \"'+name+'\" read')\n",
        "        print(\"| Entries: \"+str(dat.shape[0]))\n",
        "        print(\"╰─ Features: \"+str(dat.shape[1]))\n",
        "\n",
        "original_feature_number = original_dataset_features.shape[1]\n",
        "(relief_data_X,relief_data_y) = (original_dataset_features.values,dataset_labels.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "v2gv9fFFavMz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6d44679eced50127e10b740d7545884fbc05374333f9b48c07281417687ca13d\n"
          ]
        }
      ],
      "source": [
        "# Compute has of the dataset files.\n",
        "# This are used to cache precomputed feature selection with ReliefF (Which is rather slow)\n",
        "import hashlib\n",
        " \n",
        "dataset_sha256_hash = hashlib.sha256()\n",
        "with open(DATASET_PATH,\"rb\") as fy:\n",
        "    for byte_block in iter(lambda: fy.read(4096),b\"\"):    # Read and update hash string value in blocks of 4K\n",
        "        dataset_sha256_hash.update(byte_block)\n",
        "dataset_sha256_hash = dataset_sha256_hash.hexdigest()\n",
        "\n",
        "print(dataset_sha256_hash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjF3Cif5zr1p"
      },
      "source": [
        "## Subset features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "--hCfGHKZK98"
      },
      "outputs": [],
      "source": [
        "def get_manual_selected_features(data):\n",
        "    print (\"Subsetting features...\")\n",
        "    columns_to_keep = []\n",
        "    # if USE_ATTACKTIME_PEAKSAMP:\n",
        "    #     columns_to_keep.append(\"attackTime_peaksamp\")\n",
        "    # if USE_ATTACKTIME_ATTACKSTARTIDX:\n",
        "    #     columns_to_keep.append(\"attackTime_attackStartIdx\")\n",
        "    if USE_ATTACKTIME_VALUE:\n",
        "        columns_to_keep.append(\"attackTime_value\")\n",
        "    if USE_BARKSPECBRIGHTNESS:\n",
        "        columns_to_keep.append(\"barkSpecBrightness\")\n",
        "    if USE_PEAKSAMPLE_VALUE:\n",
        "        columns_to_keep.append(\"peakSample_value\")\n",
        "    # if USE_PEAKSAMPLE_INDEX:\n",
        "    #     columns_to_keep.append(\"peakSample_index\")\n",
        "    if USE_ZEROCROSSING:\n",
        "        columns_to_keep.append(\"zeroCrossing\")\n",
        "\n",
        "    assert USE_BARKSPEC <= 50 and USE_BARKSPEC >= 0 and USE_BFCC <= 49 and USE_BFCC >= 0 and USE_CEPSTRUM <= 353 and USE_CEPSTRUM >= 0 and USE_MFCC <= 37 and USE_MFCC >= 0\n",
        "\n",
        "    if USE_BARKSPEC > 0:\n",
        "        columns_to_keep += ['barkSpec_'+str(i+1) for i in range(USE_BARKSPEC)]\n",
        "    if USE_BFCC > 0:\n",
        "        columns_to_keep += ['bfcc_'+str(i+2) for i in range(USE_BFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "    if USE_CEPSTRUM > 0:\n",
        "        columns_to_keep += ['cepstrum_'+str(i+1) for i in range(USE_CEPSTRUM)]\n",
        "    if USE_MFCC > 0:\n",
        "        columns_to_keep += ['mfcc_'+str(i+2) for i in range(USE_MFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "\n",
        "    return columns_to_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "agcUWSWVbokS"
      },
      "outputs": [],
      "source": [
        "## To Compeltely reset RelieFF cache\n",
        "# with open(RELIEF_CACHE_FILEPATH, 'wb') as rcf:\n",
        "#     pickle.dump(set(), rcf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PBDXJV0dnx2W"
      },
      "outputs": [],
      "source": [
        "# how_many_examples_per_class =10\n",
        "# subselection = list(range(0,how_many_examples_per_class))+\\\n",
        "#                list(range(600,600+how_many_examples_per_class))+\\\n",
        "#                list(range(1100,1100+how_many_examples_per_class))+\\\n",
        "#                list(range(1400,1400+how_many_examples_per_class))+\\\n",
        "#                list(range(1900,1900+how_many_examples_per_class))+\\\n",
        "#                list(range(3000,3000+how_many_examples_per_class))+\\\n",
        "#                list(range(9000,9000+how_many_examples_per_class))+\\\n",
        "#                list(range(14000,14000+how_many_examples_per_class))\n",
        "\n",
        "# testprova_dataset_features = original_dataset_features.iloc[subselection]\n",
        "# testprova_dataset_labels = dataset_labels.iloc[subselection]\n",
        "import os, platform, subprocess, re\n",
        "\n",
        "def get_processor_name():\n",
        "    if platform.system() == \"Windows\":\n",
        "        return platform.processor()\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n",
        "        command =\"sysctl -n machdep.cpu.brand_string\"\n",
        "        return subprocess.check_output(command).strip()\n",
        "    elif platform.system() == \"Linux\":\n",
        "        command = \"cat /proc/cpuinfo\"\n",
        "        all_info = subprocess.check_output(command, shell=True).decode().strip()\n",
        "        for line in all_info.split(\"\\n\"):\n",
        "            if \"model name\" in line:\n",
        "                return re.sub( \".*model name.*:\", \"\", line,1)\n",
        "    return \"\"\n",
        "\n",
        "class ReliefCacheElem(dict):\n",
        "\n",
        "    PRINT_HASH = False\n",
        "\n",
        "    def __init__(self,dataset_sha256,n_neighbors,relieff_top_features,relieff_feature_importances,time_of_computation):\n",
        "        self.dataset_sha256 = dataset_sha256\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.relieff_top_features = relieff_top_features\n",
        "        self.relieff_feature_importances = relieff_feature_importances\n",
        "        self.date = strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
        "\n",
        "        self.cpu_info = get_processor_name()\n",
        "        self.time_of_computation = time_of_computation\n",
        "\n",
        "    def __key(self):\n",
        "        return tuple([self.dataset_sha256,\n",
        "                     self.n_neighbors,\n",
        "                     str(self.relieff_top_features),\n",
        "                     str(self.relieff_feature_importances)])\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.__key())\n",
        "\n",
        "    def __str__(self):\n",
        "        res = '{date: '+self.date+', n_neighbors:'+str(self.n_neighbors)\n",
        "        \n",
        "        if self.PRINT_HASH:\n",
        "            res += 'dataset_sha256:'+str(self.dataset_sha256)+','\n",
        "\n",
        "        res += 'cpu_info:'+str(self.cpu_info)+','\n",
        "        res += 'time_of_computation:'+str(self.time_of_computation)+','\n",
        "        res += '}'\n",
        "        return res\n",
        "\n",
        "\n",
        "def relieff_selection(X:list,y:list,n_features,n_neighbors,relief_cache_filepath,verbose_ = True):\n",
        "    relief_data_X = X\n",
        "    relief_data_y = y\n",
        "    relief_top_features_ = None\n",
        "    relief_feature_importances_ = None\n",
        "    # First check if result is already cached\n",
        "    ## Load Cache\n",
        "    relief_cache = None\n",
        "\n",
        "    ##----------------------------------------------##\n",
        "    if not os.path.exists(relief_cache_filepath):\n",
        "        raise Exception(\"RELIEF CACHE NOT FOUND at '\"+relief_cache_filepath+\"'! Comment exception to create empty cache\")\n",
        "        with open(relief_cache_filepath, 'wb') as rcf:\n",
        "            pickle.dump(set(), rcf)\n",
        "    ##----------------------------------------------##\n",
        "\n",
        "    with open(relief_cache_filepath,'rb') as rcf:\n",
        "        relief_cache = pickle.load(rcf)\n",
        "        if verbose_: \n",
        "            print('Loaded Relief cache ('+str(len(relief_cache))+' solutions)')\n",
        "    # Check if present\n",
        "    for cache_elem in relief_cache:\n",
        "        if cache_elem.dataset_sha256 == dataset_sha256_hash and\\\n",
        "           cache_elem.n_neighbors == n_neighbors:\n",
        "            if verbose_:\n",
        "                print(\"Result found in cache!\")\n",
        "            return cache_elem.relieff_top_features[:n_features]\n",
        "    \n",
        "    # If not present, compute\n",
        "    if verbose_:\n",
        "        print(\"Result NOT found in cache, computing now... (might take a long while)\")\n",
        "    \n",
        "    from skrebate import ReliefF\n",
        "    r = ReliefF(n_neighbors=n_neighbors,verbose=verbose_)\n",
        "    \n",
        "    start_fit = time()\n",
        "    r.fit(X=relief_data_X,y=relief_data_y)\n",
        "    top_features = r.top_features_\n",
        "    feature_importances = r.feature_importances_\n",
        "    stop_fit = time()\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done. Now storing in cache...\")\n",
        "\n",
        "    savedata = ReliefCacheElem(\n",
        "        dataset_sha256 = dataset_sha256_hash,\n",
        "        n_neighbors = n_neighbors,\n",
        "        relieff_top_features = top_features,\n",
        "        relieff_feature_importances = feature_importances,\n",
        "        time_of_computation = stop_fit - start_fit)\n",
        "    relief_cache.add(savedata)\n",
        "    with open(relief_cache_filepath, 'wb') as rcf:\n",
        "        pickle.dump(relief_cache, rcf)\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done.\")\n",
        "\n",
        "\n",
        "    return top_features[:n_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oas8vnnMQ0uP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 cached relief runs:\n",
            "(1/1 are from the same dataset)\n",
            "0 : {date: 2022/10/03-12:18:23, n_neighbors:5cpu_info: Intel(R) Core(TM) i9-10940X CPU @ 3.30GHz,time_of_computation:3267.6181452274323,}\n"
          ]
        }
      ],
      "source": [
        "with open(RELIEF_CACHE_FILEPATH,'rb') as rcf:\n",
        "    relief_cache = pickle.load(rcf)\n",
        "    \n",
        "    print(len(relief_cache),'cached relief runs:')\n",
        "\n",
        "    if len(relief_cache) != 0:\n",
        "        samedataset = [e for e in relief_cache if e.dataset_sha256 == dataset_sha256_hash]\n",
        "        print('('+str(len(samedataset))+'/'+str(len(relief_cache)), 'are from the same dataset)')\n",
        "        if len(samedataset) != len(relief_cache):\n",
        "            raise Exception('Some of the cached results are from a different dataset!')\n",
        "\n",
        "        for i,e in enumerate(relief_cache):\n",
        "            print(i,':',e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kLX_8-4Zaagm"
      },
      "outputs": [],
      "source": [
        "class FeatureSelection(Enum):\n",
        "    MANUAL_VARIABLES = 1\n",
        "    MANUAL_LIST = 2\n",
        "    AUTO_ANOVA = 3      # https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
        "    AUTO_RELIEF = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JVkiDFjjztbM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30 best features:['barkSpecBrightness', 'barkSpec_1', 'barkSpec_2', 'barkSpec_3', 'barkSpec_4', 'barkSpec_8', 'barkSpec_9', 'barkSpec_10', 'barkSpec_11', 'barkSpec_12', 'barkSpec_22', 'barkSpec_30', 'barkSpec_31', 'barkSpec_50', 'bfcc_2', 'bfcc_3', 'bfcc_4', 'bfcc_5', 'bfcc_6', 'bfcc_7', 'bfcc_8', 'bfcc_9', 'cepstrum_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9']\n",
            "Features reduced automatically (FeatureSelection.AUTO_ANOVA) from 495 to : 30\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------------------------------------------------------------------- #\n",
        "#\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_VARIABLES\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_LIST\n",
        "FEATURE_SELECTION = FeatureSelection.AUTO_ANOVA\n",
        "# FEATURE_SELECTION = FeatureSelection.AUTO_RELIEF\n",
        "AUTO_FEATURE_NUMBER = args['features']    # If FEATURE_SELECTION is AUTO_ANOVA or AUTO_RELIEF, select this number of features automatically\n",
        "#\n",
        "# ------------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "if FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES:\n",
        "    ''' Features '''\n",
        "    USE_ATTACKTIME_VALUE = True\n",
        "    USE_BARKSPECBRIGHTNESS = True\n",
        "    USE_PEAKSAMPLE_VALUE = True\n",
        "    USE_ZEROCROSSING = False\n",
        "\n",
        "    USE_BARKSPEC = 40 # Number in range [0-50]\n",
        "    USE_BFCC = 40     # Number in range [0-50]\n",
        "    USE_CEPSTRUM = 60 # Number in range [0-353]\n",
        "    USE_MFCC = 30     # Number in range [0-38]\n",
        "\n",
        "    selected_features = get_manual_selected_features(original_dataset_features)\n",
        "elif FEATURE_SELECTION == FeatureSelection.MANUAL_LIST:\n",
        "    selected_features = ['attackTime_value', 'barkSpecBrightness', 'barkSpec_1', 'barkSpec_2', 'barkSpec_3', 'barkSpec_4', 'barkSpec_5', 'barkSpec_6', 'barkSpec_7', 'barkSpec_8', 'barkSpec_9', 'barkSpec_10', 'barkSpec_11', 'barkSpec_12', 'barkSpec_13', 'barkSpec_14', 'barkSpec_15', 'barkSpec_16', 'barkSpec_17', 'barkSpec_18', 'barkSpec_19', 'barkSpec_20', 'barkSpec_21', 'barkSpec_22', 'barkSpec_23', 'barkSpec_24', 'barkSpec_25', 'barkSpec_26', 'barkSpec_27', 'barkSpec_28', 'barkSpec_29', 'barkSpec_30', 'barkSpec_31', 'barkSpec_32', 'barkSpec_33', 'barkSpec_34', 'barkSpec_35', 'barkSpec_36', 'barkSpec_37', 'barkSpec_38', 'barkSpec_39', 'barkSpec_40', 'barkSpec_41', 'barkSpec_42', 'barkSpec_43', 'barkSpec_44', 'barkSpec_45', 'barkSpec_46', 'barkSpec_47', 'barkSpec_48', 'barkSpec_49', 'barkSpec_50', 'bfcc_2', 'bfcc_3', 'bfcc_4', 'bfcc_5', 'bfcc_6', 'bfcc_7', 'bfcc_8', 'bfcc_9', 'bfcc_10', 'bfcc_11', 'bfcc_12', 'bfcc_13', 'bfcc_15', 'bfcc_16', 'bfcc_17', 'bfcc_18', 'bfcc_19', 'bfcc_20', 'bfcc_21', 'bfcc_25', 'bfcc_26', 'bfcc_27', 'bfcc_28', 'bfcc_29', 'bfcc_30', 'bfcc_31', 'bfcc_35', 'bfcc_36', 'bfcc_37', 'bfcc_39', 'bfcc_40', 'bfcc_42', 'bfcc_43', 'bfcc_44', 'bfcc_45', 'bfcc_46', 'bfcc_48', 'cepstrum_1', 'cepstrum_2', 'cepstrum_3', 'cepstrum_4', 'cepstrum_5', 'cepstrum_6', 'cepstrum_7', 'cepstrum_8', 'cepstrum_9', 'cepstrum_10', 'cepstrum_11', 'cepstrum_12', 'cepstrum_13', 'cepstrum_14', 'cepstrum_15', 'cepstrum_16', 'cepstrum_17', 'cepstrum_18', 'cepstrum_19', 'cepstrum_20', 'cepstrum_21', 'cepstrum_22', 'cepstrum_23', 'cepstrum_24', 'cepstrum_25', 'cepstrum_26', 'cepstrum_27', 'cepstrum_28', 'cepstrum_29', 'cepstrum_30', 'cepstrum_31', 'cepstrum_32', 'cepstrum_33', 'cepstrum_34', 'cepstrum_35', 'cepstrum_36', 'cepstrum_37', 'cepstrum_41', 'cepstrum_42', 'cepstrum_43', 'cepstrum_44', 'cepstrum_45', 'cepstrum_46', 'cepstrum_47', 'cepstrum_48', 'cepstrum_49', 'cepstrum_54', 'cepstrum_56', 'cepstrum_59', 'cepstrum_60', 'cepstrum_67', 'cepstrum_72', 'cepstrum_86', 'cepstrum_87', 'cepstrum_108', 'cepstrum_164', 'cepstrum_205', 'cepstrum_206', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13', 'mfcc_14', 'mfcc_15', 'mfcc_16', 'mfcc_17', 'mfcc_18', 'mfcc_19', 'mfcc_20', 'mfcc_21', 'mfcc_22', 'mfcc_23', 'mfcc_24', 'mfcc_25', 'mfcc_26', 'mfcc_32', 'mfcc_33', 'mfcc_34', 'mfcc_35', 'mfcc_36', 'peakSample_value', 'zeroCrossing']\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_ANOVA:\n",
        "    if original_dataset_features.shape[1] != original_feature_number:\n",
        "        raise ValueError(\"ERROR: please import dataset again since you are trying to subset an already processed feature set (\"+str(dataset_features.shape[1])+\"<\"+str(original_feature_number)+\")\")\n",
        "\n",
        "    # ANOVA feature selection for numeric input and categorical output (https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/#:~:text=Feature%20selection%20is%20the%20process,the%20performance%20of%20the%20model)\n",
        "    from sklearn.feature_selection import SelectKBest\n",
        "    from sklearn.feature_selection import f_classif\n",
        "    \n",
        "    fs = SelectKBest(score_func=f_classif, k=AUTO_FEATURE_NUMBER) # Define feature selection\n",
        "    X_selected = fs.fit_transform(original_dataset_features.to_numpy(), dataset_labels.to_numpy().ravel())                         # Apply feature selection\n",
        "    support = fs.get_support(indices=True)                      # Extract selected indexes\n",
        "    selected_features = original_dataset_features.columns[support].tolist()\n",
        "    print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_RELIEF:\n",
        "    \n",
        "    support = relieff_selection(relief_data_X,relief_data_y,AUTO_FEATURE_NUMBER,n_neighbors=5,relief_cache_filepath=RELIEF_CACHE_FILEPATH,verbose_= True)\n",
        "    selected_features = original_dataset_features.columns[support].tolist()\n",
        "    print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "    \n",
        "\n",
        "else:\n",
        "    raise Exception(\"ERROR! This type of feature selection is not supported\")\n",
        "\n",
        "dataset_features = original_dataset_features.copy().loc[:,selected_features]\n",
        "if USE_AUGMENTED_DATA:\n",
        "    features_aug = features_aug.copy().loc[:,selected_features]\n",
        "print(\"Features reduced \"+('manually' if (FEATURE_SELECTION == FeatureSelection.MANUAL_LIST or FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES) else 'automatically')+\" (\"+str(FEATURE_SELECTION)+\") from \"+str(original_feature_number)+\" to : \"+str(dataset_features.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxb5oNdswky3"
      },
      "source": [
        "## Evaluate class support\n",
        "(What percentage of dataset entries represent each class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vOwPsK_h5r3q"
      },
      "outputs": [],
      "source": [
        "DO_PRINT_SUPPORT = False\n",
        "def printSupport (labels_ds):\n",
        "    binc = np.bincount(np.reshape(labels_ds,labels_ds.size))\n",
        "    for i in range(binc.size):\n",
        "        print(\"Class \" + str(i) + \" support: \" + str(\"{:.2f}\".format(binc[i]/sum(binc) * 100)) + \"%\")\n",
        "        \n",
        "if DO_PRINT_SUPPORT:\n",
        "    printSupport(dataset_labels.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w196aHu6YqnH"
      },
      "source": [
        "# Define model architecture,\n",
        "Loss, optimizer and compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1Be14IPJTg_4"
      },
      "outputs": [],
      "source": [
        "def define_model_architecture(num_classes:int, _verbose = False):\n",
        "    tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "    net_width = args['net_width']\n",
        "\n",
        "    dropout_rate = args['dropout']\n",
        "\n",
        "    # sequential_structure = [tf.keras.Input(shape=(args['features'],))]\n",
        "    sequential_structure = []\n",
        "\n",
        "    for i in range(0,args['net_depth']):\n",
        "        sequential_structure += [tf.keras.layers.Dense(net_width,activation='relu',\n",
        "                                                       kernel_initializer='he_uniform'), #   X   |           |         |\n",
        "                                 tf.keras.layers.BatchNormalization(),                   #       |     X     |         |\n",
        "                                 tf.keras.layers.Dropout(dropout_rate)                   #       |           |    X    |\n",
        "                                ]\n",
        "\n",
        "    sequential_structure += [tf.keras.layers.Dense(net_width,activation='relu',\n",
        "                                                   kernel_initializer='he_uniform'),     #   X   |           |         |\n",
        "                                                   tf.keras.layers.Dense(num_classes)]               \n",
        "\n",
        "    model = tf.keras.models.Sequential(sequential_structure)\n",
        "\n",
        "    timestr = strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model._name = \"guitar_timbre_classifier_\" + timestr\n",
        "    if _verbose:\n",
        "        print(\"Created model: '\" + model.name + \"'\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_loss():\n",
        "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3kOMPQBTXVXb"
      },
      "outputs": [],
      "source": [
        "def compile_model(model,optimizer,loss_fn,_verbose = False):\n",
        "    opt = None\n",
        "    if optimizer[\"method\"] == \"sgd\":\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate = optimizer[\"learning_rate\"], momentum=optimizer[\"momentum\"])\n",
        "    elif optimizer[\"method\"] == \"adam\":\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = optimizer[\"learning_rate\"])\n",
        "    else:\n",
        "        raise Exception(\"Optimizer method not supported\")\n",
        "\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss_fn,\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    if _verbose:\n",
        "        print(\"Model compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc66NhrugHsG"
      },
      "source": [
        "# Save Models and Info functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1Vbu_AA6dhUU"
      },
      "outputs": [],
      "source": [
        "def save_model_info(model,optimizer,final_cross_validation_results,folds,metrics,outpath, fold_zerobased = None, smote_strategy = None):\n",
        "    info_filename = '/info.txt' if fold_zerobased is None else '/info_fold_'+str(fold_zerobased+1)+'.txt'\n",
        "    assert not (final_cross_validation_results and (fold_zerobased is not None))\n",
        "\n",
        "    with open(outpath + info_filename, \"w\") as f:\n",
        "        if not is_notebook():\n",
        "            f.write('Execution command:\\n')\n",
        "            f.write(\" \".join(sys_argv[:])+'\\n')\n",
        "        else:\n",
        "            f.write('Trained with the jupyter notebook (not the script version)\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if DO_OVERSAMPLING:\n",
        "            f.write(\"Oversampling (SMOTE) with aggressiveness: \"+str(OVERSAMPLING_AGGRESSIVENESS)+'\\n')\n",
        "            if smote_strategy is not None:\n",
        "                f.write(\"SMOTE strategy: \"+str(smote_strategy)+'\\n')\n",
        "            else:\n",
        "                f.write(\"SMOTE strategy: \"+str(SMOTE_STRATEGY)+')\\n')\n",
        "        else:\n",
        "            f.write('NOT performing Oversampling'+'\\n')\n",
        "\n",
        "        if USE_AUGMENTED_DATA:\n",
        "            f.write('Using augmented data'+'\\n')\n",
        "            f.write('Augmented audio file paths: ' + ','.join([os.path.basename(x) for x in augmented_data_paths])+'\\n')\n",
        "            f.write('Loaded '+str(len(augmented_featuredataset))+' augmented samples'+'\\n')\n",
        "            f.write('Used '+str(len(features_aug))+' augmented samples after filtering the dataset'+'\\n')\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('Augmented data used in this fold: '+str(len(train_aug_indexes[fold_zerobased]))+'\\n')\n",
        "            else:\n",
        "                f.write('Augmented data used for all splits: \\n')\n",
        "                for split_idx in train_aug_indexes:\n",
        "                    f.write(str(len(train_aug_indexes[split_idx]))+'\\n')\n",
        "\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if fold_zerobased is not None:\n",
        "            f.write(\"FOLD [\"+str(fold_zerobased+1)+\"/\"+str(folds)+\"]\\n\\n\")\n",
        "        f.write(\"Summary:\\n\")\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"+--| Features: \\n\")\n",
        "        f.write('Number of features selected: '+str(len(selected_features))+'\\n')\n",
        "        f.write('Selected features: '+str(selected_features)+'\\n')\n",
        "        f.write('Feature Selection method: '+str(FEATURE_SELECTION)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write('Run arguments: '+str(args)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"Optimizer: \" + optimizer[\"method\"])\n",
        "        if optimizer[\"method\"] == \"sgd\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]) + \" momentum: \" + str(optimizer[\"momentum\"]))\n",
        "        elif optimizer[\"method\"] == \"adam\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]))\n",
        "        else:\n",
        "            assert(False) # If triggered check new optimizer and add case\n",
        "        f.write(\"\\n\\n\")\n",
        "        if final_cross_validation_results:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs for each fold (\"+str(folds)+\"-foldCrossValidation)\\n\")\n",
        "            f.write(\"Single results in the folds directories\\n\")\n",
        "            f.write('\\n\\n-------- Average results --------\\n\\n')\n",
        "        else:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs\\n\")\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('(K-Fold cross validation run (fold '+str(fold_zerobased+1)+'/' +str(folds)+ '))\\n')\n",
        "            else:\n",
        "                f.write('(Single run, NO k-fold cross validation)\\n')\n",
        "\n",
        "        for metric in metrics.keys():\n",
        "            value = metrics[metric] if fold_zerobased is None else metrics[metric][fold_zerobased]\n",
        "            f.write(str(metric) + \":\\n\" + str(value) + \"\\n\\n\")\n",
        "        f.close()\n",
        "\n",
        "    # Copy Tensorboard Logs\n",
        "    if fold_zerobased == None and DO_SAVE_TENSORBOARD_LOGS:\n",
        "        LOGPATH=outpath+\"/tensorboardlogs\"\n",
        "        shutil.copytree('./logs', LOGPATH)\n",
        "\n",
        "    if not COLAB and fold_zerobased == None:\n",
        "        # Copy script or notebook depending on the execution environment\n",
        "        script_path = None\n",
        "        if is_notebook():\n",
        "            script_path = 'expressive-technique-classifier-phase3.ipynb'\n",
        "            pass #TODO: make this work\n",
        "        else:\n",
        "            script_path = os.path.realpath(__file__)\n",
        "        shutil.copyfile(script_path, os.path.join(outpath, 'backup_'+os.path.basename(script_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0iA0sVlINRz"
      },
      "source": [
        "# Prepare Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "VTVv1XlUOgBh"
      },
      "outputs": [],
      "source": [
        "if os.path.exists('./logs'):\n",
        "    shutil.rmtree('./logs', ignore_errors=True) #Clear logs if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "u2NMIioKEYyP"
      },
      "outputs": [],
      "source": [
        "def start_tensorboard(tb_dir,logname):\n",
        "    log_dir = tb_dir\n",
        "    if logname is not None: \n",
        "        log_dir += logname\n",
        "    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "tb_dir = \"logs/fit/\"\n",
        "# %tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MY1u63rqX7fn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(train_metric, validation_metric, title, xlabel, ylabel, filename=None, show = False):\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.plot(train_metric)\n",
        "    ax.plot(validation_metric)\n",
        "    ax.legend(['Training','Validation'])\n",
        "    if show:\n",
        "        fig.show()\n",
        "    if filename is not None:\n",
        "        plt.savefig(filename+\".pdf\",bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qExZCK5ipvaa"
      },
      "source": [
        "F1-Score on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5hBHvhH2punc"
      },
      "outputs": [],
      "source": [
        "def macroweighted_f1(y_true,y_pred):\n",
        "    f1scores = []\n",
        "    numSamples = []\n",
        "    for selclass in CLASSES:\n",
        "        classSelection = (y_true == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        numSamples.append(sum(classSelection))\n",
        "        classPrediction = (y_pred == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        true_positives = np.sum(np.logical_and(classSelection,(y_true == y_pred)))\n",
        "\n",
        "        precision = 1.0 * true_positives / np.sum(classPrediction)\n",
        "        recall = 1.0 * true_positives / np.sum(classSelection)\n",
        "        f1score = 2 /((1/precision)+(1/recall))\n",
        "        f1scores.append(f1score)\n",
        "    macroWeightedF1 = sum(np.array(f1scores) * np.array(numSamples)) / sum(numSamples)\n",
        "    return macroWeightedF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4SQi2z0Hw2Km"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred,_verbose = False):\n",
        "    accuracy = np.sum(y_pred == y_true)/np.shape(y_true)[0]\n",
        "    f1mw = macroweighted_f1(y_true,y_pred)\n",
        "    confusion_matrix = sk_conf_matrix(y_true, y_pred)\n",
        "    \n",
        "    assert len(y_true) == len(y_pred), 'The \"y_true\" and \"y_pred\" arrays have a different length' \n",
        "\n",
        "    assert len(np.unique(y_true)) >= len(np.unique(y_pred)) \n",
        "    assert np.isin(np.unique(y_pred),np.unique(y_true)).all(), 'Some classes in y_pred are not in y_true ('+str(np.setdiff1d(y_pred,y_true))+')'\n",
        "    \n",
        "    classification_report = sk_class_report(y_true, y_pred, digits=6,target_names = CLASSES_DESC.values(),output_dict=True)\n",
        "    printable_classification_report = sk_class_report(y_true, y_pred, digits=4,target_names = CLASSES_DESC.values())\n",
        "\n",
        "    if _verbose:\n",
        "        print(\"Test Accuracy: \" + str(accuracy) + \"\\nTest macro_weighted_avg f1-score: \" + str(f1mw)+'\\n'+str(confusion_matrix)+'\\n'+str(printable_classification_report))\n",
        "\n",
        "    return accuracy, f1mw, confusion_matrix, classification_report, printable_classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEGBYnUF1vXn"
      },
      "source": [
        "# Prepare TFLite conversion and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uAvtjWCwxx1I"
      },
      "outputs": [],
      "source": [
        "# TFLite conversion function\n",
        "def convert2tflite(tf_model_dir,tflite_model_dir = None,model_name=\"model\",quantization=None,dataset=None):\n",
        "    assert (quantization==None or quantization==\"dynamic\" or quantization==\"float-fallback\" or quantization==\"full\")\n",
        "    # Convert the model saved in the previous step.\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "    if quantization is not None:\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        if quantization == \"full\" or quantization==\"float-fallback\":\n",
        "            assert dataset is not None\n",
        "            def representative_dataset():\n",
        "                for data in tf.data.Dataset.from_tensor_slices((dataset)).batch(1).take(100):\n",
        "                    yield [tf.dtypes.cast(data, tf.float32)]\n",
        "            converter.representative_dataset = representative_dataset\n",
        "        if quantization == \"full\":\n",
        "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "            converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "            converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "        if quantization == \"dynamic\":\n",
        "            assert dataset is None\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the TF Lite model.\n",
        "    if tflite_model_dir is None:\n",
        "        TF_MODEL_PATH = tf_model_dir + \"/\" + model_name + '.tflite'\n",
        "    else:\n",
        "        TF_MODEL_PATH = tflite_model_dir + \"/\" + model_name + '.tflite'\n",
        "\n",
        "    with tf.io.gfile.GFile(TF_MODEL_PATH, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "## USAGE\n",
        "# model_path = MODELFOLDER + \"/\" + RUN_NAME + \"/fold_1\"\n",
        "# convert2tflite(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "M6wGuSE91syp"
      },
      "outputs": [],
      "source": [
        "def test_tflite_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    tflite_interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    input_details = tflite_interpreter.get_input_details()[0]\n",
        "    output_details = tflite_interpreter.get_output_details()[0]\n",
        "    \n",
        "    if verbose_test:\n",
        "        print(\"+--------------------------------------------+\\n| Testing the TF lite model saved            |\\n+--------------------------------------------+\\n\")\n",
        "        print(\"[Model loaded]\\n\")\n",
        "        print(\"\\n== Input details ==\\nname:\"+ str(input_details['name']) + \"\\nshape:\"+str(input_details['shape']) +  \"\\ntype:\"+str(input_details['dtype']))\n",
        "        print(\"\\n== Output details ==\\nname:\"+str(output_details['name']) + \"\\nshape:\"+str(output_details['shape']) + \"\\ntype:\"+str(output_details['dtype']))\n",
        "        print(\"+--------------------------------------------+\\n| Testing on TEST set...                     |\\n+--------------------------------------------+\\n\")\n",
        "    \n",
        "    tflite_interpreter.allocate_tensors()\n",
        "    y_pred = list()\n",
        "    for i in range(X_test.shape[0]):\n",
        "        extracted_test_sample = np.array(X_test[i:i+1]).astype(np.float32)\n",
        "        \n",
        "        # Quantize inputs if necessary (full uint model)\n",
        "        if input_details['dtype'] is np.int8:\n",
        "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "            extracted_test_sample = (extracted_test_sample / input_scale + input_zero_point).astype(np.int8)\n",
        "\n",
        "        if first_layer_is_conv:\n",
        "            input_tensor = np.expand_dims(extracted_test_sample,axis=2).astype(input_details[\"dtype\"])\n",
        "        else:\n",
        "            input_tensor = extracted_test_sample\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Setting \"+str(input_tensor.shape)+\" \"+str(input_tensor.dtype)+\" as input\")\n",
        "\n",
        "        tflite_interpreter.set_tensor(input_details['index'], input_tensor)\n",
        "        tflite_interpreter.invoke()\n",
        "        prediction_vec = tflite_interpreter.get_tensor(output_details['index'])\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Getting \"+str(prediction_vec.shape)+\" \"+str(prediction_vec.dtype)+\" as output\")\n",
        "\n",
        "        if output_details['dtype'] is np.int8:\n",
        "            output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "            prediction_vec = (prediction_vec + output_zero_point) * output_scale\n",
        "\n",
        "        if verbose_test:\n",
        "            print(prediction_vec)\n",
        "        y_pred.append(np.argmax(prediction_vec))\n",
        "    return y_pred\n",
        "\n",
        "def test_regulartf_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    imported = tf.keras.models.load_model(model_path)\n",
        "    if first_layer_is_conv:\n",
        "        test_set = np.expand_dims(X_test,axis=2)\n",
        "    else:\n",
        "        test_set = X_test\n",
        "    _, accuracy = imported.evaluate(test_set,  y_test, verbose=2)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPVrRtLoxvu"
      },
      "source": [
        "# k-Fold Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ISgdGWqRFkMt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using training epochs:  10\n",
            "Using batch size:  1024\n"
          ]
        }
      ],
      "source": [
        "# --> Epochs / Batches\n",
        "print('Using training epochs: ', args['epochs'])\n",
        "print('Using batch size: ', args['batchsize'])\n",
        "\n",
        "# --> Quantize (Dynamic) and test the TF Lite model obtained (quicker but lower accuracy)\n",
        "TEST_QUANTIZATION = False\n",
        "# --> Early Stopping\n",
        "use_early_stopping = False\n",
        "\n",
        "# --> OVERSAMPLING ################################################\n",
        "DO_OVERSAMPLING = args['oversampling']                            #\n",
        "OVERSAMPLING_AGGRESSIVENESS = args['oversampling_aggressiveness'] #\n",
        "VERBOSE_OVERSAMPLING = False                                      #\n",
        "SMOTE_STRATEGY = {} # Do not set this variable\n",
        "###################################################################\n",
        "\n",
        "# --> KFOLD RUN #################################################\n",
        "K_SPLITS = args['k_folds']\n",
        "USE_CROSS_VALIDATION = K_SPLITS > 1 # Activate K-Fold Cross Validation only if K_SPLITS > 1\n",
        "VAL_SPLIT_SIZE = 0.1                                            # percentage of total entries going into the validation set\n",
        "# TODO: this is something to fix.\n",
        "# For the custom splitter that keeps guitarists separate, there\n",
        "# is the need to take the validation set from the test and not \n",
        "# the train set. To do this we now have two percentages.\n",
        "# TODO: change all code to always take the validation percentage\n",
        "# from the test set, so that there can be a single percentage \n",
        "# constant.\n",
        "VAL_SPLIT_SIZE_TESTPERC = 0.4                                   # percentage of test entries going into the validation set\n",
        "random_state = global_random_state                              # seed for pseudo random generator\n",
        "#################################################################\n",
        "\n",
        "# --> SINGLE RUN ################################################\n",
        "SAVE_MODEL_INFO = True                                          #\n",
        "test_split_size = 0.2                                           #\n",
        "#################################################################\n",
        "\n",
        "DO_TEST = False\n",
        "\n",
        "# optimizer = { \"method\" : \"sgd\", \"learning_rate\" : args['learning_rate'], \"momentum\" : 0.7 }\n",
        "optimizer = { \"method\" : \"adam\", \"learning_rate\" : args['learning_rate'] }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "kGkWC3hVoiz-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 players in the dataset\n",
            "1 - Player \"DavRos\" \thas 120 note entries   0:  30  \t1:  30  \t2:  30  \t3:  30  \t\n",
            "2 - Player \"DomSte\" \thas 115 note entries   0:  29  \t1:  29  \t2:  27  \t3:  30  \t\n",
            "3 - Player \"GiaFer\" \thas 109 note entries   0:  20  \t1:  29  \t2:  30  \t3:  30  \t\n",
            "4 - Player \"LucFra\" \thas 119 note entries   0:  30  \t1:  30  \t2:  29  \t3:  30  \t\n",
            "5 - Player \"LucTur\" \thas 117 note entries   0:  27  \t1:  30  \t2:  30  \t3:  30  \t\n",
            "6 - Player \"LucTur2\" \thas 1040 note entries   0: 267  \t1: 211  \t2: 262  \t3: 300  \t\n",
            "__________________________________________________________________________________________________________________________________________________________________________\n",
            "                                          Tot:  0: 403  \t1: 359  \t2: 408  \t3: 450  \t\n"
          ]
        }
      ],
      "source": [
        "def player_stats(metadata,exclude_extra_500 = False):\n",
        "    # metadata = metadata[metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    if exclude_extra_500:\n",
        "        metadata = metadata[~metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el) for el in metadata['meta_audiofilePath']]\n",
        "    players_meta_list = [el[0] for el in players_meta_list]\n",
        "    players = np.unique(players_meta_list).tolist()\n",
        "\n",
        "    print(len(players),'players in the dataset')\n",
        "\n",
        "    for pix, p in enumerate(players):\n",
        "        print(str(pix+1)+' - Player \"'+p+'\" \\thas '+str(players_meta_list.count(p))+' note entries',end='')\n",
        "\n",
        "        # p_records = [el for el in metadata if '_'+p+'_' in metadata['meta_audiofilePath']]\n",
        "\n",
        "        p_records = metadata[metadata['meta_audiofilePath'].str.contains('_'+p+'_')]\n",
        "        assert len(p_records) == players_meta_list.count(p)\n",
        "        count_for_each_tech = p_records.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "        print('  ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in count_for_each_tech.items()]))\n",
        "\n",
        "    tot_count_for_each_tech = metadata.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "    print('_'*170)\n",
        "    print('                                          Tot: ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in tot_count_for_each_tech.items()]))\n",
        "\n",
        "player_stats(dataset_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wxfTUtYgg_nO"
      },
      "outputs": [],
      "source": [
        "class CustomPlayerFold():\n",
        "    def __init__(self, metadata: pd.DataFrame, features: pd.DataFrame, labels: pd.DataFrame, _val_split_size: float):\n",
        "        self.val_split_size = _val_split_size\n",
        "        self.metadata = metadata.copy()\n",
        "        self.features = features.copy()\n",
        "        self.labels = labels.copy()\n",
        "\n",
        "        assert self.metadata.shape[0] == self.features.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.features.shape[0]) \n",
        "        assert self.metadata.shape[0] == self.labels.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.labels.shape[0]) \n",
        "        assert np.max(self.metadata.index) == len(self.metadata.index)-1, 'np.max(self.metadata.index) == len(self.metadata.index)-1 ('+str(np.max(self.metadata.index))+'!='+str(len(self.metadata.index)-1)+')'\n",
        "        assert np.max(self.features.index) == len(self.features.index)-1, 'np.max(self.features.index) == len(self.features.index)-1 ('+str(np.max(self.features.index))+'!='+str(len(self.features.index)-1)+')'\n",
        "        assert np.max(self.labels.index) == len(self.labels.index)-1, 'np.max(self.labels.index) == len(self.labels.index)-1 ('+str(np.max(self.labels.index))+'!='+str(len(self.labels.index)-1)+')'\n",
        "\n",
        "        players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el)[0] for el in metadata['meta_audiofilePath']]\n",
        "        self.players = np.unique(players_meta_list).tolist()\n",
        "        self.k_splits = len(self.players)\n",
        "\n",
        "        # player_stats(metadata)\n",
        "\n",
        "    def get_k_splits(self,):\n",
        "        return self.k_splits\n",
        "\n",
        "    def split(self,_X,_y):\n",
        "        res = []\n",
        "\n",
        "        assert (_X == self.features.to_numpy()).all()\n",
        "        assert (_y == self.labels.to_numpy()).all()\n",
        "\n",
        "        for player in self.players:\n",
        "            p_records = self.metadata[self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "            not_p_records = self.metadata[~self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "\n",
        "            train_idx = not_p_records.index.values\n",
        "            test_idx = p_records.index.values\n",
        "\n",
        "            # print('test_idx',test_idx)\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "\n",
        "            stratify_labels = p_records['meta_expressive_technique_id'].tolist()\n",
        "            \n",
        "            # In this case, we want to take the validation split from the TEST split,\n",
        "            # unlike all the other cases where we take it from the test set.\n",
        "            # This is because we want the validation results to highlight GENERALIZATION.\n",
        "            # So we split the test and validation indexes here\n",
        "\n",
        "            # To keep things somewhat consistent, we take 'val_split_size', which is supposed to be a percentage of the train set, and convert it so that it is the same number of samples, when extracted from the test set\n",
        "            # new_valsplit_perc = self.val_split_size * len(train_idx) / len(test_idx)\n",
        "            # print('val_split_size:',self.val_split_size)\n",
        "            # print('len(train_idx):',len(train_idx))\n",
        "            # print('len(test_idx):', len(test_idx))\n",
        "            # assert new_valsplit_perc <= 1.0, 'new_valsplit_perc > 1.0 ('+str(new_valsplit_perc)+')'\n",
        "            # assert new_valsplit_perc >= 0.0, 'new_valsplit_perc < 0.0 ('+str(new_valsplit_perc)+')'\n",
        "\n",
        "            test_idx,val_idx = train_test_split(test_idx,test_size=self.val_split_size,random_state=random_state, shuffle=True, stratify = stratify_labels)\n",
        "\n",
        "            # print('len(val_idx)/(len(val_idx)+len(test_idx))',len(val_idx)/(len(val_idx)+len(test_idx)))\n",
        "            # print()\n",
        "\n",
        "            # print('len(val_idx)',len(val_idx))\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "            \n",
        "            # print()\n",
        "            # print()\n",
        "\n",
        "\n",
        "            assert type(train_idx) == type(val_idx) == type(test_idx)\n",
        "            assert np.all(train_idx < len(self.labels))\n",
        "            assert np.all(val_idx < len(self.labels))\n",
        "            assert np.all(test_idx < len(self.labels))\n",
        "            assert np.array_equal(np.unique(train_idx),sorted(train_idx))\n",
        "            assert np.array_equal(np.unique(test_idx),sorted(test_idx))\n",
        "            assert np.array_equal(np.unique(val_idx),sorted(val_idx))\n",
        "\n",
        "            res.append((train_idx,test_idx,val_idx))\n",
        "\n",
        "            players_intest = np.unique([re.findall(r'_([A-Z][a-z][a-z][A-Z][a-z][a-z][0-9]?)_',dat) for dat in self.metadata.iloc[test_idx]['meta_audiofilePath'].tolist()])\n",
        "            assert len(players_intest) == 1, 'There should be only one player in the test set. Instead, there are '+str(len(players_intest))+' players in the test set: '+str(players_intest)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "G1hUwlybmgEE"
      },
      "outputs": [],
      "source": [
        "def oversample(features: list, labels: list, aggressiveness = 1, verbose: bool = False):\n",
        "    if verbose:\n",
        "        print(\"Oversampling...\")\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        target_count = int(round(max(counts) * aggressiveness,0))\n",
        "        sampling_strategy = dict(zip(unique, counts))\n",
        "        sampling_strategy = {k:(v if v > target_count else target_count) for k,v in sampling_strategy.items()}\n",
        "        smote_strategy = sampling_strategy\n",
        "\n",
        "        print('sampling_strategy:',sampling_strategy)\n",
        "\n",
        "        ovs_features, ovs_labels = imblearn.over_sampling.SMOTE(sampling_strategy=sampling_strategy).fit_resample(features, labels)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Increased training samples from \" + str(features.shape[0]) + \" to \" + str(ovs_features.shape[0]))\n",
        "            printSupport(ovs_labels)\n",
        "    return ovs_features, ovs_labels, smote_strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hy9WiiwEpHXf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning! Using custom K-Fold, which splits the dataset in the data from different players.\n",
            "as a result, the number of folds has been OVERWRITTEN and is now  6\n"
          ]
        }
      ],
      "source": [
        "prefix = \"CrossValidated\" if USE_CROSS_VALIDATION else \"Single\"\n",
        "RUN_NAME = prefix + \"Run_\"+strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "if CUSTOM_PLAYER_K_FOLD:\n",
        "    print('Warning! Using custom K-Fold, which splits the dataset in the data from different players.')\n",
        "    cv = CustomPlayerFold(dataset_metadata,dataset_features,dataset_labels,_val_split_size=VAL_SPLIT_SIZE_TESTPERC)\n",
        "    K_SPLITS = cv.get_k_splits()\n",
        "    print('as a result, the number of folds has been OVERWRITTEN and is now ',K_SPLITS)\n",
        "\n",
        "else:\n",
        "    cv = StratifiedKFold(n_splits=K_SPLITS,shuffle=True,random_state=random_state)\n",
        "\n",
        "def main_routine(X,y,train_idx=None,test_idx=None,val_idx=None,_val_split_size=None,aug_data=None,foldcount=None,is_k_fold=False, eval_metrics=None, quantized_eval_metrics=None):\n",
        "    assert np.equal(np.sort(CLASSES),np.unique(y)).all(), 'np.sort(CLASSES) != np.unique(y)  ('+np.sort(CLASSES)+'!='+np.unique(y)+')'\n",
        "\n",
        "    np.random.shuffle(train_idx)   #TODO: remove if this turns out to be useless\n",
        "    np.random.shuffle(test_idx)    #TODO: remove if this turns out to be useless\n",
        "    np.random.shuffle(val_idx) #TODO: remove if this turns out to be useless\n",
        "    \n",
        "    if eval_metrics is None:\n",
        "        raise ValueError(\"provide a eval_metrics dict\")\n",
        "\n",
        "    current_dir = MODELFOLDER + \"/\" + RUN_NAME\n",
        "    # %mkdir -p \"$current_dir\"\n",
        "    os.makedirs(current_dir,exist_ok = True) \n",
        "    if is_k_fold:\n",
        "        fold_dir = current_dir + '/Fold_' + str(foldcount)\n",
        "        # %mkdir -p \"$fold_dir\"\n",
        "        os.makedirs(fold_dir,exist_ok = True)\n",
        "\n",
        "    ### PRINT INFO\n",
        "    if is_k_fold:\n",
        "        print(\"\\nFold [\"+str(foldcount)+\"/\"+str(K_SPLITS)+\"]\")\n",
        "        #### SPLIT DATA\n",
        "        print(\"Selecting Split Data...\")\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "    else:\n",
        "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_split_size,random_state=random_state, shuffle=True, stratify = y)\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "\n",
        "    if val_idx is None:\n",
        "        print('Performing generic validation/train split from training split')    \n",
        "        X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=_val_split_size,random_state=random_state, shuffle=True, stratify = y_train)\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "    else:\n",
        "        print('Taking validation split from provided indexes')    \n",
        "        X_valid, y_valid = X[val_idx], y[val_idx]\n",
        "        # printSupport(y_valid)                                                       # Verify that the split is STRATIFIED\n",
        "\n",
        "    ### OVERSAMPLE\n",
        "    if VERBOSE_OVERSAMPLING:\n",
        "        if DO_OVERSAMPLING:\n",
        "            print(\"Oversampling...\")\n",
        "        else:\n",
        "            print('NOT performing oversampling')\n",
        "    if DO_OVERSAMPLING:\n",
        "        # VERBOSE_OVERSAMPLING = True\n",
        "        # with warnings.catch_warnings():\n",
        "        #     warnings.simplefilter(\"ignore\")\n",
        "        #     if VERBOSE_OVERSAMPLING:\n",
        "        #         prev_len = y_train.shape[0]\n",
        "        #     X_train, y_train = SMOTE(smote_mask).fit_sample(X_train, y_train)\n",
        "        #     if VERBOSE_OVERSAMPLING:\n",
        "        #         print(\"Increased training samples from \" + str(prev_len) + \" to \" + str(y_train.shape[0]))\n",
        "        #         printSupport(y_train)\n",
        "\n",
        "        X_train, y_train, smote_strategy = oversample(X_train, y_train, aggressiveness = OVERSAMPLING_AGGRESSIVENESS, verbose = VERBOSE_OVERSAMPLING)\n",
        "\n",
        "    ### ADD AUGMENTED DATA\n",
        "    assert not USE_AUGMENTED_DATA or (aug_data != None), 'USE_AUGMENTED_DATA should imply that aug_data is not None, however this is not the case'\n",
        "\n",
        "    if USE_AUGMENTED_DATA:\n",
        "        (metadata_aug, features_aug, labels_aug, train_aug_idx) = aug_data\n",
        "        X_train_aug, y_train_aug = features_aug.to_numpy()[train_aug_idx], labels_aug.to_numpy()[train_aug_idx]\n",
        "        print('Adding augmented data to training set...')\n",
        "        print('Length of training set before augmentation: ',len(X_train))\n",
        "        X_train = np.concatenate((X_train,X_train_aug),axis=0)\n",
        "        y_train = np.concatenate((y_train,y_train_aug),axis=0)\n",
        "        print('Length of training set after augmentation: ',len(X_train))\n",
        "        \n",
        "\n",
        "    ### DEFINE MODEL\n",
        "    model = define_model_architecture(len(CLASSES),_verbose = True)\n",
        "    \n",
        "    ### DEFINE LOSS\n",
        "    loss_fn = get_loss()\n",
        "\n",
        "    ### PREPARE DATA IN CASE OF A FIRST CONV LAYER IN THE NET\n",
        "    if type(model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "        X_train = np.expand_dims(X_train,axis = 2) # Adapt data for Conv1d ([batch_shape, steps, input_dim] -> in our case indim = 1, steps = features, batchshape = train datset size)\n",
        "        X_valid= np.expand_dims(X_valid,axis = 2)  # Adapt data for Conv1d\n",
        "        X_test= np.expand_dims(X_test,axis = 2)    # Adapt data for Conv1d\n",
        "\n",
        "    ### PERFORM TEST (**OPTIONAL)\n",
        "    if DO_TEST:\n",
        "        predictions = model(X_test[:1].astype('float32')).numpy()\n",
        "        print(\"Predictions: \" + str(predictions) + \"\\nWith Softmax: \" + str(tf.nn.softmax(predictions).numpy()) + \"\\nLoss: \" + str(loss_fn(y_test[:1], predictions).numpy()))\n",
        "\n",
        "    ### COMPILE MODEL\n",
        "    compile_model(model,optimizer,loss_fn,_verbose = True)\n",
        "\n",
        "    ### SETUP TENSORBOARD\n",
        "    tensorboard_callback = start_tensorboard(tb_dir,\"Fold_\"+str(foldcount) if is_k_fold else None)\n",
        "    callbacks=[tensorboard_callback,]\n",
        "    \n",
        "    ### SETUP EARLY STOPPING (only if not in K-fold mode)\n",
        "    if is_k_fold is False and use_early_stopping:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200))\n",
        "\n",
        "    # * FIT MODEL *\n",
        "    history = model.fit(X_train, y_train, epochs=args['epochs'], validation_data = (X_valid,y_valid),\n",
        "                        callbacks=callbacks,\n",
        "                        batch_size=args['batchsize'])\n",
        "    # Plot history\n",
        "    plot_folder = fold_dir if is_k_fold else current_dir\n",
        "    getval = lambda metric: history.history[metric]\n",
        "    plot_history(getval('accuracy'),getval('val_accuracy'), \"Training and validation accuracy\",\"Epochs\",\"Accuracy\", filename = plot_folder + \"/AccuracyPlot\")\n",
        "    plot_history(getval('loss'),    getval('val_loss'),     \"Training and validation loss\",\"Epochs\",\"Accuracy\",     filename = plot_folder + \"/LossPlot\")\n",
        "    plt.close()\n",
        "    plt.ioff()\n",
        "\n",
        "    # * TEST MODEL *\n",
        "    # keras_test_loss, keras_test_accuracy = model.evaluate(X_test,  y_test, verbose=2) # Keras solution, might not be needed\n",
        "    y_true = np.squeeze(y_test)\n",
        "    y_pred = np.argmax(model(X_test),axis=1)\n",
        "    cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, \\\n",
        "                                                                                                 y_pred, \\\n",
        "                                                                                                 _verbose=True)\n",
        "    eval_metrics[\"accuracy\"].append(cm_acc)\n",
        "    eval_metrics[\"f1_weightedmacroavg\"].append(f1mw)\n",
        "    eval_metrics[\"confusion_matrix\"].append(cm_conf_matrix)\n",
        "    eval_metrics[\"classification_report\"].append(cm_classf_report)\n",
        "    eval_metrics[\"printable_classification_report\"].append(cm_printable_classf_report)\n",
        "\n",
        "    SAVED_MODEL_PATH = None\n",
        "    if is_k_fold:\n",
        "        # Save fold history\n",
        "        with open(fold_dir+\"/history_fold_\"+str(foldcount)+\".pickle\",'wb') as picklefile:\n",
        "            pickle.dump(history.history,picklefile)\n",
        "\n",
        "        # Save the fold models only if we want them, or need them to test (in the second case they will be deleted after test)        \n",
        "        if TEST_QUANTIZATION or DO_SAVE_FOLD_MODELS:\n",
        "            model.save(fold_dir)\n",
        "            SAVED_MODEL_PATH = fold_dir\n",
        "    else:\n",
        "        assert len(eval_metrics['accuracy']) == 1\n",
        "        \n",
        "        # Save the entire model as a SavedModel.\n",
        "        if TEST_QUANTIZATION or DO_SAVE_FOLD_MODELS:\n",
        "            model.save(current_dir)\n",
        "            SAVED_MODEL_PATH = current_dir\n",
        "        with open(current_dir+\"/history.pickle\",'wb') as picklefile:\n",
        "            pickle.dump(history.history,picklefile)\n",
        "\n",
        "    if TEST_QUANTIZATION:\n",
        "        assert quantized_eval_metrics is not None\n",
        "        model_filename = 'partially_quantized_test_model'\n",
        "        # Convert and save lite model\n",
        "        convert2tflite(SAVED_MODEL_PATH,model_name=model_filename,quantization=\"dynamic\")\n",
        "        # Load and Test lite model\n",
        "        y_quant_pred = test_tflite_model(SAVED_MODEL_PATH+'/'+model_filename+'.tflite',X_test,y_test,type(model.layers[0]) == tf.keras.layers.Conv1D,verbose_test = False)\n",
        "        # Compute Test Metrics\n",
        "        cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, \\\n",
        "                                                                                                     y_quant_pred, \\\n",
        "                                                                                                     _verbose=True)\n",
        "        quantized_eval_metrics[\"accuracy\"].append(cm_acc)\n",
        "        quantized_eval_metrics[\"f1_weightedmacroavg\"].append(f1mw)\n",
        "        quantized_eval_metrics[\"confusion_matrix\"].append(cm_conf_matrix)\n",
        "        quantized_eval_metrics[\"classification_report\"].append(cm_classf_report)\n",
        "        quantized_eval_metrics[\"printable_classification_report\"].append(cm_printable_classf_report)\n",
        "\n",
        "    if not DO_SAVE_FOLD_MODELS:\n",
        "        if os.path.exists(os.path.join(fold_dir,'assets')):\n",
        "            shutil.rmtree(os.path.join(fold_dir,'assets'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'variables')):\n",
        "            shutil.rmtree(os.path.join(fold_dir,'variables'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'savedmodel.pb')):\n",
        "            os.remove(os.path.join(fold_dir,'savedmodel.pb'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'keras_metadata.pb')):\n",
        "            os.remove(os.path.join(fold_dir,'keras_metadata.pb'))\n",
        "        if TEST_QUANTIZATION and os.path.exists(os.path.join(fold_dir,'partially_quantized_test_model.tflite')):\n",
        "            os.remove(os.path.join(fold_dir,'partially_quantized_test_model.tflite'))\n",
        "\n",
        "    # Now that all the tests are performed, all the info can be saved\n",
        "\n",
        "    if is_k_fold:\n",
        "        metrics_to_save = {}\n",
        "        metrics_to_save.update({'def_model_'+key:value for (key,value) in eval_metrics.items()})\n",
        "        if TEST_QUANTIZATION:\n",
        "            metrics_to_save.update({'quant_model_'+key:value for (key,value) in quantized_eval_metrics.items()})\n",
        "\n",
        "        # save_fold_info(model,optimizer,foldcount,K_SPLITS,X_test.shape[0],eval_metrics,list(dataset_features.columns),fold_dir)\n",
        "        save_model_info(model,\n",
        "                        optimizer,\n",
        "                        False,K_SPLITS,\n",
        "                        metrics_to_save,\n",
        "                        fold_dir,\n",
        "                        fold_zerobased=foldcount-1,\n",
        "                        smote_strategy = smote_strategy)\n",
        "\n",
        "    SMOTE_STRATEGY = smote_strategy\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dwKI-vRUdDs0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-53fb0775db5c587a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-53fb0775db5c587a\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for split_idx, (train_idx, test_idx, val_idx) in enumerate(cv.split(dataset_features.to_numpy(), dataset_labels.to_numpy())):\n",
        "#     if USE_AUGMENTED_DATA:\n",
        "#         # Here we want to select the augmented data that corresponds to the current training split\n",
        "#         training_metadata = metadata.iloc[train_idx]\n",
        "#         training_aug_indices = []\n",
        "        \n",
        "#         sources_and_times_aug = metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].values\n",
        "#         training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "\n",
        "#         for idx, (source_aug, time_aug) in enumerate(sources_and_times_aug):\n",
        "#             if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "#                 training_aug_indices.append(idx)\n",
        "                \n",
        "# Faster code\n",
        "# for split_idx, (train_idx, test_idx, val_idx) in enumerate(cv.split(dataset_features.to_numpy(), dataset_labels.to_numpy())):\n",
        "#     training_aug_indices = []\n",
        "#     if USE_AUGMENTED_DATA:\n",
        "#         # Here we want to select the augmented data that corresponds to the current training split\n",
        "#         training_metadata = metadata.iloc[train_idx]\n",
        "#         training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "\n",
        "#         for idx,source_aug,time_aug in  metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].itertuples():\n",
        "#             if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "#                 training_aug_indices.append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "orQZbq4df5jK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting augmented data for fold 0 ...\n",
            "Found 5801 augmented samples for fold 0\n",
            "\n",
            "Fold [1/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 373, 1: 329, 2: 378, 3: 420}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  1500\n",
            "Length of training set after augmentation:  7301\n",
            "Created model: 'guitar_timbre_classifier_20221129-192409'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 87ms/step - loss: 2.2287 - accuracy: 0.2437 - val_loss: 1.4547 - val_accuracy: 0.2500\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.2187 - accuracy: 0.2558 - val_loss: 1.4481 - val_accuracy: 0.2500\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.2185 - accuracy: 0.2506 - val_loss: 1.4421 - val_accuracy: 0.2500\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.2013 - accuracy: 0.2535 - val_loss: 1.4364 - val_accuracy: 0.2500\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.2103 - accuracy: 0.2482 - val_loss: 1.4310 - val_accuracy: 0.2500\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.1994 - accuracy: 0.2466 - val_loss: 1.4260 - val_accuracy: 0.2500\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.2058 - accuracy: 0.2441 - val_loss: 1.4208 - val_accuracy: 0.2500\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.1814 - accuracy: 0.2512 - val_loss: 1.4153 - val_accuracy: 0.2500\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.2032 - accuracy: 0.2516 - val_loss: 1.4097 - val_accuracy: 0.2500\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2.1865 - accuracy: 0.2489 - val_loss: 1.4036 - val_accuracy: 0.2500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 1.0 * true_positives / np.sum(classPrediction)\n",
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.25\n",
            "Test macro_weighted_avg f1-score: nan\n",
            "[[ 0 18  0  0]\n",
            " [ 0 18  0  0]\n",
            " [ 0 18  0  0]\n",
            " [ 0 18  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.0000    0.0000    0.0000        18\n",
            "     Snare 1     0.2500    1.0000    0.4000        18\n",
            "         Tom     0.0000    0.0000    0.0000        18\n",
            "     Snare 2     0.0000    0.0000    0.0000        18\n",
            "\n",
            "    accuracy                         0.2500        72\n",
            "   macro avg     0.0625    0.2500    0.1000        72\n",
            "weighted avg     0.0625    0.2500    0.1000        72\n",
            "\n",
            "Selecting augmented data for fold 1 ...\n",
            "Found 5842 augmented samples for fold 1\n",
            "\n",
            "Fold [2/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 374, 1: 330, 2: 381, 3: 420}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  1505\n",
            "Length of training set after augmentation:  7347\n",
            "Created model: 'guitar_timbre_classifier_20221129-192411'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 38ms/step - loss: 1.5828 - accuracy: 0.3468 - val_loss: 1.3632 - val_accuracy: 0.2826\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5743 - accuracy: 0.3551 - val_loss: 1.3670 - val_accuracy: 0.2174\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5747 - accuracy: 0.3520 - val_loss: 1.3712 - val_accuracy: 0.2174\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5739 - accuracy: 0.3450 - val_loss: 1.3758 - val_accuracy: 0.1957\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5566 - accuracy: 0.3576 - val_loss: 1.3804 - val_accuracy: 0.1957\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5633 - accuracy: 0.3498 - val_loss: 1.3851 - val_accuracy: 0.2174\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.5595 - accuracy: 0.3583 - val_loss: 1.3904 - val_accuracy: 0.2174\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.5652 - accuracy: 0.3547 - val_loss: 1.3954 - val_accuracy: 0.2391\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.5542 - accuracy: 0.3550 - val_loss: 1.4001 - val_accuracy: 0.2174\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.5474 - accuracy: 0.3594 - val_loss: 1.4047 - val_accuracy: 0.2174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n",
            "/tmp/ipykernel_48979/2383121875.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 1.0 * true_positives / np.sum(classPrediction)\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.18840579710144928\n",
            "Test macro_weighted_avg f1-score: nan\n",
            "[[12  3  2  0]\n",
            " [ 5  1 12  0]\n",
            " [ 4 12  0  0]\n",
            " [18  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.3077    0.7059    0.4286        17\n",
            "     Snare 1     0.0625    0.0556    0.0588        18\n",
            "         Tom     0.0000    0.0000    0.0000        16\n",
            "     Snare 2     0.0000    0.0000    0.0000        18\n",
            "\n",
            "    accuracy                         0.1884        69\n",
            "   macro avg     0.0925    0.1904    0.1218        69\n",
            "weighted avg     0.0921    0.1884    0.1209        69\n",
            "\n",
            "Selecting augmented data for fold 2 ...\n",
            "Found 5843 augmented samples for fold 2\n",
            "\n",
            "Fold [3/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 383, 1: 330, 2: 378, 3: 420}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  1511\n",
            "Length of training set after augmentation:  7354\n",
            "Created model: 'guitar_timbre_classifier_20221129-192413'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 49ms/step - loss: 2.9960 - accuracy: 0.2133 - val_loss: 1.5954 - val_accuracy: 0.2727\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.9912 - accuracy: 0.2154 - val_loss: 1.5883 - val_accuracy: 0.2727\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.9820 - accuracy: 0.2161 - val_loss: 1.5822 - val_accuracy: 0.2727\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.9844 - accuracy: 0.2102 - val_loss: 1.5772 - val_accuracy: 0.2727\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.9886 - accuracy: 0.2087 - val_loss: 1.5733 - val_accuracy: 0.2727\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.9705 - accuracy: 0.2106 - val_loss: 1.5713 - val_accuracy: 0.2727\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.9727 - accuracy: 0.2096 - val_loss: 1.5713 - val_accuracy: 0.2727\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.9765 - accuracy: 0.2117 - val_loss: 1.5722 - val_accuracy: 0.2727\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.9582 - accuracy: 0.2163 - val_loss: 1.5719 - val_accuracy: 0.2727\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2.9843 - accuracy: 0.2121 - val_loss: 1.5716 - val_accuracy: 0.2727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 1.0 * true_positives / np.sum(classPrediction)\n",
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.26153846153846155\n",
            "Test macro_weighted_avg f1-score: nan\n",
            "[[ 0 12  0  0]\n",
            " [ 0 17  0  0]\n",
            " [ 0 18  0  0]\n",
            " [ 0 18  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.0000    0.0000    0.0000        12\n",
            "     Snare 1     0.2615    1.0000    0.4146        17\n",
            "         Tom     0.0000    0.0000    0.0000        18\n",
            "     Snare 2     0.0000    0.0000    0.0000        18\n",
            "\n",
            "    accuracy                         0.2615        65\n",
            "   macro avg     0.0654    0.2500    0.1037        65\n",
            "weighted avg     0.0684    0.2615    0.1084        65\n",
            "\n",
            "Selecting augmented data for fold 3 ...\n",
            "Found 5809 augmented samples for fold 3\n",
            "\n",
            "Fold [4/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 373, 1: 329, 2: 379, 3: 420}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  1501\n",
            "Length of training set after augmentation:  7310\n",
            "Created model: 'guitar_timbre_classifier_20221129-192415'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 30ms/step - loss: 2.5563 - accuracy: 0.1163 - val_loss: 1.4641 - val_accuracy: 0.2292\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 2.5487 - accuracy: 0.1145 - val_loss: 1.4649 - val_accuracy: 0.2083\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.5427 - accuracy: 0.1206 - val_loss: 1.4664 - val_accuracy: 0.2083\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.5328 - accuracy: 0.1170 - val_loss: 1.4687 - val_accuracy: 0.1875\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.5549 - accuracy: 0.1187 - val_loss: 1.4716 - val_accuracy: 0.1458\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.5655 - accuracy: 0.1165 - val_loss: 1.4751 - val_accuracy: 0.1042\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2.5572 - accuracy: 0.1110 - val_loss: 1.4794 - val_accuracy: 0.0625\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.5569 - accuracy: 0.1171 - val_loss: 1.4844 - val_accuracy: 0.0625\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2.5360 - accuracy: 0.1185 - val_loss: 1.4900 - val_accuracy: 0.0625\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2.5352 - accuracy: 0.1131 - val_loss: 1.4966 - val_accuracy: 0.0625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n",
            "/tmp/ipykernel_48979/2383121875.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 1.0 * true_positives / np.sum(classPrediction)\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.028169014084507043\n",
            "Test macro_weighted_avg f1-score: nan\n",
            "[[ 0 18  0  0]\n",
            " [ 6  2 10  0]\n",
            " [ 0 17  0  0]\n",
            " [ 6  4  8  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.0000    0.0000    0.0000        18\n",
            "     Snare 1     0.0488    0.1111    0.0678        18\n",
            "         Tom     0.0000    0.0000    0.0000        17\n",
            "     Snare 2     0.0000    0.0000    0.0000        18\n",
            "\n",
            "    accuracy                         0.0282        71\n",
            "   macro avg     0.0122    0.0278    0.0169        71\n",
            "weighted avg     0.0124    0.0282    0.0172        71\n",
            "\n",
            "Selecting augmented data for fold 4 ...\n",
            "Found 5811 augmented samples for fold 4\n",
            "\n",
            "Fold [5/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 376, 1: 329, 2: 378, 3: 420}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  1503\n",
            "Length of training set after augmentation:  7314\n",
            "Created model: 'guitar_timbre_classifier_20221129-192416'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 31ms/step - loss: 1.8238 - accuracy: 0.1262 - val_loss: 1.3953 - val_accuracy: 0.1489\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.8305 - accuracy: 0.1215 - val_loss: 1.3960 - val_accuracy: 0.1915\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.8325 - accuracy: 0.1243 - val_loss: 1.3982 - val_accuracy: 0.2340\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.8272 - accuracy: 0.1248 - val_loss: 1.4006 - val_accuracy: 0.2340\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.8243 - accuracy: 0.1269 - val_loss: 1.4031 - val_accuracy: 0.2340\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.8168 - accuracy: 0.1269 - val_loss: 1.4053 - val_accuracy: 0.2340\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.8311 - accuracy: 0.1284 - val_loss: 1.4075 - val_accuracy: 0.2553\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.8229 - accuracy: 0.1259 - val_loss: 1.4097 - val_accuracy: 0.2979\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.8203 - accuracy: 0.1272 - val_loss: 1.4118 - val_accuracy: 0.3404\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.8203 - accuracy: 0.1303 - val_loss: 1.4140 - val_accuracy: 0.3617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.4\n",
            "Test macro_weighted_avg f1-score: 0.3982154475322986\n",
            "[[ 7  1  8  0]\n",
            " [ 6 12  0  0]\n",
            " [ 0 18  0  0]\n",
            " [ 0  4  5  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.5385    0.4375    0.4828        16\n",
            "     Snare 1     0.3429    0.6667    0.4528        18\n",
            "         Tom     0.0000    0.0000    0.0000        18\n",
            "     Snare 2     1.0000    0.5000    0.6667        18\n",
            "\n",
            "    accuracy                         0.4000        70\n",
            "   macro avg     0.4703    0.4010    0.4006        70\n",
            "weighted avg     0.4684    0.4000    0.3982        70\n",
            "\n",
            "Selecting augmented data for fold 5 ...\n",
            "Found 2279 augmented samples for fold 5\n",
            "\n",
            "Fold [6/6]\n",
            "Selecting Split Data...\n",
            "Taking validation split from provided indexes\n",
            "sampling_strategy: {0: 136, 1: 148, 2: 146, 3: 150}\n",
            "Adding augmented data to training set...\n",
            "Length of training set before augmentation:  580\n",
            "Length of training set after augmentation:  2859\n",
            "Created model: 'guitar_timbre_classifier_20221129-192418'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 1s 151ms/step - loss: 2.3714 - accuracy: 0.1064 - val_loss: 1.6778 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 2.3703 - accuracy: 0.1091 - val_loss: 1.6770 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 2.3734 - accuracy: 0.1089 - val_loss: 1.6763 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 2.3782 - accuracy: 0.1081 - val_loss: 1.6755 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3685 - accuracy: 0.1030 - val_loss: 1.6747 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 2.3603 - accuracy: 0.1152 - val_loss: 1.6739 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 2.3946 - accuracy: 0.1062 - val_loss: 1.6731 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 2.3908 - accuracy: 0.1120 - val_loss: 1.6721 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 2.3728 - accuracy: 0.0992 - val_loss: 1.6708 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 2.3741 - accuracy: 0.1100 - val_loss: 1.6693 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.0\n",
            "Test macro_weighted_avg f1-score: 0.0\n",
            "[[  0 160   0   0]\n",
            " [  1   0   0 126]\n",
            " [136   5   0  16]\n",
            " [ 17 162   1   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Kick     0.0000    0.0000    0.0000     160.0\n",
            "     Snare 1     0.0000    0.0000    0.0000     127.0\n",
            "         Tom     0.0000    0.0000    0.0000     157.0\n",
            "     Snare 2     0.0000    0.0000    0.0000     180.0\n",
            "\n",
            "    accuracy                         0.0000     624.0\n",
            "   macro avg     0.0000    0.0000    0.0000     624.0\n",
            "weighted avg     0.0000    0.0000    0.0000     624.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''Call the main routine for each fold'''\n",
        "result_model = []\n",
        "train_aug_indexes = {}\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    metric_names = ['accuracy','f1_weightedmacroavg','confusion_matrix','classification_report','printable_classification_report']\n",
        "    evaluation_metrics, quantized_model_evaluation_metrics = {k:[] for k in metric_names}, {k:[] for k in metric_names}\n",
        "    \n",
        "    X,y = dataset_features.to_numpy(), dataset_labels.to_numpy()\n",
        "\n",
        "    if USE_CROSS_VALIDATION:\n",
        "        for foldidx,split_folds in enumerate(cv.split(X, y)): \n",
        "            if CUSTOM_PLAYER_K_FOLD:\n",
        "                train_idx, test_idx, val_idx  = split_folds\n",
        "            else:\n",
        "                train_idx, test_idx = split_folds\n",
        "                val_idx = None\n",
        "\n",
        "            train_aug_idx = []  # Indexes of the augmented data that correspond to the current training split\n",
        "            if USE_AUGMENTED_DATA: # Here we want to select the augmented data that corresponds to the current training split\n",
        "                print('Selecting augmented data for fold',foldidx,'...')\n",
        "                training_metadata = metadata.iloc[train_idx]\n",
        "                training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "                \n",
        "                for idx,source_aug,time_aug in  metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].itertuples():\n",
        "                    if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "                        train_aug_idx.append(idx)\n",
        "                assert len(train_aug_idx) > 0, \"No augmented data found for the current training split\"\n",
        "                print('Found',len(train_aug_idx),'augmented samples for fold',foldidx)\n",
        "            train_aug_indexes[foldidx] = train_aug_idx\n",
        "\n",
        "            result_model.append(main_routine(X, y,\n",
        "                                             train_idx = train_idx,\n",
        "                                             test_idx  = test_idx,\n",
        "                                             val_idx   = val_idx,\n",
        "                                             aug_data = None if not USE_AUGMENTED_DATA else (metadata_aug, features_aug, labels_aug, train_aug_idx),\n",
        "                                             foldcount = foldidx+1,\n",
        "                                             is_k_fold = USE_CROSS_VALIDATION,\n",
        "                                             eval_metrics           = evaluation_metrics,\n",
        "                                             quantized_eval_metrics = quantized_model_evaluation_metrics))        \n",
        "    else:\n",
        "        result_model = main_routine(X, y,\n",
        "                                    eval_metrics           = evaluation_metrics,\n",
        "                                    quantized_eval_metrics = quantized_model_evaluation_metrics,\n",
        "                                    _val_split_size        = val_split_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLTufCRmlpIW"
      },
      "source": [
        "# Cross Validation average results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqw9k1wlWZC"
      },
      "source": [
        "## Utilities for reports and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "LvpD2bFcYcCi"
      },
      "outputs": [],
      "source": [
        "def report_average(reports):\n",
        "    mean_dict = dict()\n",
        "    for label in reports[0].keys():\n",
        "        dictionary = dict()\n",
        "\n",
        "        if label in 'accuracy':\n",
        "            mean_dict[label] = sum(d[label] for d in reports) / len(reports)\n",
        "            continue\n",
        "\n",
        "        for key in reports[0][label].keys():\n",
        "            dictionary[key] = sum(d[label][key] for d in reports) / len(reports)\n",
        "        mean_dict[label] = dictionary\n",
        "\n",
        "    return mean_dict\n",
        "\n",
        "def classification_report_dict2print(report):\n",
        "    ret = \"\"\n",
        "    classes = list(report.keys())[0:-3]\n",
        "    summary_metrics = list(report.keys())[-3:]\n",
        "    longest_1st_column_name = max([len(key) for key in report.keys()])\n",
        "    ret = ' ' * longest_1st_column_name\n",
        "    ret += '  precision    recall  f1-score   support\\n\\n'\n",
        "\n",
        "    METRIC_DECIMAL_DIGITS = 4\n",
        "    metric_digits = METRIC_DECIMAL_DIGITS + 2 # add 0 and dot\n",
        "\n",
        "    header_spacing = 1\n",
        "    metrics = list(report[classes[0]].keys())\n",
        "    longest_1st_row_name = max([len(key) for key in report[classes[0]].keys()]) + header_spacing\n",
        "\n",
        "    for classname in classes:\n",
        "        ret += (' '*(longest_1st_column_name-len(classname))) + classname + ' '\n",
        "        for metric in metrics:\n",
        "            if metric != \"support\":\n",
        "                ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "                ret += \"%.4f\" % round(report[classname][metric],METRIC_DECIMAL_DIGITS)\n",
        "            else:\n",
        "                current_support_digits = len(str(int(report[classname][metric])))\n",
        "                ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "                ret += \"%d\" % round(report[classname][metric],0)\n",
        "        ret += '\\n'\n",
        "    ret += '\\n'\n",
        "\n",
        "    # Accuracy\n",
        "    ret += (' '*(longest_1st_column_name-len(summary_metrics[0]))) + summary_metrics[0] + ' '\n",
        "    ret += 2* (' '*longest_1st_row_name)\n",
        "    ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "    ret += \"%.4f\" % round(report[\"accuracy\"],METRIC_DECIMAL_DIGITS)\n",
        "    current_support_digits = len(str(int(report[summary_metrics[-1]]['support'])))\n",
        "    ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "    ret += \"%d\" % round(report[summary_metrics[-1]]['support'],0)\n",
        "    ret += '\\n'\n",
        "  \n",
        "  \n",
        "    for classname in summary_metrics[1:]:\n",
        "        ret += (' '*(longest_1st_column_name-len(classname))) + classname + ' '\n",
        "        for metric in metrics:\n",
        "            if metric != \"support\":\n",
        "                ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "                ret += \"%.4f\" % round(report[classname][metric],METRIC_DECIMAL_DIGITS)\n",
        "            else:\n",
        "                current_support_digits = len(str(int(report[classname][metric])))\n",
        "                ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "                ret += \"%d\" % round(report[classname][metric],0)\n",
        "        ret += '\\n'\n",
        "    ret += '\\n'\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vw1nmcpmApM"
      },
      "source": [
        "## Compute average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "S-xQAAnQu6BO"
      },
      "outputs": [],
      "source": [
        "if USE_CROSS_VALIDATION:\n",
        "    assert len(evaluation_metrics['accuracy']) == K_SPLITS, \"The number of accuracy values does not match the number of folds ({} != {})\".format(len(evaluation_metrics['accuracy']),K_SPLITS)\n",
        "    \n",
        "    printable_avg_report = classification_report_dict2print(report_average(evaluation_metrics[\"classification_report\"]))\n",
        "    qm_printable_avg_report = \"Not performed\"\n",
        "    if TEST_QUANTIZATION:\n",
        "        qm_printable_avg_report = classification_report_dict2print(report_average(quantized_model_evaluation_metrics[\"classification_report\"]))\n",
        "    metrics_to_save = {\"avg_classification_report\" : printable_avg_report, \"avg_classification_report_for_quantized_model\" : qm_printable_avg_report}\n",
        "else:\n",
        "    assert len(evaluation_metrics['accuracy']) == 1\n",
        "    metrics_to_save = {}\n",
        "    for metric in evaluation_metrics.keys():\n",
        "        metrics_to_save[metric] = evaluation_metrics[metric][0]\n",
        "    for metric in quantized_model_evaluation_metrics.keys():\n",
        "        metrics_to_save['quantizedmod_'+str(metric)] = quantized_model_evaluation_metrics[metric][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sYtK9DZu64h"
      },
      "source": [
        "# Save Model Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "JNDoHM4gXw5x"
      },
      "outputs": [],
      "source": [
        "if SAVE_MODEL_INFO:\n",
        "    current_dir = MODELFOLDER + \"/\" + RUN_NAME\n",
        "    # %mkdir -p \"$current_dir\"\n",
        "    os.makedirs(current_dir,exist_ok = True)\n",
        "\n",
        "    save_model_info(result_model[0] if type(result_model) == list else result_model,\n",
        "                    optimizer,\n",
        "                    USE_CROSS_VALIDATION,K_SPLITS,\n",
        "                    metrics_to_save,\n",
        "                    current_dir)\n",
        "else:\n",
        "    print(\"RESULTS\\n\\n\" + metrics_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7ZdTjKVcE3"
      },
      "source": [
        "# Train final model on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "RT6_bSsfVbiH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created model: 'guitar_timbre_classifier_20221129-192422'\n",
            "Model compiled\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 1.6527 - accuracy: 0.1805\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.6358 - accuracy: 0.1845\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.6515 - accuracy: 0.1827\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.6480 - accuracy: 0.1698\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 1.6522 - accuracy: 0.1889\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.6472 - accuracy: 0.1884\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.6416 - accuracy: 0.1867\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.6437 - accuracy: 0.1762\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.6488 - accuracy: 0.1775\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.6357 - accuracy: 0.1965\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac758366a0>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_early_stopping = False\n",
        "\n",
        "### DEFINE MODEL\n",
        "final_model = define_model_architecture(len(CLASSES),_verbose = True)\n",
        "loss_fn = get_loss()\n",
        "\n",
        "### PREPARE DATA IN CASE OF A FIRST CONV LAYER IN THE NET\n",
        "if type(final_model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "    X_all = np.expand_dims(X,axis = 2) # Adapt data for Conv1d ([batch_shape, steps, input_dim] -> in our case indim = 1, steps = features, batchshape = train datset size)\n",
        "else:\n",
        "    X_all = X\n",
        "\n",
        "### COMPILE MODEL\n",
        "compile_model(final_model,optimizer,loss_fn,_verbose = True)\n",
        "\n",
        "### SETUP TENSORBOARD\n",
        "tensorboard_callback = start_tensorboard(tb_dir,None)\n",
        "callbacks=[tensorboard_callback,]\n",
        "\n",
        "### SETUP EARLY STOPPING (only if not in K-fold mode)\n",
        "if use_early_stopping:\n",
        "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='min', verbose=1, patience=200))\n",
        "\n",
        "# * FIT MODEL *\n",
        "final_model.fit(X_all, y, epochs=args['epochs'],\n",
        "                callbacks=callbacks,\n",
        "                batch_size=args['batchsize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "LXJ9wBMbb4sp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./output/CrossValidatedRun_20221129-192400/finalModel/assets\n"
          ]
        }
      ],
      "source": [
        "final_model_dir = MODELFOLDER + \"/\" + RUN_NAME + \"/finalModel\"\n",
        "# %mkdir -p \"$final_model_dir\"\n",
        "os.makedirs(final_model_dir,exist_ok = True)\n",
        "\n",
        "final_model.save(final_model_dir)\n",
        "\n",
        "# Convert and save lite model (Non quantized)\n",
        "convert2tflite(final_model_dir,model_name='final_model',quantization=None)\n",
        "# Convert and save lite model (Dynamically quantized)\n",
        "convert2tflite(final_model_dir,model_name='final_model_dynquant',quantization=\"dynamic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pBrSDphxyxL"
      },
      "source": [
        "# Save the model for TF Lite\n",
        "## *(Only if not a Cross Validated run)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "k3ScdRpC-m_J"
      },
      "outputs": [],
      "source": [
        "# if USE_CROSS_VALIDATION is False:\n",
        "#     model_path = MODELFOLDER + \"/\" + RUN_NAME\n",
        "#     convert2tflite(model_path)                                                # standard TFLITE model\n",
        "#     convert2tflite(model_path,model_name=\"model_partially_quantized\",quantization=\"dynamic\")   # Partial quantization  https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization\n",
        "    \n",
        "#     quantization_dataset = X\n",
        "#     if type(result_model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "#         quantization_dataset = np.expand_dims(X,axis = 2) # Adapt data for Conv1d\n",
        "    \n",
        "#     convert2tflite(model_path,model_name=\"model_float_fallback\",quantization=\"float-fallback\",dataset=quantization_dataset) # https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_float_fallback_quantization\n",
        "#     convert2tflite(model_path,model_name=\"model_fully_quantized\",quantization=\"full\",dataset=quantization_dataset)          # FULL uint8 quantization https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dnPqZ8k4box0"
      },
      "outputs": [],
      "source": [
        "# first_layer_is_conv = (type(result_model.layers[0]) == tf.keras.layers.Conv1D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3QdCLmkgzonI"
      },
      "outputs": [],
      "source": [
        "# TEST_SAVED_MODEL = None\n",
        "# # TEST_SAVED_MODEL = 'model.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_partially_quantized.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_float_fallback.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_fully_quantized.tflite'\n",
        "# # TEST_SAVED_MODEL = 'quant_aware_model.tflite'\n",
        "# # TEST_SAVED_MODEL = 'saved_model.pb'\n",
        "# verbose_test = False\n",
        "\n",
        "# def test_generic_model(model_filename,model_path,X_test,Y_test,first_layer_is_conv,verbose_test = False):\n",
        "#     if model_filename.split('.')[-1] == 'tflite':\n",
        "#         y_pred = test_tflite_model(model_path+'/'+model_filename,X_test,y_test,first_layer_is_conv,verbose_test = verbose_test)\n",
        "#         correct = np.count_nonzero((np.array(y_pred) == np.ravel(y_test)).astype(int))\n",
        "#         total = np.shape(y_test)[0]\n",
        "#         accuracy = round(correct/total,4)\n",
        "#     elif model_filename.split('.')[-1] == 'pb':\n",
        "#         accuracy = test_regulartf_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = verbose_test)\n",
        "#     else:\n",
        "#         raise ValueError(\"\")\n",
        "\n",
        "#     return accuracy\n",
        "\n",
        "# if USE_CROSS_VALIDATION is False and TEST_SAVED_MODEL is not None:\n",
        "#     assert np.max([len(ev_metric) for ev_metric in evaluation_metrics]) == K_SPLITS\n",
        "\n",
        "#     target_accuracy = evaluation_metrics['accuracy'][0]\n",
        "#     accuracy = test_generic_model(TEST_SAVED_MODEL,model_path,X_test,Y_test,first_layer_is_conv)\n",
        "\n",
        "#     epsilon = 1e-4\n",
        "#     EQUAL_ACCURACY = abs(target_accuracy - accuracy) < epsilon\n",
        "\n",
        "#     print(\"accuracy: \" + str(accuracy))\n",
        "\n",
        "#     if EQUAL_ACCURACY:\n",
        "#         print(\"Accuracy of the original model and the saved TF model correspond(on same test set)\")\n",
        "#     else:\n",
        "#         raise ValueError('Accuracy does not match target (Target: '+str(target_accuracy)+' but got '+str(accuracy)+' instead)')\n",
        "# else:\n",
        "#     print(\"TF model testing is disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERYDqiDVXUqX"
      },
      "source": [
        "# Quantization aware fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "n2QJCdx9HOfW"
      },
      "outputs": [],
      "source": [
        "# #################################################\n",
        "# PERFORM_QUANZATION_AWARE_TRAINING = False       #\n",
        "# #################################################\n",
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     pip_install('tensorflow_model_optimization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "E5W8KuwyXTsX"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     imported_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "#     import tensorflow_model_optimization as tfmot\n",
        "\n",
        "#     quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "#     # q_aware stands for for quantization aware.\n",
        "#     q_aware_model = None\n",
        "#     q_aware_model = quantize_model(imported_model)\n",
        "\n",
        "#     # `quantize_model` requires a recompile.\n",
        "#     _,loss_fn = define_model_architecture(len(CLASSES),_verbose = True)  # Get only the loss function\n",
        "#     compile_model(q_aware_model,optimizer,loss_fn,_verbose = True)  # Recompile the quantization aware model\n",
        "\n",
        "#     q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_Pb9pAqTA0h5"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     tb_dir = \"logs2/fit/\"\n",
        "#     %tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "WQPy1nbXtWto"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     finetuning_epochs = 50\n",
        "#     tensorboard_callback = start_tensorboard(tb_dir,None)\n",
        "\n",
        "#     q_history = q_aware_model.fit(X_train, y_train, epochs=finetuning_epochs, validation_data = (X_valid,y_valid),\n",
        "#                                 callbacks=[tensorboard_callback],\n",
        "#                                 batch_size=args['batchsize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "jIbbjlPRx1bx"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     quant_model_path = MODELFOLDER + \"/\" + RUN_NAME + \"/quant_aware_model.tflite\"\n",
        "\n",
        "#     converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "#     converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "#     quantized_tflite_model = converter.convert()\n",
        "\n",
        "#     with tf.io.gfile.GFile(quant_model_path, 'wb') as f:\n",
        "#         f.write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynglS4cE6uFi"
      },
      "source": [
        "# Rename all output folders by prefixing the accuracy value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "gIB4EF5l6uFi"
      },
      "outputs": [],
      "source": [
        "# from glob import glob\n",
        "# import re\n",
        "\n",
        "# for dirpattern, newprefix in [('SingleRun*','acc'),('CrossValidated*','c_acc')]:\n",
        "#     for resdir in glob(os.path.join(MODELFOLDER,dirpattern)):\n",
        "#         filesindir = glob(os.path.join(resdir,'*'))\n",
        "#         if len(filesindir) == 0:\n",
        "#             os.rename(resdir,os.path.join(MODELFOLDER,'todelete',os.path.basename(resdir)))\n",
        "#         else:\n",
        "#             if os.path.exists(os.path.join(resdir,'info.txt')):\n",
        "#                 with open(os.path.join(resdir,'info.txt')) as infof:\n",
        "#                     acclines = []\n",
        "#                     for e in infof.readlines():\n",
        "#                         acclines += re.findall('accuracy[ ]+0\\.\\d+',e)\n",
        "#                     assert len(acclines) > 0\n",
        "#                     accstr = re.findall('0\\.\\d+',acclines[0])\n",
        "#                     assert len(accstr) == 1\n",
        "#                     newfoldername = os.path.join(os.path.dirname(resdir),newprefix+accstr[0]+'_'+os.path.basename(resdir))\n",
        "#                     os.rename(resdir,newfoldername)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVw_zjlf6uFi"
      },
      "source": [
        "## Rename current output folder by prefixing the accuracy value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rzJLnKEZ6uFi"
      },
      "outputs": [],
      "source": [
        "run_dir = os.path.join(MODELFOLDER,RUN_NAME)\n",
        "assert os.path.exists(run_dir)\n",
        "\n",
        "if os.path.exists(os.path.join(run_dir,'info.txt')):\n",
        "    with open(os.path.join(run_dir,'info.txt')) as infof:\n",
        "        acclines = []\n",
        "        for e in infof.readlines():\n",
        "            acclines += re.findall('accuracy[ ]+0\\.\\d+',e)\n",
        "        assert len(acclines) > 0\n",
        "        accstr = re.findall('0\\.\\d+',acclines[0])\n",
        "        assert len(accstr) == 1\n",
        "        if 'CrossValidated' in os.path.basename(run_dir):\n",
        "            newprefix = 'c_acc'\n",
        "        elif 'SingleRun' in os.path.basename(run_dir):\n",
        "            newprefix = 'acc'\n",
        "        else:\n",
        "            raise Exception('Bad folder name '+str(os.path.basename(run_dir)))\n",
        "\n",
        "        newprefix = classification_task.value[1] + '_' + newprefix\n",
        "            \n",
        "        newfoldername = os.path.join(os.path.dirname(run_dir),newprefix+accstr[0]+'_'+os.path.basename(run_dir))\n",
        "        # print('Renaming \"'+run_dir+'\" to \"'+newfoldername+'\"')\n",
        "        os.rename(run_dir,newfoldername)\n",
        "        run_dir = newfoldername\n",
        "else:\n",
        "    errfoldername = os.path.join(os.path.dirname(run_dir),'ERR_'+os.path.basename(run_dir))\n",
        "    os.rename(run_dir,errfoldername)\n",
        "    run_dir = errfoldername"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAgndl_b6uFi"
      },
      "source": [
        "## Testing with extra test data\n",
        "This data was extracted from extra recordings, made to test the system in a real life scenario.  \n",
        "Here we test only to veryfy that everything is working here, before making a shift to the real life test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "9t0N81FrWSST"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 32\n",
            "drwxrwxr-x 3 cimil-01 cimil-01 4096 nov 28 15:03 .\n",
            "drwxrwxr-x 3 cimil-01 cimil-01 4096 nov 28 15:01 ..\n",
            "lrwxrwxrwx 1 cimil-01 cimil-01  118 ott 11 11:12 20220929_115154_onlycorrectdetections.pickle -> /home/cimil-01/Develop/expressive_guitar_techniques_dataset/data/features/20220929_115154_onlycorrectdetections.pickle\n",
            "lrwxrwxrwx 1 cimil-01 cimil-01  123 ott 11 11:12 20221011_110715_test_onlycorrectdetections.pickle -> /home/cimil-01/Develop/expressive_guitar_techniques_dataset/data/features/20221011_110715_test_onlycorrectdetections.pickle\n",
            "drwxrwxr-x 2 cimil-01 cimil-01 4096 nov 28 13:28 augmented_data\n",
            "-rw-rw-r-- 1 cimil-01 cimil-01 8451 ott 10 19:38 relief_cache.pickle\n"
          ]
        }
      ],
      "source": [
        "THEPATH = os.path.join(DATAFOLDER)\n",
        "!ls -la \"$THEPATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "sdhiK3rD6uFj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data from pickle...\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Load the test data \"\"\"\n",
        "\n",
        "TEST_DATA_FILE_PATH = os.path.join(DATAFOLDER,'20221011_110715_test_onlycorrectdetections.pickle')\n",
        "print(\"Loading test data from pickle...\")\n",
        "with open(TEST_DATA_FILE_PATH,'rb') as pf:\n",
        "    testdataset = pickle.load(pf)\n",
        "testdataset.sort_values(['meta_expressive_technique_id','meta_audiofilePath'],inplace = True)\n",
        "print('Done.')\n",
        "# If this fails, the dataset has changed from the last time the program was run successfully (CHECK THE DATA!!!)\n",
        "assert testdataset.shape == (754,507)\n",
        "# display(testdataset)\n",
        "\n",
        "\n",
        "\"\"\" Drop unused features (like the train/test dataset) \"\"\"\n",
        "\n",
        "drop_unused_features(testdataset,inplace=True)\n",
        "assert testdataset.shape == (754,504)\n",
        "\n",
        "\n",
        "\"\"\" Divide the test data into metadata, features and labels (like the train/test dataset) \"\"\"\n",
        "\n",
        "test_metadata, test_features, test_labels = divide_dataset(testdataset)\n",
        "assert test_metadata.shape[0] == test_features.shape[0] == test_labels.shape[0] == 754\n",
        "assert test_metadata.shape[1] == 9\n",
        "assert test_features.shape[1] == 495\n",
        "\n",
        "\"\"\" Filter the dataset according to the task \"\"\"\n",
        "# This might mean removing samples or renaming classes\n",
        "test_features,test_labels,test_metadata = filter_dataset(test_features.copy(),test_labels.copy(),test_metadata.copy(),classification_task)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Apply the feature selection computed for the train/test set (like the train/test dataset) \"\"\"\n",
        "\n",
        "test_features = test_features.copy().loc[:,selected_features]\n",
        "\n",
        "if len(selected_features) != AUTO_FEATURE_NUMBER:\n",
        "    raise Exception('The number of selected_features ('+str(len(selected_features))+') is not the same as AUTO_FEATURE_NUMBER ('+str(AUTO_FEATURE_NUMBER)+'). Check the code.')\n",
        "\n",
        "if test_features.shape[1] != AUTO_FEATURE_NUMBER:\n",
        "    raise Exception('The number of features in the test dataset ('+str(test_features.shape[1])+') is different from the number of features in the train/test dataset ('+str(AUTO_FEATURE_NUMBER)+')')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "XskezstrklRC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48979/2383121875.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  f1score = 2 /((1/precision)+(1/recall))\n",
            "/tmp/ipykernel_48979/2383121875.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 1.0 * true_positives / np.sum(classPrediction)\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/cimil-01/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "extra_test_x = test_features.to_numpy()\n",
        "extra_test_y = test_labels.to_numpy()\n",
        "\n",
        "y_true = np.squeeze(extra_test_y)\n",
        "y_pred = np.argmax(final_model(extra_test_x),axis=1)\n",
        "cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, y_pred, _verbose=False)\n",
        "\n",
        "with open(os.path.join(run_dir,'info.txt'),'a') as infof:\n",
        "    infof.write('______________________________________________________________________________________________________________________________________________________\\n\\n\\n')\n",
        "    infof.write('+----------------------------------------------------------------+\\n')\n",
        "    infof.write('| Results obtained on extra test recordings with the FINAL MODEL |\\n')\n",
        "    infof.write('+----------------------------------------------------------------+\\n\\n')\n",
        "    infof.write('Extra-test-Accuracy: '+str(cm_acc)+'\\n\\n')\n",
        "    infof.write('Extra-test-F1 Score (weighted average): '+str(f1mw)+'\\n\\n')\n",
        "    infof.write('Extra-test-ConfusionMatrix: \\n'+str(cm_conf_matrix)+'\\n\\n')\n",
        "    infof.write('Extra-test-Report: \\n'+str(cm_printable_classf_report)+'\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Jp5iv_0pklRC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*--* Training successfully completed. *--*\n",
            "Data at ./output/perc_c_acc0.1880_CrossValidatedRun_20221129-192400\n"
          ]
        }
      ],
      "source": [
        "print('*--* Training successfully completed. *--*')\n",
        "print(\"Data at\",run_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uLTufCRmlpIW",
        "4pBrSDphxyxL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('tf-CLONE')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "959cb910f4c92fed33bcbb2d615fe10bd0473a33e22a9b4e00ebf3acf1e03643"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
