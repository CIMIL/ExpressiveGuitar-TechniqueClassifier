{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAJRnwSBQNbQ"
      },
      "source": [
        "# Expressive Guitar Technique classifier\n",
        "Ph.D. research project of [Domenico Stefani](work.domenicostefani.com)  \n",
        "This notebook loads a dataset of feature vectors extracted from **pitched** and **percussive** sounds recorded with many acoustic guitars.\n",
        "The techniques/classes recorded are:  \n",
        "\n",
        "0.    **Kick**      (Palm on lower body)\n",
        "1.    **Snare 1**   (All fingers on lower side)\n",
        "2.    **Tom**       (Thumb on higher body)\n",
        "3.    **Snare 2**   (Fingers on the muted strings, over the end\n",
        "of the fingerboard)\n",
        "___\n",
        "4.    **Natural Harmonics** (Stop strings from playing the dominant frequency, letting harmonics ring)\n",
        "5.    **Palm Mute** (Muting partially the strings with the palm\n",
        "of the pick hand)\n",
        "6.    **Pick Near Bridge** (Playing toward the bridge/saddle)\n",
        "7.    **Pick Over the Soundhole** (Playing over the sound hole)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2ha_JDqV9I"
      },
      "source": [
        "## Import modules and mount drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPrPhmjHk-kc"
      },
      "outputs": [],
      "source": [
        "# Choose ClassificationTask task\n",
        "from enum import Enum\n",
        "class ClassificationTask(Enum):\n",
        "    FULL_8_CLASS_PROBLEM,BINARY_PERCUSSIVE_PITCHED,PERCUSSIVE_4_ONLY,PITCHED_4_ONLY,PERCUSSIVE_PLUS_PITCHED_CLASS,ONE_GUITARIST_FULL = ((1,'full'), (2,'binary'), (3,'perc'), (4,'pitch'), (5,'perc+pitch'), (6,'one-guit-full'))\n",
        "class FeatureSelection(Enum):\n",
        "    NONE,MANUAL_VARIABLES,MANUAL_LIST,AUTO_ANOVA,AUTO_RELIEF = (0, 1, 2, 3, 4)\n",
        "class FeatureWindowSize(Enum):\n",
        "    s4800_SAMPLES_100ms, s704_Samples_14ms, _704windowed, _2112windowed, _3456windowed, _4800windowed = (1, 2,3,4,5,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_task = ClassificationTask.FULL_8_CLASS_PROBLEM\n",
        "# classification_task = ClassificationTask.BINARY_PERCUSSIVE_PITCHED\n",
        "# classification_task = ClassificationTask.PERCUSSIVE_4_ONLY\n",
        "# classification_task = ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS\n",
        "# classification_task = ClassificationTask.ONE_GUITARIST_FULL\n",
        "\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_VARIABLES\n",
        "# FEATURE_SELECTION = FeatureSelection.MANUAL_LIST\n",
        "FEATURE_SELECTION = FeatureSelection.AUTO_ANOVA #ANOVA: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
        "# FEATURE_SELECTION = FeatureSelection.AUTO_RELIEF\n",
        "\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize.s4800_SAMPLES_100ms\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize.s704_Samples_14ms\n",
        "FEATURE_WINDOW_SIZE = FeatureWindowSize._704windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._2112windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._3456windowed\n",
        "# FEATURE_WINDOW_SIZE = FeatureWindowSize._4800windowed\n",
        "\n",
        "\n",
        "# DROP_ADDITIONAL_CEPSTRUM_FROM_BIG_WINDOW = True\n",
        "\n",
        "# SCALER_TO_USE = 'StandardScaler'\n",
        "SCALER_TO_USE = 'MinMaxScaler'\n",
        "\n",
        "#import sklearn scalers\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "if SCALER_TO_USE == 'StandardScaler':\n",
        "    SCALER_TO_USE = StandardScaler()\n",
        "elif SCALER_TO_USE == 'MinMaxScaler':\n",
        "    SCALER_TO_USE = MinMaxScaler()\n",
        "\n",
        "TRAIN_FINAL_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuprRjSbkuh3"
      },
      "outputs": [],
      "source": [
        "# Install module for the ReliefF feature selection\n",
        "# !pip install skrebate\n",
        "# !pip install tensorboard\n",
        "# !pip3 install pickle5\n",
        "# !pip3 install tensorflow==2.4.1\n",
        "# !pip3 install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIRE_GPU = False\n",
        "DO_SAVE_TENSORBOARD_LOGS = False \n",
        "DO_SAVE_FOLD_MODELS = False \n",
        "CUSTOM_PLAYER_K_FOLD = True         # Very important, this ditches the k-fold stratified random shuffle, and creates as many splits as the guitar players, separating natural groups\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS = True # If true, drop the data from files that have 'extra' in the filename, which otherwise make the dataset unbalanced\n",
        "\n",
        "\n",
        "USE_TENSORBOARD = False\n",
        "\n",
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    USE_AUGMENTED_DATA = True\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "    \n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "    USE_AUGMENTED_DATA = False\n",
        "else:\n",
        "    raise ValueError('Invalid feature window size')\n",
        "# USE_AUGMENTED_DATA = False\n",
        "DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG = False\n",
        "\n",
        "DO_NORMALIZE_DATA = True\n",
        "\n",
        "DO_NORMALIZE_FOR_FEATURE_SELECTION = True\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if USE_TENSORBOARD:\n",
        "    %load_ext tensorboard\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "from sys import executable as sys_executable\n",
        "from sys import argv as sys_argv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import strftime, time\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from bz2 import BZ2File # To open compressed data\n",
        "import re\n",
        "import shutil\n",
        "import imblearn\n",
        "from sklearn.metrics import confusion_matrix as sk_conf_matrix\n",
        "from sklearn.metrics import classification_report as sk_class_report\n",
        "\n",
        "print(\"Tensorflow version: \" + tf.version.VERSION)\n",
        "print('Imblearn version:',imblearn.__version__)\n",
        "\n",
        "global_random_state = 42\n",
        "np.random.seed(global_random_state)\n",
        "tf.random.set_seed(global_random_state)\n",
        "\n",
        "COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if COLAB:\n",
        "    print('Running on CoLab')\n",
        "    #Connect and mount the drive folder that contains the train dataset and the output folder\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "    HOMEBASE = os.path.join('/content','gdrive','MyDrive','dottorato','Publications','02-IEEE-RTEmbeddedTimbreClassification(submitted)','Classifier')\n",
        "    THISDIR = \"/content/\"\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    HOMEBASE = \".\"\n",
        "    THISDIR = \"./\"\n",
        "DATAFOLDER = os.path.join(HOMEBASE,\"data/phase3\")\n",
        "MODELFOLDER = os.path.join(HOMEBASE,\"output\")\n",
        "\n",
        "RELIEF_CACHE_FILEPATH = os.path.join(DATAFOLDER,'relief_cache.pickle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY_34JvX6uFT"
      },
      "outputs": [],
      "source": [
        "def is_notebook() -> bool:\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7qyEuAk6uFV"
      },
      "source": [
        "## Enforce GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BxHBUDQPXKS"
      },
      "outputs": [],
      "source": [
        "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "print(physical_devices)\n",
        "if REQUIRE_GPU:\n",
        "  assert len(tf.config.experimental.list_physical_devices('GPU')) >= 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZMbpx2eM2G"
      },
      "source": [
        "## Check Real avaliable GRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lSUK12K6uFW"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def pip_install(package):\n",
        "    subprocess.check_call([executable, \"-m\", \"pip\", \"install\", package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_8hnGH7eL_T"
      },
      "outputs": [],
      "source": [
        "CHECK_GRAM = False\n",
        "\n",
        "if CHECK_GRAM:\n",
        "    # memory footprint support libraries/code\n",
        "    os.symlink('/opt/bin/nvidia-smi','/usr/bin/nvidia-smi')\n",
        "    pip_install('gputil')\n",
        "    pip_install('psutil')\n",
        "    pip_install('humanize')\n",
        "    import psutil\n",
        "    import humanize\n",
        "    import os\n",
        "    import GPUtil as GPU\n",
        "    GPUs = GPU.getGPUs()\n",
        "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "    gpu = GPUs[0]\n",
        "    def printm():\n",
        "        process = psutil.Process(os.getpid())\n",
        "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "    printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2hdmJnSsEOM"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CozyQAQgznvK"
      },
      "outputs": [],
      "source": [
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    DATASET_FILENAME = 'onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221206-165551_SUPERLONGdataset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    DATASET_FILENAME = 'onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20221201-182312_REALTIMEdataset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-141803_windowed704-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-141803_windowed2112-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-141803_windowed3456-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "    DATASET_FILENAME = 'paper-onlycorrectdetections_extraction-outputPROCESSED_FEATURES_20230119-105343_windowed4800-mainset-phase3PROCESSED_FEATURES.pickle'\n",
        "else:\n",
        "    raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "\n",
        "print('Loading dataset from file:',DATASET_FILENAME)\n",
        "\n",
        "\n",
        "if os.path.splitext(DATASET_FILENAME)[1] == '.bz2':\n",
        "    print(\"Reading dataset from compressed pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,DATASET_FILENAME)\n",
        "    startime = time()\n",
        "    ifile = BZ2File(DATASET_PATH,'rb')\n",
        "    featuredataset = pickle.load(ifile)\n",
        "    ifile.close()\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from compressed pickle' % (time()-startime))\n",
        "elif os.path.splitext(DATASET_FILENAME)[1] == '.pickle':\n",
        "    print(\"Reading dataset from pickle...\")\n",
        "    DATASET_PATH = os.path.join(DATAFOLDER,DATASET_FILENAME)\n",
        "    startime = time()\n",
        "    with open(DATASET_PATH,'rb') as pf:\n",
        "        featuredataset = pickle.load(pf)\n",
        "    print('Successfully Loaded!\\nIt took %.1fs to load from regular pickle' % (time()-startime))\n",
        "else:\n",
        "    raise Exception(\"Extension %s not supported!\" % os.path.splitext(DATASET_FILENAME)[1])\n",
        "print('Dataset loaded!')\n",
        "# display(featuredataset)\n",
        "DATA_IS_WINDOWED = featuredataset.columns.str.match('0_').any()\n",
        "WINDOW_INDEXES = sorted(list(set([int(e.split('_')[0]) for e in featuredataset.columns[featuredataset.columns.str.match('\\d+_')].to_list()])))\n",
        "print('Data is WINDOWED!' if DATA_IS_WINDOWED else '', '%d windows' % (len(WINDOW_INDEXES)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "augmented_featuredataset_list = []\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_data_paths = glob(os.path.join(DATAFOLDER,'augmented_data','*.pickle'))\n",
        "    for augmented_data_path in augmented_data_paths:\n",
        "        print(\"Loading file %s\" % os.path.basename(augmented_data_path))\n",
        "        with open(augmented_data_path,'rb') as pf:\n",
        "            augmented_featuredataset_list.append(pickle.load(pf))\n",
        "    augmented_featuredataset = pd.concat(augmented_featuredataset_list, ignore_index=True)\n",
        "    print(\"Loaded %d augmented samples\" % len(augmented_featuredataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1rKcNy5RqZ"
      },
      "source": [
        "### Drop features that we have found to be problematic with feature selection and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhip8r7F4sSv"
      },
      "outputs": [],
      "source": [
        "def drop_unused_features_old(features_df: pd.DataFrame, is_windowed = False, inplace = False) -> pd.DataFrame:\n",
        "    if not inplace:\n",
        "        res_df = features_df.copy()\n",
        "        print('Copied',flush=True)\n",
        "    else:\n",
        "        res_df = features_df\n",
        "        print('Not Copied',flush=True)\n",
        "\n",
        "\n",
        "    if is_windowed:\n",
        "        for window_index in WINDOW_INDEXES:\n",
        "            print('Dropping bad columns for index %d'%(window_index),flush=True)\n",
        "\n",
        "            if '%s_attackTime_peaksamp'%window_index       not in res_df.columns.to_list() or\\\n",
        "               '%s_attackTime_attackStartIdx'%window_index not in res_df.columns.to_list() or\\\n",
        "               '%s_peakSample_index'%window_index          not in res_df.columns.to_list():\n",
        "                # raise Exception(\"The features dataframe does not contain the required columns!\")\n",
        "                # Show warning instead of exception\n",
        "                print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\"%('%s_attackTime_peaksamp'%window_index,'%s_attackTime_attackStartIdx'%window_index,'%s_peakSample_index'%window_index))\n",
        "            else:\n",
        "                res_df.drop(columns=['%s_attackTime_peaksamp' % window_index,\\\n",
        "                                        '%s_attackTime_attackStartIdx' % window_index,\\\n",
        "                                        '%s_peakSample_index' % window_index], inplace=True)\n",
        "    else:\n",
        "        if 'attackTime_peaksamp'       not in res_df.columns.to_list() or\\\n",
        "        'attackTime_attackStartIdx' not in res_df.columns.to_list() or\\\n",
        "        'peakSample_index'          not in res_df.columns.to_list():\n",
        "            # raise Exception(\"The features dataframe does not contain the required columns!\")\n",
        "            print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\" % ('attackTime_peaksamp' in res_df.columns.to_list(), 'attackTime_attackStartIdx' in res_df.columns.to_list(), 'peakSample_index' in res_df.columns.to_list()))\n",
        "        else:\n",
        "            res_df.drop(columns=['attackTime_peaksamp',\\\n",
        "                                    'attackTime_attackStartIdx',\\\n",
        "                                    'peakSample_index'], inplace=True)\n",
        "    return res_df\n",
        "\n",
        "def drop_unused_features(features_df: pd.DataFrame, is_windowed = False, inplace = False) -> pd.DataFrame:\n",
        "    if not inplace:\n",
        "        res_df = features_df.copy()\n",
        "        print('Copied',flush=True)\n",
        "    else:\n",
        "        res_df = features_df\n",
        "        print('Not Copied',flush=True)\n",
        "\n",
        "    todrop = []\n",
        "    if is_windowed:\n",
        "        for window_index in WINDOW_INDEXES:\n",
        "            if '%s_attackTime_peaksamp'%window_index not in res_df.columns.to_list() or '%s_attackTime_attackStartIdx'%window_index not in res_df.columns.to_list() or '%s_peakSample_index'%window_index not in res_df.columns.to_list():\n",
        "                print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\"%('%s_attackTime_peaksamp'%window_index,'%s_attackTime_attackStartIdx'%window_index,'%s_peakSample_index'%window_index))\n",
        "            else:\n",
        "                todrop.extend(['%s_attackTime_peaksamp' % window_index,'%s_attackTime_attackStartIdx' % window_index,'%s_peakSample_index' % window_index])\n",
        "    else:\n",
        "        if 'attackTime_peaksamp' not in res_df.columns.to_list() or 'attackTime_attackStartIdx' not in res_df.columns.to_list() or 'peakSample_index' not in res_df.columns.to_list():\n",
        "            print(\"Warning! The features dataframe does not contain the required columns! (%s, %s, %s)\" % ('attackTime_peaksamp' in res_df.columns.to_list(), 'attackTime_attackStartIdx' in res_df.columns.to_list(), 'peakSample_index' in res_df.columns.to_list()))\n",
        "        else:\n",
        "            todrop.extend(['attackTime_peaksamp','attackTime_attackStartIdx','peakSample_index'])\n",
        "\n",
        "    res_df.drop(columns=todrop, inplace=True)\n",
        "    return res_df\n",
        "\n",
        "\n",
        "#measure time\n",
        "startime = time()\n",
        "featuredataset = drop_unused_features(featuredataset, is_windowed = DATA_IS_WINDOWED)\n",
        "print('It took %.1fs to drop unused features' % (time()-startime))\n",
        "if USE_AUGMENTED_DATA:\n",
        "    augmented_featuredataset = drop_unused_features(augmented_featuredataset, is_windowed = DATA_IS_WINDOWED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if DROP_ADDITIONAL_CEPSTRUM_FROM_BIG_WINDOW:\n",
        "\n",
        "#     # Largest cepstrum now:\n",
        "#     larg_ceps = max([int(e.split('_')[-1]) for e in featuredataset.columns.to_list() if 'cepstrum' in e])\n",
        "#     # Max xepstrum coeff. of smallest window (704 samples)\n",
        "#     smallest_ceps = 704//2+1\n",
        "\n",
        "#     featuredataset.drop(columns=[f'cepstrum_{v}' for v in range(smallest_ceps+1,larg_ceps + 1)], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xh-UknklQy"
      },
      "source": [
        "### If specified, drop extra percussive recorded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvUB_btbklQy"
      },
      "outputs": [],
      "source": [
        "# if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "#     assert featuredataset.shape == (EXPECTED_DATASED_SIZE, 504)\n",
        "if DROP_EXTRA_PERCUSSIVE_SOUNDS:\n",
        "    to_drop_count = np.count_nonzero(featuredataset.meta_audiofilePath.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count))\n",
        "        featuredataset = featuredataset[~featuredataset.meta_audiofilePath.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(featuredataset.shape)))\n",
        "    # if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    #     assert featuredataset.shape == (EXPECTED_DATASED_SIZE-2237, 504)\n",
        "\n",
        "\n",
        "if USE_AUGMENTED_DATA and DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG:\n",
        "    augmented_featuredataset_dr = augmented_featuredataset.copy()\n",
        "    to_drop_count_aug = np.count_nonzero(augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\").values)\n",
        "    if to_drop_count_aug >= 0:\n",
        "        print('Dropping %d additional percussive recordings because \"DROP_EXTRA_PERCUSSIVE_SOUNDS\" was specified.'%(to_drop_count_aug))\n",
        "        augmented_featuredataset_dr = augmented_featuredataset[~augmented_featuredataset.meta_augmentation_source.str.contains(\"additional-500\")].reset_index(drop=True)\n",
        "        print('Dataset shape after dropping extra percussive recordings: %s'%(str(augmented_featuredataset_dr.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HN56MAc5kjA"
      },
      "outputs": [],
      "source": [
        "# Extract separate DFs\n",
        "from typing import Tuple\n",
        "\n",
        "# Divide dataset into metadata, features and labels\n",
        "def divide_dataset(features_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    # print time of eacch operation / line\n",
        "    # startime = time()\n",
        "    metadata = features_df.filter(regex='^meta_',axis=1)\n",
        "    # print('It took %.1fs to extract metadata' % (time()-startime))\n",
        "    # startime = time()\n",
        "    labels = features_df.meta_expressive_technique_id\n",
        "    # print('It took %.1fs to extract labels' % (time()-startime))\n",
        "    # startime = time()\n",
        "    features = features_df.loc[:,[col for col in features_df.columns if col not in metadata.columns]]\n",
        "    # print('It took %.1fs to extract features' % (time()-startime))\n",
        "    # Convert to numeric formats where possible (somehow convert_dtypes doesn't work [https://stackoverflow.com/questions/65915048/pandas-convert-dtypes-not-working-on-numbers-marked-as-objects])\n",
        "    # startime = time()\n",
        "    metadata = metadata.apply(pd.to_numeric, errors='ignore')\n",
        "    labels = labels.apply(pd.to_numeric, errors='ignore')\n",
        "    features = features.apply(pd.to_numeric, errors='ignore')\n",
        "    # print('It took %.1fs to convert to numeric types' % (time()-startime))\n",
        "    return metadata, features, labels\n",
        "\n",
        "metadata, features, labels = divide_dataset(featuredataset)\n",
        "assert metadata.shape[1] == 9\n",
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    assert features.shape[1] == 495\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    assert features.shape[1] == 2543\n",
        "\n",
        "if USE_AUGMENTED_DATA:\n",
        "    metadata_aug, features_aug, labels_aug = divide_dataset(augmented_featuredataset_dr if DROP_EXTRA_PERCUSSIVE_SOUNDS_FROMAUG else augmented_featuredataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iiyTkcYKFi"
      },
      "outputs": [],
      "source": [
        "def get_classes_description(classftask: ClassificationTask):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        classes_desk = {0:\"Kick\",1:\"Snare 1\",2:\"Tom\",3:\"Snare 2\",4:\"Natural Harmonics\",5:\"Palm Mute\",6:\"Pick Near Bridge\",7:\"Pick Over the Soundhole\"}\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        classes_desk = {0:\"Percussive\",1:\"Pitched\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\"}\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        classes_desk = {0:\"Natural Harmonics\", 1:\"Palm Mute\", 2:\"Pick Near Bridge\", 3:\"Pick Over the Soundhole\"}\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        classes_desk = {0:\"Kick\", 1:\"Snare 1\", 2:\"Tom\", 3:\"Snare 2\", 4:\"Pitched\"}\n",
        "    elif classification_task == ClassificationTask.ONE_GUITARIST_FULL:\n",
        "        classes_desk = {0:\"Kick\",1:\"Snare 1\",2:\"Tom\",3:\"Snare 2\",4:\"Natural Harmonics\",5:\"Palm Mute\",6:\"Pick Near Bridge\",7:\"Pick Over the Soundhole\"}\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "    classes = list(classes_desk.keys())\n",
        "    return classes,classes_desk\n",
        "\n",
        "def filter_dataset(tofilt_features,tofilt_labels,tofilt_metadata,classftask: ClassificationTask, hardcoded_sizes_test = False):\n",
        "    if classification_task == ClassificationTask.FULL_8_CLASS_PROBLEM:\n",
        "        pass\n",
        "    elif classification_task == ClassificationTask.BINARY_PERCUSSIVE_PITCHED:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        tofilt_labels = tofilt_labels.replace([0,1,2,3],[0,0,0,0])\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[1,1,1,1])\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        filtered_idxs = tofilt_labels < 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        if hardcoded_sizes_test:\n",
        "            assert len(tofilt_features) == 1620\n",
        "    elif classification_task == ClassificationTask.PITCHED_4_ONLY:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        filtered_idxs = tofilt_labels >= 4\n",
        "        tofilt_features = tofilt_features[filtered_idxs]\n",
        "        tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        tofilt_labels = tofilt_labels.replace([4,5,6,7],[0,1,2,3])\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237-1620\n",
        "    elif classification_task == ClassificationTask.PERCUSSIVE_PLUS_PITCHED_CLASS:\n",
        "        assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == EXPECTED_DATASED_SIZE-2237\n",
        "        tofilt_labels = tofilt_labels.replace([5,6,7],[4,4,4])\n",
        "    elif classification_task == ClassificationTask.ONE_GUITARIST_FULL:\n",
        "\n",
        "        filtered_idxs = tofilt_features['']\n",
        "        # tofilt_features = tofilt_features[filtered_idxs]\n",
        "        # tofilt_labels = tofilt_labels[filtered_idxs]\n",
        "        # tofilt_metadata = tofilt_metadata[filtered_idxs].copy()\n",
        "        # assert len(tofilt_features) == len(tofilt_labels)\n",
        "        # if hardcoded_sizes_test:\n",
        "        #     assert len(tofilt_features) == 1620\n",
        "    else:\n",
        "        raise Exception('The Classification Task selected is not supported')\n",
        "\n",
        "\n",
        "    tofilt_features.reset_index(drop=True,inplace=True)\n",
        "    tofilt_labels.reset_index(drop=True,inplace=True)\n",
        "    tofilt_metadata.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    return tofilt_features, tofilt_labels, tofilt_metadata\n",
        "\n",
        "original_dataset_features = features.copy()\n",
        "dataset_labels = labels.copy()\n",
        "dataset_metadata = metadata.copy()\n",
        "\n",
        "CLASSES,CLASSES_DESC = get_classes_description(classification_task)\n",
        "original_dataset_features,dataset_labels,dataset_metadata = filter_dataset(original_dataset_features,dataset_labels,dataset_metadata,classification_task,hardcoded_sizes_test=True if FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms else False)\n",
        "if USE_AUGMENTED_DATA:\n",
        "    features_aug,labels_aug,metadata_aug = filter_dataset(features_aug,labels_aug,metadata_aug,classification_task,hardcoded_sizes_test=False)\n",
        "    assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(labels_aug))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(labels_aug))).all()\n",
        "\n",
        "assert len(np.sort(CLASSES)) == len(np.sort(pd.unique(dataset_labels))) and np.equal(np.sort(CLASSES),np.sort(pd.unique(dataset_labels))).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(metadata[metadata['meta_audiofilePath'].str.contains('LucTur2')]['meta_audiofilePath'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_JEEbL-mcJ-"
      },
      "outputs": [],
      "source": [
        "toprint = [(original_dataset_features,'Main dataset')]\n",
        "if USE_AUGMENTED_DATA:\n",
        "    toprint.append((features_aug,'Augmented data'))\n",
        "\n",
        "for dat,name in toprint:\n",
        "    print('Dataset \"'+name+'\" read')\n",
        "    print(\"├╴Entries: \"+str(dat.shape[0]))\n",
        "    if DATA_IS_WINDOWED:\n",
        "        print('├╴Features per window: '+str(dat.shape[1]//len(WINDOW_INDEXES)))\n",
        "        print(\"└╴Windows: \"+str(len(WINDOW_INDEXES)))\n",
        "    else:\n",
        "        print('├╴Features: '+str(dat.shape[1]))\n",
        "\n",
        "original_feature_number = original_dataset_features.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2gv9fFFavMz"
      },
      "outputs": [],
      "source": [
        "# Compute has of the dataset files.\n",
        "# This are used to cache precomputed feature selection with ReliefF (Which is rather slow)\n",
        "import hashlib\n",
        " \n",
        "dataset_sha256_hash = hashlib.sha256()\n",
        "with open(DATASET_PATH,\"rb\") as fy:\n",
        "    for byte_block in iter(lambda: fy.read(4096),b\"\"):    # Read and update hash string value in blocks of 4K\n",
        "        dataset_sha256_hash.update(byte_block)\n",
        "dataset_sha256_hash = dataset_sha256_hash.hexdigest()\n",
        "\n",
        "print(dataset_sha256_hash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ND_A4d6uFT"
      },
      "source": [
        "## Parse Command Line arguments\n",
        "\n",
        "*_Important_*: If you are running this from a jupyter Notebook, change the run parameters at the end of the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVHOAmQ-6uFU"
      },
      "outputs": [],
      "source": [
        "args = None\n",
        "if not is_notebook() and not COLAB:\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Train the expressive guitar technique classifier.')\n",
        "\n",
        "    def featnum_type(x):\n",
        "        (MIN,MAX) = (1,495)\n",
        "        if x == 'all':\n",
        "            return x\n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Feature parameter must either 'all' or a number be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netdepth_type(x):\n",
        "        (MIN,MAX) = (0,20) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network depth must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def netwidth_type(x):\n",
        "        (MIN,MAX) = (1,2000) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Network width must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def dropout_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Dropout Rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def aggressiveness_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Oversampling aggressiveness value must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def lr_type(x):\n",
        "        (MIN,MAX) = (0,1) \n",
        "        x = float(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Learning rate must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def batchsize_type(x):\n",
        "        (MIN,MAX) = (1,4096) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Batchsize must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def epochs_type(x):\n",
        "        (MIN,MAX) = (1,10000) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Epoch number must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def kfold_type(x):\n",
        "        (MIN,MAX) = (1,20) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"KFOLD size must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "    def c1d_type(x):\n",
        "        (MIN,MAX) = (0,5) \n",
        "        x = int(x)\n",
        "        if x < MIN or x > MAX:\n",
        "            raise argparse.ArgumentTypeError(\"Number of conv1d layers must be between {} and {}\".format(MIN, MAX))\n",
        "        return x\n",
        "\n",
        "    # class RangedInt():\n",
        "\n",
        "\n",
        "    # def ranged_int_type(x,_min,_max):\n",
        "    #     x = int(x)\n",
        "    #     if x < _min or x > _max:\n",
        "    #         raise argparse.ArgumentTypeError(\"Value must be an integer number between {} and {}\".format(_min, _max))\n",
        "    #     return x\n",
        "\n",
        "    # def ranged_float_type(x,_min,_max):\n",
        "    #     x = float(x)\n",
        "    #     if x < _min or x > _max:\n",
        "    #         raise argparse.ArgumentTypeError(\"Value must be a real number between {} and {}\".format(_min, _max))\n",
        "    #     return x\n",
        "\n",
        "    parser.add_argument('-f',  '--features',      default='all',     type=featnum_type,   help='Number of features to use for training [1-495] (default: 80)')\n",
        "    parser.add_argument('-d',  '--net-depth',     default=3,      type=netdepth_type,  help='Number of dense layers in the network [0-20] (default: 3)')\n",
        "    parser.add_argument('-w',  '--net-width',     default=100,    type=netwidth_type,  help='Number of layers in the FFNN [1-2000] (default: 100)')\n",
        "    parser.add_argument('-dr', '--dropout',       default=0.15,   type=dropout_type,   help='Dropout amount [0-1] (default: 0.15)')\n",
        "    parser.add_argument('-lr', '--learning-rate', default=0.0001, type=lr_type,        help='Learning rate [0-1] (default: 0.0001)')\n",
        "    parser.add_argument('-bs', '--batchsize',     default=256,    type=batchsize_type, help='Learning rate [1-4096] (default: 256)')\n",
        "    parser.add_argument('-e',  '--epochs',        default=1000,   type=epochs_type,    help='Learning rate [1-10000] (default: 1000)')\n",
        "    parser.add_argument('-k',  '--k-folds',       default=5,      type=kfold_type,     help='K of K-folds [1-20] (default: 5)')\n",
        "    parser.add_argument('-os', '--oversampling',  action='store_true', help='Perform oversampling')\n",
        "    parser.add_argument('-osagg', '--oversampling-aggressiveness',  default=0.2,   type=aggressiveness_type,   help='Oversampling aggressiveness [0-1] (default: 0.2)')\n",
        "    parser.add_argument('-c1d',   '--conv1d',     default=0,      type=c1d_type,     help='Number of conv1D layers at the beginning [1-5] (default: 0)')\n",
        "    parser.add_argument('-ck',    '--conv1d-kernels',     default='',      type=ascii,     help='Comma-separated list of kernel sizes for conv1D layers (es: 3,5,7)')\n",
        "    parser.add_argument('-cs',    '--conv1d-strides',     default='',      type=ascii,     help='Comma-separated list of strides for conv1D layers (es: 1,1,1)')\n",
        "    parser.add_argument('-cf',    '--conv1d-filters',     default='',      type=ascii,     help='Comma-separated list of filters for conv1D layers (es: 32,64,128)')\n",
        "    parser.add_argument('-c1dact','--conv1d-activations', default='',  type=ascii,     help='Comma-separated list of activations for conv1D layers (es: relu,relu,relu)')\n",
        "    parser.add_argument('-pl','--pool_layers', default='',  type=ascii,     help='Comma-separated list of pool layers. Use \"N\" for none, \"M\" for max-pooling and \"A\" for average pooling  (es: M,N,M)')\n",
        "    parser.add_argument('-v', '--verbose',        action='store_true', help='increase output verbosity')\n",
        "    args = parser.parse_args()\n",
        "    args = vars(args)\n",
        "else:\n",
        "\n",
        "    \"\"\"\n",
        "    ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
        "    █████████████████████████████     ██     ██     ██     ██ ███ ██     ███████████████████████████████\n",
        "    █████████████████████████████ ███ ██ ███ ██ ███ ██ ███ ██  █  ██ ███████████████████████████████████\n",
        "    █████████████████████████████ ███ ██ ███ ██ ███ ██ ███ ██ █ █ ██ ███████████████████████████████████\n",
        "    █████████████████████████████     ██     ██     ██     ██ ███ ██     ███████████████████████████████\n",
        "    █████████████████████████████ ██████ ███ ██ ██ ███ ███ ██ ███ ██████ ███████████████████████████████\n",
        "    █████████████████████████████ ██████ ███ ██ ███ ██ ███ ██ ███ ██████ ███████████████████████████████\n",
        "    █████████████████████████████ ██████ ███ ██ ███ ██ ███ ██ ███ ██     ███████████████████████████████\n",
        "    ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
        "    \"\"\"\n",
        "    \n",
        "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "    \"\"\" +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    #   |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #   + ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ +                                 #\n",
        "    \"\"\" #↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓#\n",
        "    args = {'features':      'all', \n",
        "            'net_depth':     0, \n",
        "            'net_width':     64, \n",
        "            'dropout':       0.6,\n",
        "            'learning_rate': 0.0005,\n",
        "            'batchsize':     1024,\n",
        "            'epochs':        120,\n",
        "            'k_folds':       5,\n",
        "            'oversampling':  True,\n",
        "            'oversampling_aggressiveness':  0,\n",
        "            'conv1d':        4,\n",
        "            'conv1d_kernels': '5,5,5,5',\n",
        "            'conv1d_strides': '3,2,1,1',\n",
        "            'conv1d_filters': '4,4,8,16',\n",
        "            'conv1d_activations': 'relu,relu,relu,relu',\n",
        "            'pool_layers': 'M,A,N,M',\n",
        "            'verbose':       False}\n",
        "    #↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑#\n",
        "    \"\"\" + ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ +                                 #\n",
        "    #   |    CHANGE THE VALUES HERE IF RUNNING THE TRAINING FROM A JUPYTER NOTEBOOK (e.g., on Colab)    |                                 #\n",
        "    #   +-----------------------------------------------------------------------------------------------+                                 #\n",
        "    \"\"\"#----------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "args['conv1d_kernels'] = args['conv1d_kernels'].strip(\"'\")\n",
        "args['conv1d_filters'] = args['conv1d_filters'].strip(\"'\")\n",
        "args['conv1d_activations'] = args['conv1d_activations'].strip(\"'\")\n",
        "args['conv1d_strides'] = args['conv1d_strides'].strip(\"'\")\n",
        "\n",
        "KERNEL_SIZES = [int(x) for x in args['conv1d_kernels'].split(',')] if args['conv1d'] > 1 else [int(args['conv1d_kernels'])] if args['conv1d'] == 1 else []\n",
        "print('KERNEL_SIZES: ', KERNEL_SIZES)\n",
        "FILTERS = [int(x) for x in args['conv1d_filters'].split(',')] if args['conv1d'] > 1 else [int(args['conv1d_filters'])] if args['conv1d'] == 1 else []\n",
        "print('FILTERS: ', FILTERS)\n",
        "CONV_ACTIVATIONS = args['conv1d_activations'].split(',')    if args['conv1d'] > 1 else [args['conv1d_activations']] if args['conv1d'] == 1 else []\n",
        "CONV_ACTIVATIONS = [e if e.lower() != 'none' else None for e in CONV_ACTIVATIONS]\n",
        "print('CONV_ACTIVATIONS: ', CONV_ACTIVATIONS)\n",
        "STRIDES = [int(x) for x in args['conv1d_strides'].split(',')] if args['conv1d'] > 1 else [int(args['conv1d_strides'])] if args['conv1d'] == 1 else []\n",
        "print('STRIDES: ', STRIDES)\n",
        "POOL_LAYERS = args['pool_layers'].split(',') if args['conv1d'] > 1 else [args['pool_layers']] if args['conv1d'] == 1 else []\n",
        "#Pooling layers must be one of 'M', 'N', or 'A' \n",
        "assert all([e in ['M','N','A'] for e in POOL_LAYERS]), \"Pooling layers must be one of 'M', 'N', or 'A'\"\n",
        "print('POOL_LAYERS: ', POOL_LAYERS)\n",
        "\n",
        "POOL_SIZES = [2]*args['conv1d'] # TODO: parameterize this as well\n",
        "\n",
        "assert len(KERNEL_SIZES)     == args['conv1d'], \"The number of kernel sizes must be equal to the number of conv1d layers ({} != {})\".format(len(KERNEL_SIZES), args['conv1d'])\n",
        "assert len(FILTERS)          == args['conv1d'], \"The number of filters must be equal to the number of conv1d layers ({} != {})\".format(len(FILTERS), args['conv1d'])\n",
        "assert len(CONV_ACTIVATIONS) == args['conv1d'], \"The number of activations must be equal to the number of conv1d layers ({} != {})\".format(len(CONV_ACTIVATIONS), args['conv1d'])\n",
        "assert len(STRIDES)          == args['conv1d'], \"The number of strides must be equal to the number of conv1d layers ({} != {})\".format(len(STRIDES), args['conv1d'])\n",
        "assert len(POOL_LAYERS)      == args['conv1d'], \"The number of pool layers must be equal to the number of conv1d layers ({} != {})\".format(len(POOL_LAYERS), args['conv1d'])\n",
        "# raise Exception('STOP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "# call garbage collector to free up memory\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjF3Cif5zr1p"
      },
      "source": [
        "## Subset features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--hCfGHKZK98"
      },
      "outputs": [],
      "source": [
        "def get_manual_selected_features(data,prefix = ''):\n",
        "    print (\"Subsetting features...\")\n",
        "    columns_to_keep = []\n",
        "    # if USE_ATTACKTIME_PEAKSAMP:\n",
        "    #     columns_to_keep.append(\"attackTime_peaksamp\")\n",
        "    # if USE_ATTACKTIME_ATTACKSTARTIDX:\n",
        "    #     columns_to_keep.append(\"attackTime_attackStartIdx\")\n",
        "    if USE_ATTACKTIME_VALUE:\n",
        "        columns_to_keep.append(prefix+\"attackTime_value\")\n",
        "    if USE_BARKSPECBRIGHTNESS:\n",
        "        columns_to_keep.append(prefix+\"barkSpecBrightness\")\n",
        "    if USE_PEAKSAMPLE_VALUE:\n",
        "        columns_to_keep.append(prefix+\"peakSample_value\")\n",
        "    # if USE_PEAKSAMPLE_INDEX:\n",
        "    #     columns_to_keep.append(\"peakSample_index\")\n",
        "    if USE_ZEROCROSSING:\n",
        "        columns_to_keep.append(prefix+\"zeroCrossing\")\n",
        "\n",
        "    assert USE_BARKSPEC <= 50 and USE_BARKSPEC >= 0 and USE_BFCC <= 49 and USE_BFCC >= 0 and USE_CEPSTRUM <= 353 and USE_CEPSTRUM >= 0 and USE_MFCC <= 37 and USE_MFCC >= 0\n",
        "\n",
        "    if USE_BARKSPEC > 0:\n",
        "        columns_to_keep += [prefix+'barkSpec_'+str(i+1) for i in range(USE_BARKSPEC)]\n",
        "    if USE_BFCC > 0:\n",
        "        columns_to_keep += [prefix+'bfcc_'+str(i+2) for i in range(USE_BFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "    if USE_CEPSTRUM > 0:\n",
        "        columns_to_keep += [prefix+'cepstrum_'+str(i+1) for i in range(USE_CEPSTRUM)]\n",
        "    if USE_MFCC > 0:\n",
        "        columns_to_keep += [prefix+'mfcc_'+str(i+2) for i in range(USE_MFCC)]  # +2 is correct here since we want to skip the first normalized coefficient\n",
        "\n",
        "    return columns_to_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agcUWSWVbokS"
      },
      "outputs": [],
      "source": [
        "# # To Compeltely reset RelieFF cache\n",
        "# with open(RELIEF_CACHE_FILEPATH, 'wb') as rcf:\n",
        "#     pickle.dump(set(), rcf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBDXJV0dnx2W"
      },
      "outputs": [],
      "source": [
        "# how_many_examples_per_class =10\n",
        "# subselection = list(range(0,how_many_examples_per_class))+\\\n",
        "#                list(range(600,600+how_many_examples_per_class))+\\\n",
        "#                list(range(1100,1100+how_many_examples_per_class))+\\\n",
        "#                list(range(1400,1400+how_many_examples_per_class))+\\\n",
        "#                list(range(1900,1900+how_many_examples_per_class))+\\\n",
        "#                list(range(3000,3000+how_many_examples_per_class))+\\\n",
        "#                list(range(9000,9000+how_many_examples_per_class))+\\\n",
        "#                list(range(14000,14000+how_many_examples_per_class))\n",
        "\n",
        "# testprova_dataset_features = original_dataset_features.iloc[subselection]\n",
        "# testprova_dataset_labels = dataset_labels.iloc[subselection]\n",
        "import os, platform, subprocess, re\n",
        "\n",
        "def get_processor_name():\n",
        "    if platform.system() == \"Windows\":\n",
        "        return platform.processor()\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n",
        "        command =\"sysctl -n machdep.cpu.brand_string\"\n",
        "        return subprocess.check_output(command).strip()\n",
        "    elif platform.system() == \"Linux\":\n",
        "        command = \"cat /proc/cpuinfo\"\n",
        "        all_info = subprocess.check_output(command, shell=True).decode().strip()\n",
        "        for line in all_info.split(\"\\n\"):\n",
        "            if \"model name\" in line:\n",
        "                return re.sub( \".*model name.*:\", \"\", line,1)\n",
        "    return \"\"\n",
        "\n",
        "class ReliefCacheElem(dict):\n",
        "\n",
        "    PRINT_HASH = False\n",
        "\n",
        "    def __init__(self,dataset_sha256,n_neighbors,relieff_top_features,relieff_feature_importances,time_of_computation):\n",
        "        self.dataset_sha256 = dataset_sha256\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.relieff_top_features = relieff_top_features\n",
        "        self.relieff_feature_importances = relieff_feature_importances\n",
        "        self.date = strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
        "\n",
        "        self.cpu_info = get_processor_name()\n",
        "        self.time_of_computation = time_of_computation\n",
        "\n",
        "    def __key(self):\n",
        "        return tuple([self.dataset_sha256,\n",
        "                     self.n_neighbors,\n",
        "                     str(self.relieff_top_features),\n",
        "                     str(self.relieff_feature_importances)])\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.__key())\n",
        "\n",
        "    def __str__(self):\n",
        "        res = '{date: '+self.date+', n_neighbors:'+str(self.n_neighbors)\n",
        "        \n",
        "        if self.PRINT_HASH:\n",
        "            res += 'dataset_sha256:'+str(self.dataset_sha256)+','\n",
        "\n",
        "        res += 'cpu_info:'+str(self.cpu_info)+','\n",
        "        res += 'time_of_computation:'+str(self.time_of_computation)+','\n",
        "        res += '}'\n",
        "        return res\n",
        "\n",
        "def relieff_selection(X:list,y:list,n_features,n_neighbors,relief_cache_filepath,verbose_ = True):\n",
        "    relief_data_X = X\n",
        "    relief_data_y = y\n",
        "    # First check if result is already cached\n",
        "    ## Load Cache\n",
        "    relief_cache = None\n",
        "\n",
        "    ##----------------------------------------------##\n",
        "    if not os.path.exists(relief_cache_filepath):\n",
        "        raise Exception(\"RELIEF CACHE NOT FOUND at '\"+relief_cache_filepath+\"'! Comment exception to create empty cache\")\n",
        "        with open(relief_cache_filepath, 'wb') as rcf:\n",
        "            pickle.dump(set(), rcf)\n",
        "    ##----------------------------------------------##\n",
        "\n",
        "    with open(relief_cache_filepath,'rb') as rcf:\n",
        "        relief_cache = pickle.load(rcf)\n",
        "        if verbose_: \n",
        "            print('Loaded Relief cache ('+str(len(relief_cache))+' solutions)')\n",
        "    # Check if present\n",
        "    for cache_elem in relief_cache:\n",
        "        if cache_elem.dataset_sha256 == dataset_sha256_hash and\\\n",
        "           cache_elem.n_neighbors == n_neighbors:\n",
        "            if verbose_:\n",
        "                print(\"Result found in cache!\")\n",
        "            return cache_elem.relieff_top_features[:n_features]\n",
        "    \n",
        "    # If not present, compute\n",
        "    if verbose_:\n",
        "        print(\"Result NOT found in cache, computing now... (might take a long while)\")\n",
        "    \n",
        "    from skrebate import ReliefF\n",
        "    r = ReliefF(n_neighbors=n_neighbors,verbose=verbose_)\n",
        "    \n",
        "    start_fit = time()\n",
        "    r.fit(X=relief_data_X,y=relief_data_y)\n",
        "    top_features = r.top_features_\n",
        "    feature_importances = r.feature_importances_\n",
        "    stop_fit = time()\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done. Now storing in cache...\")\n",
        "\n",
        "    savedata = ReliefCacheElem(\n",
        "        dataset_sha256 = dataset_sha256_hash,\n",
        "        n_neighbors = n_neighbors,\n",
        "        relieff_top_features = top_features,\n",
        "        relieff_feature_importances = feature_importances,\n",
        "        time_of_computation = stop_fit - start_fit)\n",
        "    relief_cache.add(savedata)\n",
        "    with open(relief_cache_filepath, 'wb') as rcf:\n",
        "        pickle.dump(relief_cache, rcf)\n",
        "\n",
        "    if verbose_:\n",
        "        print(\"Done.\")\n",
        "\n",
        "    return top_features[:n_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oas8vnnMQ0uP"
      },
      "outputs": [],
      "source": [
        "if FEATURE_SELECTION == FeatureSelection.AUTO_RELIEF:\n",
        "    with open(RELIEF_CACHE_FILEPATH,'rb') as rcf:\n",
        "        relief_cache = pickle.load(rcf)\n",
        "        print(len(relief_cache),'cached relief runs:')\n",
        "\n",
        "        if len(relief_cache) != 0:\n",
        "            samedataset = [e for e in relief_cache if e.dataset_sha256 == dataset_sha256_hash]\n",
        "            print('('+str(len(samedataset))+'/'+str(len(relief_cache)), 'are from the same dataset)')\n",
        "            if len(samedataset) != len(relief_cache):\n",
        "                raise Exception('Some of the cached results are from a different dataset!')\n",
        "            for i,e in enumerate(relief_cache):\n",
        "                print(i,':',e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkiDFjjztbM"
      },
      "outputs": [],
      "source": [
        "if args['features'] == 'all':\n",
        "    FEATURE_SELECTION = FeatureSelection.NONE\n",
        "    AUTO_FEATURE_NUMBER = len(original_dataset_features.columns)\n",
        "    print\n",
        "else:\n",
        "    AUTO_FEATURE_NUMBER = args['features']    # If FEATURE_SELECTION is AUTO_ANOVA or AUTO_RELIEF, select this number of features automatically\n",
        "\n",
        "assert AUTO_FEATURE_NUMBER > 0, 'Number of features must be > 0'\n",
        "assert AUTO_FEATURE_NUMBER <= len(original_dataset_features.columns), 'Number of features is bigger than the number of columns in dataset ({} > {})'.format(AUTO_FEATURE_NUMBER,len(original_dataset_features.columns))\n",
        "\n",
        "if FEATURE_SELECTION != FeatureSelection.NONE and DO_NORMALIZE_FOR_FEATURE_SELECTION:\n",
        "    print('Normalizing data for feature selection...')\n",
        "    feature_dataset_for_selection = original_dataset_features.to_numpy()\n",
        "    feature_dataset_for_selection = SCALER_TO_USE.fit_transform(feature_dataset_for_selection)\n",
        "    feature_dataset_for_selection_D = pd.DataFrame(columns=original_dataset_features.columns, data=feature_dataset_for_selection)\n",
        "    # print('example row' + str(feature_dataset_for_selection[0]))\n",
        "    (relief_data_X,relief_data_y) = (feature_dataset_for_selection,dataset_labels.values.ravel())\n",
        "    print('Done.')\n",
        "else:\n",
        "    feature_dataset_for_selection_D = original_dataset_features\n",
        "    feature_dataset_for_selection = original_dataset_features.to_numpy()\n",
        "    (relief_data_X,relief_data_y) = (original_dataset_features.values,dataset_labels.values.ravel())\n",
        "\n",
        "\n",
        "selected_features = []\n",
        "if FEATURE_SELECTION == FeatureSelection.NONE:\n",
        "    selected_features = original_dataset_features.columns\n",
        "elif FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES:\n",
        "    ''' Features '''\n",
        "    USE_ATTACKTIME_VALUE = True\n",
        "    USE_BARKSPECBRIGHTNESS = True\n",
        "    USE_PEAKSAMPLE_VALUE = True\n",
        "    USE_ZEROCROSSING = False\n",
        "\n",
        "    USE_BARKSPEC = 40 # Number in range [0-50]\n",
        "    USE_BFCC = 40     # Number in range [0-50]\n",
        "    USE_CEPSTRUM = 60 # Number in range [0-353]\n",
        "    USE_MFCC = 30     # Number in range [0-38]\n",
        "\n",
        "    if DATA_IS_WINDOWED:\n",
        "        for index in WINDOW_INDEXES:\n",
        "            selected_features.extend(get_manual_selected_features(original_dataset_features,prefix=index+'_'))\n",
        "    else:\n",
        "        selected_features = get_manual_selected_features(original_dataset_features)\n",
        "elif FEATURE_SELECTION == FeatureSelection.MANUAL_LIST:\n",
        "    selected_features = ['attackTime_value', 'barkSpecBrightness', 'barkSpec_1', 'barkSpec_2', 'barkSpec_3', 'barkSpec_4', 'barkSpec_5', 'barkSpec_6', 'barkSpec_7', 'barkSpec_8', 'barkSpec_9', 'barkSpec_10', 'barkSpec_11', 'barkSpec_12', 'barkSpec_13', 'barkSpec_14', 'barkSpec_15', 'barkSpec_16', 'barkSpec_17', 'barkSpec_18', 'barkSpec_19', 'barkSpec_20', 'barkSpec_21', 'barkSpec_22', 'barkSpec_23', 'barkSpec_24', 'barkSpec_25', 'barkSpec_26', 'barkSpec_27', 'barkSpec_28', 'barkSpec_29', 'barkSpec_30', 'barkSpec_31', 'barkSpec_32', 'barkSpec_33', 'barkSpec_34', 'barkSpec_35', 'barkSpec_36', 'barkSpec_37', 'barkSpec_38', 'barkSpec_39', 'barkSpec_40', 'barkSpec_41', 'barkSpec_42', 'barkSpec_43', 'barkSpec_44', 'barkSpec_45', 'barkSpec_46', 'barkSpec_47', 'barkSpec_48', 'barkSpec_49', 'barkSpec_50', 'bfcc_2', 'bfcc_3', 'bfcc_4', 'bfcc_5', 'bfcc_6', 'bfcc_7', 'bfcc_8', 'bfcc_9', 'bfcc_10', 'bfcc_11', 'bfcc_12', 'bfcc_13', 'bfcc_15', 'bfcc_16', 'bfcc_17', 'bfcc_18', 'bfcc_19', 'bfcc_20', 'bfcc_21', 'bfcc_25', 'bfcc_26', 'bfcc_27', 'bfcc_28', 'bfcc_29', 'bfcc_30', 'bfcc_31', 'bfcc_35', 'bfcc_36', 'bfcc_37', 'bfcc_39', 'bfcc_40', 'bfcc_42', 'bfcc_43', 'bfcc_44', 'bfcc_45', 'bfcc_46', 'bfcc_48', 'cepstrum_1', 'cepstrum_2', 'cepstrum_3', 'cepstrum_4', 'cepstrum_5', 'cepstrum_6', 'cepstrum_7', 'cepstrum_8', 'cepstrum_9', 'cepstrum_10', 'cepstrum_11', 'cepstrum_12', 'cepstrum_13', 'cepstrum_14', 'cepstrum_15', 'cepstrum_16', 'cepstrum_17', 'cepstrum_18', 'cepstrum_19', 'cepstrum_20', 'cepstrum_21', 'cepstrum_22', 'cepstrum_23', 'cepstrum_24', 'cepstrum_25', 'cepstrum_26', 'cepstrum_27', 'cepstrum_28', 'cepstrum_29', 'cepstrum_30', 'cepstrum_31', 'cepstrum_32', 'cepstrum_33', 'cepstrum_34', 'cepstrum_35', 'cepstrum_36', 'cepstrum_37', 'cepstrum_41', 'cepstrum_42', 'cepstrum_43', 'cepstrum_44', 'cepstrum_45', 'cepstrum_46', 'cepstrum_47', 'cepstrum_48', 'cepstrum_49', 'cepstrum_54', 'cepstrum_56', 'cepstrum_59', 'cepstrum_60', 'cepstrum_67', 'cepstrum_72', 'cepstrum_86', 'cepstrum_87', 'cepstrum_108', 'cepstrum_164', 'cepstrum_205', 'cepstrum_206', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13', 'mfcc_14', 'mfcc_15', 'mfcc_16', 'mfcc_17', 'mfcc_18', 'mfcc_19', 'mfcc_20', 'mfcc_21', 'mfcc_22', 'mfcc_23', 'mfcc_24', 'mfcc_25', 'mfcc_26', 'mfcc_32', 'mfcc_33', 'mfcc_34', 'mfcc_35', 'mfcc_36', 'peakSample_value', 'zeroCrossing']\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_ANOVA:\n",
        "    if original_dataset_features.shape[1] != original_feature_number:\n",
        "        raise ValueError(\"ERROR: please import dataset again since you are trying to subset an already processed feature set (\"+str(dataset_features.shape[1])+\"<\"+str(original_feature_number)+\")\")\n",
        "\n",
        "    # ANOVA feature selection for numeric input and categorical output (https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/#:~:text=Feature%20selection%20is%20the%20process,the%20performance%20of%20the%20model)\n",
        "    from sklearn.feature_selection import SelectKBest\n",
        "    from sklearn.feature_selection import f_classif\n",
        "    \n",
        "    if not DATA_IS_WINDOWED:\n",
        "        fs = SelectKBest(score_func=f_classif, k=AUTO_FEATURE_NUMBER) # Define feature selection\n",
        "        X_selected = fs.fit_transform(feature_dataset_for_selection, dataset_labels.to_numpy().ravel())                         # Apply feature selection\n",
        "        support = fs.get_support(indices=True)                      # Extract selected indexes\n",
        "        selected_features = original_dataset_features.columns[support].tolist()\n",
        "        print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "    else:\n",
        "        # For windowed data we select the best features for each window, rank the most used across all the windows and select the AUTO_FEATURE_NUMBER best ones\n",
        "        bestfeats_total_count = {}\n",
        "        for wi in WINDOW_INDEXES:\n",
        "            fs = SelectKBest(score_func=f_classif, k=AUTO_FEATURE_NUMBER)\n",
        "            currentwindowdata = feature_dataset_for_selection_D[feature_dataset_for_selection_D.columns[feature_dataset_for_selection_D.columns.str.contains(wi+'_')]]\n",
        "            X_selected = fs.fit_transform(currentwindowdata.to_numpy(), dataset_labels.to_numpy().ravel())                         # Apply feature selection\n",
        "            current_window_best_features = currentwindowdata.columns[fs.get_support(indices=True) ].tolist()\n",
        "            # Increase occurences of best features but drop window index prefix\n",
        "            for f in current_window_best_features:\n",
        "                f = '_'.join(f.split('_')[1:])\n",
        "                if f in bestfeats_total_count:\n",
        "                    bestfeats_total_count[f] += 1\n",
        "                else:\n",
        "                    bestfeats_total_count[f] = 1\n",
        "        # sort best features by occurences\n",
        "        bestfeats_total_count = [k for k, v in sorted(bestfeats_total_count.items(), key=lambda item: item[1],reverse=True)]\n",
        "        #take first AUTO_FEATURE_NUMBER\n",
        "        bestfeats_total_count = bestfeats_total_count[:AUTO_FEATURE_NUMBER]\n",
        "        # add window index prefix and expand to all indexes\n",
        "        selected_features = []\n",
        "        for wi in WINDOW_INDEXES:\n",
        "            selected_features.extend([wi+'_'+f for f in bestfeats_total_count])\n",
        "elif FEATURE_SELECTION == FeatureSelection.AUTO_RELIEF:\n",
        "    support = relieff_selection(relief_data_X,relief_data_y,AUTO_FEATURE_NUMBER,n_neighbors=5,relief_cache_filepath=RELIEF_CACHE_FILEPATH,verbose_= True)\n",
        "    selected_features = original_dataset_features.columns[support].tolist()\n",
        "    print(str(AUTO_FEATURE_NUMBER)+\" best features:\" + str(selected_features))\n",
        "else:\n",
        "    raise Exception(\"ERROR! This type of feature selection is not supported\")\n",
        "\n",
        "dataset_features = original_dataset_features.copy().loc[:,selected_features]\n",
        "# if USE_AUGMENTED_DATA:\n",
        "#     features_aug = features_aug.copy().loc[:,selected_features]\n",
        "print(\"Features reduced \"+('manually' if (FEATURE_SELECTION == FeatureSelection.MANUAL_LIST or FEATURE_SELECTION == FeatureSelection.MANUAL_VARIABLES) else 'automatically')+\" (\"+str(FEATURE_SELECTION)+\") from \"+str(original_feature_number)+\" to : \"+str(dataset_features.shape[1]))\n",
        "print('%d features for each of the %d windows'%(AUTO_FEATURE_NUMBER,len(set([e.split('_')[0] for e in original_dataset_features.columns[original_dataset_features.columns.str.match('\\d_')].to_list()]))) if DATA_IS_WINDOWED else '')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxb5oNdswky3"
      },
      "source": [
        "## Evaluate class support\n",
        "(What percentage of dataset entries represent each class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOwPsK_h5r3q"
      },
      "outputs": [],
      "source": [
        "DO_PRINT_SUPPORT = False\n",
        "def printSupport (labels_ds):\n",
        "    binc = np.bincount(np.reshape(labels_ds,labels_ds.size))\n",
        "    for i in range(binc.size):\n",
        "        print(\"Class \" + str(i) + \" support: \" + str(\"{:.2f}\".format(binc[i]/sum(binc) * 100)) + \"%\")\n",
        "        \n",
        "if DO_PRINT_SUPPORT:\n",
        "    printSupport(dataset_labels.to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w196aHu6YqnH"
      },
      "source": [
        "# Define model architecture,\n",
        "Loss, optimizer and compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class CustomNetwork():\n",
        "#     def __init__(self, number_of_conv:int, filters_per_conv:list, kernel_sizes_per_conv:list, \n",
        "#                  pool_layers:list, pool_sizes_per_conv:list, \n",
        "#                  number_of_dense:int, dense_units:list, dropout_rates:list, activations:list, \n",
        "#                  batchnorm_after_layer:list,\n",
        "#                  input_shape:list, output_shape:int):\n",
        "#         self.number_of_conv = number_of_conv\n",
        "#         self.filters_per_conv = filters_per_conv\n",
        "#         assert len(self.filters_per_conv) == self.number_of_conv\n",
        "#         self.kernel_sizes_per_conv = kernel_sizes_per_conv\n",
        "#         assert len(self.kernel_sizes_per_conv) == self.number_of_conv\n",
        "#         self.pool_sizes_per_conv = pool_sizes_per_conv\n",
        "#         assert len(self.pool_sizes_per_conv) == self.number_of_conv\n",
        "#         self.pool_layers = pool_layers\n",
        "#         assert len(self.pool_layers) == self.number_of_conv\n",
        "\n",
        "#         self.number_of_dense = number_of_dense\n",
        "#         self.dense_units = dense_units\n",
        "#         assert len(self.dense_units) == self.number_of_dense\n",
        "#         self.dropout_rates = dropout_rates\n",
        "#         assert len(self.dropout_rates) == self.number_of_dense\n",
        "#         self.activations = activations\n",
        "#         assert len(self.activations) == self.number_of_dense\n",
        "\n",
        "#         self.batchnorm_after_layer = batchnorm_after_layer\n",
        "#         assert len(self.batchnorm_after_layer) == self.number_of_dense + self.number_of_conv\n",
        "\n",
        "#         self.input_shape = input_shape\n",
        "#         self.output_shape = output_shape\n",
        "\n",
        "#     def _get_conv_unit(filters:int, kernel_size:int, activation:str, pool_size:int, pool_layer:bool, batchnorm:bool, input_shape:tuple = None):\n",
        "    \n",
        "#         if input_shape is not None:\n",
        "#             res = [tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape)]\n",
        "#         else:\n",
        "#             res = [tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation=activation)]\n",
        "\n",
        "#         res += [tf.keras.layers.BatchNormalization(),  \n",
        "#                 tf.keras.layers.MaxPooling1D(pool_size=2)]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Be14IPJTg_4"
      },
      "outputs": [],
      "source": [
        "def define_model_architecture(num_classes:int, _verbose = False):\n",
        "    tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "    sequential_structure = []\n",
        "    if args['conv1d'] > 0:\n",
        "        for i in range(args['conv1d']):\n",
        "            if i == 0:\n",
        "                sequential_structure.append(tf.keras.layers.Conv1D(filters=FILTERS[i], kernel_size=KERNEL_SIZES[i], strides=STRIDES[i], activation=CONV_ACTIVATIONS[i], input_shape=(AUTO_FEATURE_NUMBER,1)))\n",
        "            else:\n",
        "                sequential_structure.append(tf.keras.layers.Conv1D(filters=FILTERS[i], kernel_size=KERNEL_SIZES[i], strides=STRIDES[i], activation=CONV_ACTIVATIONS[i]))\n",
        "            \n",
        "            sequential_structure += [tf.keras.layers.BatchNormalization()]\n",
        "\n",
        "            if POOL_LAYERS[i] != 'N':\n",
        "                if POOL_LAYERS[i] == 'M':\n",
        "                    sequential_structure += [tf.keras.layers.MaxPooling1D(pool_size=POOL_SIZES[i])]\n",
        "                elif POOL_LAYERS[i] == 'A':\n",
        "                    sequential_structure += [tf.keras.layers.AveragePooling1D(pool_size=POOL_SIZES[i])]\n",
        "\n",
        "        sequential_structure += [tf.keras.layers.Flatten(),\n",
        "                                 tf.keras.layers.Dropout(args['dropout'])]\n",
        "\n",
        "    for i in range(args['net_depth']):\n",
        "        sequential_structure += [tf.keras.layers.Dense(args['net_width'],activation='relu',\n",
        "                                                       kernel_initializer='he_uniform'),    #   X   |           |         |\n",
        "                                 tf.keras.layers.BatchNormalization(),                      #       |     X     |         |\n",
        "                                 tf.keras.layers.Dropout(args['dropout'])                   #       |           |    X    |\n",
        "                                ]\n",
        "\n",
        "    sequential_structure += [tf.keras.layers.Dense(num_classes)]               \n",
        "\n",
        "    model = tf.keras.models.Sequential(sequential_structure)\n",
        "\n",
        "    model._name = \"guitar_timbre_classifier_\" + strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    # tf.keras.utils.vis_utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "    # TODO:fix\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_loss():\n",
        "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "PREVIEW_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kOMPQBTXVXb"
      },
      "outputs": [],
      "source": [
        "def compile_model(model,optimizer,loss_fn,_verbose = False):\n",
        "    opt = None\n",
        "    if optimizer[\"method\"] == \"sgd\":\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate = optimizer[\"learning_rate\"], momentum=optimizer[\"momentum\"])\n",
        "    elif optimizer[\"method\"] == \"adam\":\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = optimizer[\"learning_rate\"])\n",
        "    else:\n",
        "        raise Exception(\"Optimizer method not supported\")\n",
        "\n",
        "    def recall(y_true, y_pred, c):\n",
        "        y_true = K.flatten(y_true)\n",
        "        pred_c = K.cast(K.equal(K.argmax(y_pred, axis=-1), c), K.floatx())\n",
        "        true_c = K.cast(K.equal(y_true, c), K.floatx())\n",
        "        true_positives = K.sum(pred_c * true_c)\n",
        "        possible_postives = K.sum(true_c)\n",
        "        return true_positives / (possible_postives + K.epsilon())\n",
        "\n",
        "\n",
        "    def precision(y_true, y_pred, c):\n",
        "        y_true = K.flatten(y_true)\n",
        "        pred_c = K.cast(K.equal(K.argmax(y_pred, axis=-1), c), K.floatx())\n",
        "        true_c = K.cast(K.equal(y_true, c), K.floatx())\n",
        "        true_positives = K.sum(pred_c * true_c)\n",
        "        pred_positives = K.sum(pred_c)\n",
        "        return true_positives / (pred_positives + K.epsilon())\n",
        "\n",
        "    def recall_class(theclass:int):\n",
        "        funk = lambda y_true, y_pred: recall(y_true, y_pred, theclass)\n",
        "        funk.__name__ = 'recall_c' + str(theclass)\n",
        "        return funk\n",
        "\n",
        "    # model.compile(optimizer=opt,\n",
        "    #               loss=loss_fn,\n",
        "    #               metrics=['accuracy',\n",
        "    #                         recall_class(0),\n",
        "    #                         recall_class(1),\n",
        "    #                         recall_class(2),\n",
        "    #                         recall_class(3),\n",
        "    #                         recall_class(4),\n",
        "    #                         recall_class(5),\n",
        "    #                         recall_class(6),\n",
        "    #                         recall_class(7)])\n",
        "\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss_fn,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #----------------------------------------------------------#\n",
        "    # Why did we not add here a custom metric for F1-SCORE?    #\n",
        "    # In particular, we wanted ** Macro Average - F1-score **  #\n",
        "    # which is the (non-weighted) average of the F1-score for  #\n",
        "    # each class.                                              #\n",
        "    # Thus is a relevnt metric for our problem, since we have  #\n",
        "    # class imbalance.                                         #\n",
        "    #                                                          #\n",
        "    # The issue seems to be the way that tensorflow handles    #\n",
        "    # metric computation.                                      #\n",
        "    # The metric is computed for each batch, and then the      #\n",
        "    # average is computed.                                     #\n",
        "    # Also, custom metrics seems to require the use of tensor  #\n",
        "    # operations, and converting to numpy arrays does not seem #\n",
        "    # to be supported FOR COMPUTATIONS                         #\n",
        "    # https://stackoverflow.com/a/52659570                     #\n",
        "    # https://stackoverflow.com/a/52659570/10930862            #\n",
        "    #----------------------------------------------------------#\n",
        "    \n",
        "    if _verbose:\n",
        "        print(\"Model compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc66NhrugHsG"
      },
      "source": [
        "# Save Models and Info functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vbu_AA6dhUU"
      },
      "outputs": [],
      "source": [
        "def save_model_info(model,optimizer,final_cross_validation_results,folds,metrics,outpath, fold_zerobased = None, smote_strategy = None):\n",
        "    info_filename = '/info.txt' if fold_zerobased is None else '/info_fold_'+str(fold_zerobased+1)+'.txt'\n",
        "    assert not (final_cross_validation_results and (fold_zerobased is not None))\n",
        "\n",
        "    with open(outpath + info_filename, \"w\") as f:\n",
        "        if not is_notebook():\n",
        "            f.write('Execution command:\\n')\n",
        "            f.write(\" \".join(sys_argv[:])+'\\n')\n",
        "        else:\n",
        "            f.write('Trained with the jupyter notebook (not the script version)\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if DO_OVERSAMPLING:\n",
        "            f.write(\"Oversampling (SMOTE) with aggressiveness: \"+str(OVERSAMPLING_AGGRESSIVENESS)+'\\n')\n",
        "            if smote_strategy is not None:\n",
        "                f.write(\"SMOTE strategy: \"+str(smote_strategy)+'\\n')\n",
        "            else:\n",
        "                f.write(\"SMOTE strategy: \"+str(SMOTE_STRATEGY)+')\\n')\n",
        "        else:\n",
        "            f.write('NOT performing Oversampling'+'\\n')\n",
        "\n",
        "        if USE_AUGMENTED_DATA:\n",
        "            f.write('Using augmented data'+'\\n')\n",
        "            f.write('Augmented audio file paths: ' + ','.join([os.path.basename(x) for x in augmented_data_paths])+'\\n')\n",
        "            f.write('Loaded '+str(len(augmented_featuredataset))+' augmented samples'+'\\n')\n",
        "            f.write('Used '+str(len(features_aug))+' augmented samples after filtering the dataset'+'\\n')\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('Augmented data used in this fold: '+str(len(train_aug_indexes[fold_zerobased]))+'\\n')\n",
        "            else:\n",
        "                f.write('Augmented data used for all splits: \\n')\n",
        "                for split_idx in train_aug_indexes:\n",
        "                    f.write(str(len(train_aug_indexes[split_idx]))+'\\n')\n",
        "\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        if fold_zerobased is not None:\n",
        "            f.write(\"FOLD [\"+str(fold_zerobased+1)+\"/\"+str(folds)+\"]\\n\\n\")\n",
        "        f.write(\"Summary:\\n\")\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write('Data read from file '+DATASET_FILENAME+'\\n')\n",
        "        if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "            f.write('Window size of 4800 samples (100ms)\\n')\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "            f.write('Window size of 704 samples (~14ms)\\n')\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._704windowed:\n",
        "            f.write('Window size of 704 samples (~14ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._2112windowed:\n",
        "            f.write('Window size of 2112 samples (~44ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._3456windowed:\n",
        "            f.write('Window size of 3456 samples (~72ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        elif FEATURE_WINDOW_SIZE == FeatureWindowSize._4800windowed:\n",
        "            f.write('Window size of 4800 samples (~100ms) WITH WINDOWING\\n')\n",
        "            f.write('%d overlapping windows'%(len(WINDOW_INDEXES)))\n",
        "        else:\n",
        "            raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"+--| Features: \\n\")\n",
        "        f.write('Number of features selected: '+str(len(selected_features))+'\\n')\n",
        "        f.write('Selected features: '+str(selected_features)+'\\n')\n",
        "        f.write('Feature Selection method: '+str(FEATURE_SELECTION)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        if DO_NORMALIZE_DATA:\n",
        "            f.write(\"Data was normalized. Find the parameters at the end of the file, and the scaler in the pickle file 'scaler.pickle'\\n\")\n",
        "        else:\n",
        "            f.write(\"Data was NOT normalized\")\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write('Run arguments: '+str(args)+'\\n')\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"Optimizer: \" + optimizer[\"method\"])\n",
        "        if optimizer[\"method\"] == \"sgd\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]) + \" momentum: \" + str(optimizer[\"momentum\"]))\n",
        "        elif optimizer[\"method\"] == \"adam\":\n",
        "            f.write(\" lr: \" + str(optimizer[\"learning_rate\"]))\n",
        "        else:\n",
        "            assert(False) # If triggered check new optimizer and add case\n",
        "        f.write(\"\\n\\n\")\n",
        "        if final_cross_validation_results:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs for each fold (\"+str(folds)+\"-foldCrossValidation)\\n\")\n",
        "            f.write(\"Single results in the folds directories\\n\")\n",
        "            f.write('\\n\\n-------- Average results --------\\n\\n')\n",
        "        else:\n",
        "            f.write(\"Trained for \" + str(args['epochs']) + \" epochs and with_batch size '\" + str(args['batchsize']) + \"'\" + \" epochs\\n\")\n",
        "\n",
        "            if fold_zerobased is not None:\n",
        "                f.write('(K-Fold cross validation run (fold '+str(fold_zerobased+1)+'/' +str(folds)+ '))\\n')\n",
        "            else:\n",
        "                f.write('(Single run, NO k-fold cross validation)\\n')\n",
        "\n",
        "        for metric in metrics.keys():\n",
        "            value = metrics[metric] if fold_zerobased is None else metrics[metric][fold_zerobased]\n",
        "            f.write(str(metric) + \":\\n\" + str(value) + \"\\n\\n\")\n",
        "        f.close()\n",
        "\n",
        "    # Copy Tensorboard Logs\n",
        "    if fold_zerobased == None and DO_SAVE_TENSORBOARD_LOGS:\n",
        "        LOGPATH=outpath+\"/tensorboardlogs\"\n",
        "        shutil.copytree('./logs', LOGPATH)\n",
        "\n",
        "    if not COLAB and fold_zerobased == None:\n",
        "        # Copy script or notebook depending on the execution environment\n",
        "        script_path = None\n",
        "        if is_notebook():\n",
        "            script_path = 'expressive-technique-classifier-phase3.ipynb'\n",
        "            pass #TODO: make this work\n",
        "        else:\n",
        "            script_path = os.path.realpath(__file__)\n",
        "        shutil.copyfile(script_path, os.path.join(outpath, 'backup_'+os.path.basename(script_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0iA0sVlINRz"
      },
      "source": [
        "# Prepare Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTVv1XlUOgBh"
      },
      "outputs": [],
      "source": [
        "if os.path.exists('./logs'):\n",
        "    shutil.rmtree('./logs', ignore_errors=True) #Clear logs if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2NMIioKEYyP"
      },
      "outputs": [],
      "source": [
        "def start_tensorboard(tb_dir,logname):\n",
        "    log_dir = tb_dir\n",
        "    if logname is not None: \n",
        "        log_dir += logname\n",
        "    return tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "tb_dir = \"logs/fit/\"\n",
        "# %tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY1u63rqX7fn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(train_metric, validation_metric, title, xlabel, ylabel, filename=None, show = False):\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.plot(train_metric)\n",
        "    ax.plot(validation_metric)\n",
        "    ax.legend(['Training','Validation'])\n",
        "    if show:\n",
        "        fig.show()\n",
        "    if filename is not None:\n",
        "        plt.savefig(filename+\".pdf\",bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qExZCK5ipvaa"
      },
      "source": [
        "F1-Score on Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hBHvhH2punc"
      },
      "outputs": [],
      "source": [
        "def macroweighted_f1(y_true,y_pred):\n",
        "    f1scores = []\n",
        "    numSamples = []\n",
        "    for selclass in CLASSES:\n",
        "        classSelection = (y_true == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        numSamples.append(sum(classSelection))\n",
        "        classPrediction = (y_pred == (np.ones(np.shape(y_true)[0])*selclass))\n",
        "        true_positives = np.sum(np.logical_and(classSelection,(y_true == y_pred)))\n",
        "\n",
        "        precision = 1.0 * true_positives / np.sum(classPrediction)\n",
        "        recall = 1.0 * true_positives / np.sum(classSelection)\n",
        "        f1score = 2 /((1/precision)+(1/recall))\n",
        "        f1scores.append(f1score)\n",
        "    macroWeightedF1 = sum(np.array(f1scores) * np.array(numSamples)) / sum(numSamples)\n",
        "    return macroWeightedF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SQi2z0Hw2Km"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred,_verbose = False):\n",
        "    accuracy = np.sum(y_pred == y_true)/np.shape(y_true)[0]\n",
        "    f1mw = macroweighted_f1(y_true,y_pred)\n",
        "    confusion_matrix = sk_conf_matrix(y_true, y_pred)\n",
        "    \n",
        "    assert len(y_true) == len(y_pred), 'The \"y_true\" and \"y_pred\" arrays have a different length' \n",
        "\n",
        "    assert len(np.unique(y_true)) >= len(np.unique(y_pred)) \n",
        "    assert np.isin(np.unique(y_pred),np.unique(y_true)).all(), 'Some classes in y_pred are not in y_true ('+str(np.setdiff1d(y_pred,y_true))+')'\n",
        "    \n",
        "    classification_report = sk_class_report(y_true, y_pred, digits=6,target_names = CLASSES_DESC.values(),output_dict=True)\n",
        "    printable_classification_report = sk_class_report(y_true, y_pred, digits=4,target_names = CLASSES_DESC.values())\n",
        "\n",
        "    if _verbose:\n",
        "        print(\"Test Accuracy: \" + str(accuracy) + \"\\nTest macro_weighted_avg f1-score: \" + str(f1mw)+'\\n'+str(confusion_matrix)+'\\n'+str(printable_classification_report))\n",
        "\n",
        "    return accuracy, f1mw, confusion_matrix, classification_report, printable_classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEGBYnUF1vXn"
      },
      "source": [
        "# Prepare TFLite conversion and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAvtjWCwxx1I"
      },
      "outputs": [],
      "source": [
        "# TFLite conversion function\n",
        "def convert2tflite(tf_model_dir,tflite_model_dir = None,model_name=\"model\",quantization=None,dataset=None):\n",
        "    assert (quantization==None or quantization==\"dynamic\" or quantization==\"float-fallback\" or quantization==\"full\")\n",
        "    # Convert the model saved in the previous step.\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "    if quantization is not None:\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        if quantization == \"full\" or quantization==\"float-fallback\":\n",
        "            assert dataset is not None\n",
        "            def representative_dataset():\n",
        "                for data in tf.data.Dataset.from_tensor_slices((dataset)).batch(1).take(100):\n",
        "                    yield [tf.dtypes.cast(data, tf.float32)]\n",
        "            converter.representative_dataset = representative_dataset\n",
        "        if quantization == \"full\":\n",
        "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "            converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "            converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "        if quantization == \"dynamic\":\n",
        "            assert dataset is None\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the TF Lite model.\n",
        "    if tflite_model_dir is None:\n",
        "        TF_MODEL_PATH = tf_model_dir + \"/\" + model_name + '.tflite'\n",
        "    else:\n",
        "        TF_MODEL_PATH = tflite_model_dir + \"/\" + model_name + '.tflite'\n",
        "\n",
        "    with tf.io.gfile.GFile(TF_MODEL_PATH, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "## USAGE\n",
        "# model_path = MODELFOLDER + \"/\" + RUN_NAME + \"/fold_1\"\n",
        "# convert2tflite(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6wGuSE91syp"
      },
      "outputs": [],
      "source": [
        "def test_tflite_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    tflite_interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    input_details = tflite_interpreter.get_input_details()[0]\n",
        "    output_details = tflite_interpreter.get_output_details()[0]\n",
        "    \n",
        "    if verbose_test:\n",
        "        print(\"+--------------------------------------------+\\n| Testing the TF lite model saved            |\\n+--------------------------------------------+\\n\")\n",
        "        print(\"[Model loaded]\\n\")\n",
        "        print(\"\\n== Input details ==\\nname:\"+ str(input_details['name']) + \"\\nshape:\"+str(input_details['shape']) +  \"\\ntype:\"+str(input_details['dtype']))\n",
        "        print(\"\\n== Output details ==\\nname:\"+str(output_details['name']) + \"\\nshape:\"+str(output_details['shape']) + \"\\ntype:\"+str(output_details['dtype']))\n",
        "        print(\"+--------------------------------------------+\\n| Testing on TEST set...                     |\\n+--------------------------------------------+\\n\")\n",
        "    \n",
        "    tflite_interpreter.allocate_tensors()\n",
        "    y_pred = list()\n",
        "    for i in range(X_test.shape[0]):\n",
        "        extracted_test_sample = np.array(X_test[i:i+1]).astype(np.float32)\n",
        "        \n",
        "        # Quantize inputs if necessary (full uint model)\n",
        "        if input_details['dtype'] is np.int8:\n",
        "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "            extracted_test_sample = (extracted_test_sample / input_scale + input_zero_point).astype(np.int8)\n",
        "\n",
        "        if first_layer_is_conv:\n",
        "            input_tensor = np.expand_dims(extracted_test_sample,axis=2).astype(input_details[\"dtype\"])\n",
        "        else:\n",
        "            input_tensor = extracted_test_sample\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Setting \"+str(input_tensor.shape)+\" \"+str(input_tensor.dtype)+\" as input\")\n",
        "\n",
        "        tflite_interpreter.set_tensor(input_details['index'], input_tensor)\n",
        "        tflite_interpreter.invoke()\n",
        "        prediction_vec = tflite_interpreter.get_tensor(output_details['index'])\n",
        "\n",
        "        if verbose_test:\n",
        "            print(\"Getting \"+str(prediction_vec.shape)+\" \"+str(prediction_vec.dtype)+\" as output\")\n",
        "\n",
        "        if output_details['dtype'] is np.int8:\n",
        "            output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "            prediction_vec = (prediction_vec + output_zero_point) * output_scale\n",
        "\n",
        "        if verbose_test:\n",
        "            print(prediction_vec)\n",
        "        y_pred.append(np.argmax(prediction_vec))\n",
        "    return y_pred\n",
        "\n",
        "def test_regulartf_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = False):\n",
        "    imported = tf.keras.models.load_model(model_path)\n",
        "    if first_layer_is_conv:\n",
        "        test_set = np.expand_dims(X_test,axis=2)\n",
        "    else:\n",
        "        test_set = X_test\n",
        "    _, accuracy = imported.evaluate(test_set,  y_test, verbose=2)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPVrRtLoxvu"
      },
      "source": [
        "# k-Fold Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISgdGWqRFkMt"
      },
      "outputs": [],
      "source": [
        "# --> Epochs / Batches\n",
        "print('Using training epochs: ', args['epochs'])\n",
        "print('Using batch size: ', args['batchsize'])\n",
        "\n",
        "# --> Quantize (Dynamic) and test the TF Lite model obtained (quicker but lower accuracy)\n",
        "TEST_QUANTIZATION = False\n",
        "# --> Early Stopping\n",
        "use_early_stopping = False\n",
        "\n",
        "# --> OVERSAMPLING ################################################\n",
        "DO_OVERSAMPLING = args['oversampling']                            #\n",
        "OVERSAMPLING_AGGRESSIVENESS = args['oversampling_aggressiveness'] #\n",
        "VERBOSE_OVERSAMPLING = False                                      #\n",
        "SMOTE_STRATEGY = {} # Do not set this variable\n",
        "###################################################################\n",
        "\n",
        "# --> KFOLD RUN #################################################\n",
        "K_SPLITS = args['k_folds']\n",
        "USE_CROSS_VALIDATION = K_SPLITS > 1 # Activate K-Fold Cross Validation only if K_SPLITS > 1\n",
        "VAL_SPLIT_SIZE = 0.1                                            # percentage of total entries going into the validation set\n",
        "# TODO: this is something to fix.\n",
        "# For the custom splitter that keeps guitarists separate, there\n",
        "# is the need to take the validation set from the test and not \n",
        "# the train set. To do this we now have two percentages.\n",
        "# TODO: change all code to always take the validation percentage\n",
        "# from the test set, so that there can be a single percentage \n",
        "# constant.\n",
        "VAL_SPLIT_SIZE_TESTPERC = 0.3                                   # percentage of test entries going into the validation set\n",
        "random_state = global_random_state                              # seed for pseudo random generator\n",
        "#################################################################\n",
        "\n",
        "# --> SINGLE RUN ################################################\n",
        "SAVE_MODEL_INFO = True                                          #\n",
        "test_split_size = 0.2                                           #\n",
        "#################################################################\n",
        "\n",
        "DO_TEST = False\n",
        "\n",
        "# optimizer = { \"method\" : \"sgd\", \"learning_rate\" : args['learning_rate'], \"momentum\" : 0.7 }\n",
        "optimizer = { \"method\" : \"adam\", \"learning_rate\" : args['learning_rate'] }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGkWC3hVoiz-"
      },
      "outputs": [],
      "source": [
        "def player_stats(metadata,exclude_extra_500 = False):\n",
        "    # metadata = metadata[metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    if exclude_extra_500:\n",
        "        metadata = metadata[~metadata['meta_audiofilePath'].str.contains('500')]\n",
        "    players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el) for el in metadata['meta_audiofilePath']]\n",
        "    players_meta_list = [el[0] for el in players_meta_list]\n",
        "    players = np.unique(players_meta_list).tolist()\n",
        "\n",
        "    print(len(players),'players in the dataset')\n",
        "\n",
        "    for pix, p in enumerate(players):\n",
        "        print(str(pix+1)+' - Player \"'+p+'\" \\thas '+str(players_meta_list.count(p))+' note entries',end='')\n",
        "\n",
        "        # p_records = [el for el in metadata if '_'+p+'_' in metadata['meta_audiofilePath']]\n",
        "\n",
        "        p_records = metadata[metadata['meta_audiofilePath'].str.contains('_'+p+'_')]\n",
        "        assert len(p_records) == players_meta_list.count(p)\n",
        "        count_for_each_tech = p_records.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "        print('  ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in count_for_each_tech.items()]))\n",
        "\n",
        "    tot_count_for_each_tech = metadata.groupby(\"meta_expressive_technique_id\")[\"meta_audiofilePath\"].count().to_dict()\n",
        "    print('_'*170)\n",
        "    print('                                          Tot: ',''.join([str(a)+':'+(' '*(4-len(str(b))))+str(b)+'  \\t' for a,b in tot_count_for_each_tech.items()]))\n",
        "\n",
        "# player_stats(dataset_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxfTUtYgg_nO"
      },
      "outputs": [],
      "source": [
        "class CustomPlayerFold():\n",
        "    def __init__(self, metadata: pd.DataFrame, features: pd.DataFrame, labels: pd.DataFrame, _val_split_size: float):\n",
        "        self.val_split_size = _val_split_size\n",
        "        self.metadata = metadata.copy()\n",
        "        self.features = features.copy()\n",
        "        self.labels = labels.copy()\n",
        "\n",
        "        assert self.metadata.shape[0] == self.features.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.features.shape[0]) \n",
        "        assert self.metadata.shape[0] == self.labels.shape[0] , str(self.metadata.shape[0]) + '!=' + str(self.labels.shape[0]) \n",
        "        assert np.max(self.metadata.index) == len(self.metadata.index)-1, 'np.max(self.metadata.index) == len(self.metadata.index)-1 ('+str(np.max(self.metadata.index))+'!='+str(len(self.metadata.index)-1)+')'\n",
        "        assert np.max(self.features.index) == len(self.features.index)-1, 'np.max(self.features.index) == len(self.features.index)-1 ('+str(np.max(self.features.index))+'!='+str(len(self.features.index)-1)+')'\n",
        "        assert np.max(self.labels.index) == len(self.labels.index)-1, 'np.max(self.labels.index) == len(self.labels.index)-1 ('+str(np.max(self.labels.index))+'!='+str(len(self.labels.index)-1)+')'\n",
        "\n",
        "        players_meta_list = [re.findall('[A-Z][a-z][a-z][A-Z][a-z][a-z][0-2]?',el)[0] for el in metadata['meta_audiofilePath']]\n",
        "        self.players = np.unique(players_meta_list).tolist()\n",
        "        self.k_splits = len(self.players)\n",
        "\n",
        "        # player_stats(metadata)\n",
        "\n",
        "    def get_k_splits(self,):\n",
        "        return self.k_splits\n",
        "\n",
        "    def split(self,_X,_y):\n",
        "        res = []\n",
        "\n",
        "        assert (_X == self.features.to_numpy()).all()\n",
        "        assert (_y == self.labels.to_numpy()).all()\n",
        "\n",
        "        for player in self.players:\n",
        "            p_records = self.metadata[self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "            not_p_records = self.metadata[~self.metadata['meta_audiofilePath'].str.contains('_'+player+'_')]\n",
        "\n",
        "            train_idx = not_p_records.index.values\n",
        "            test_idx = p_records.index.values\n",
        "\n",
        "            # print('test_idx',test_idx)\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "\n",
        "            stratify_labels = p_records['meta_expressive_technique_id'].tolist()\n",
        "            \n",
        "            # In this case, we want to take the validation split from the TEST split,\n",
        "            # unlike all the other cases where we take it from the test set.\n",
        "            # This is because we want the validation results to highlight GENERALIZATION.\n",
        "            # So we split the test and validation indexes here\n",
        "\n",
        "            # To keep things somewhat consistent, we take 'val_split_size', which is supposed to be a percentage of the train set, and convert it so that it is the same number of samples, when extracted from the test set\n",
        "            # new_valsplit_perc = self.val_split_size * len(train_idx) / len(test_idx)\n",
        "            # print('val_split_size:',self.val_split_size)\n",
        "            # print('len(train_idx):',len(train_idx))\n",
        "            # print('len(test_idx):', len(test_idx))\n",
        "            # assert new_valsplit_perc <= 1.0, 'new_valsplit_perc > 1.0 ('+str(new_valsplit_perc)+')'\n",
        "            # assert new_valsplit_perc >= 0.0, 'new_valsplit_perc < 0.0 ('+str(new_valsplit_perc)+')'\n",
        "\n",
        "            test_idx,val_idx = train_test_split(test_idx,test_size=self.val_split_size,random_state=random_state, shuffle=True, stratify = stratify_labels)\n",
        "\n",
        "            # print('len(val_idx)/(len(val_idx)+len(test_idx))',len(val_idx)/(len(val_idx)+len(test_idx)))\n",
        "            # print()\n",
        "\n",
        "            # print('len(val_idx)',len(val_idx))\n",
        "            # print('len(test_idx)',len(test_idx))\n",
        "            \n",
        "            # print()\n",
        "            # print()\n",
        "\n",
        "\n",
        "            assert type(train_idx) == type(val_idx) == type(test_idx)\n",
        "            assert np.all(train_idx < len(self.labels))\n",
        "            assert np.all(val_idx < len(self.labels))\n",
        "            assert np.all(test_idx < len(self.labels))\n",
        "            assert np.array_equal(np.unique(train_idx),sorted(train_idx))\n",
        "            assert np.array_equal(np.unique(test_idx),sorted(test_idx))\n",
        "            assert np.array_equal(np.unique(val_idx),sorted(val_idx))\n",
        "\n",
        "            res.append((train_idx,test_idx,val_idx))\n",
        "\n",
        "            players_intest = np.unique([re.findall(r'_([A-Z][a-z][a-z][A-Z][a-z][a-z][0-9]?)_',dat) for dat in self.metadata.iloc[test_idx]['meta_audiofilePath'].tolist()])\n",
        "            assert len(players_intest) == 1, 'There should be only one player in the test set. Instead, there are '+str(len(players_intest))+' players in the test set: '+str(players_intest)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1hUwlybmgEE"
      },
      "outputs": [],
      "source": [
        "def oversample(features: list, labels: list, aggressiveness = 1, verbose: bool = False):\n",
        "    if verbose:\n",
        "        print(\"Oversampling...\")\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        target_count = int(round(max(counts) * aggressiveness,0))\n",
        "        sampling_strategy = dict(zip(unique, counts))\n",
        "        sampling_strategy = {k:(v if v > target_count else target_count) for k,v in sampling_strategy.items()}\n",
        "        smote_strategy = sampling_strategy\n",
        "\n",
        "        print('sampling_strategy:',sampling_strategy)\n",
        "\n",
        "        ovs_features, ovs_labels = imblearn.over_sampling.SMOTE(sampling_strategy=sampling_strategy).fit_resample(features, labels)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Increased training samples from \" + str(features.shape[0]) + \" to \" + str(ovs_features.shape[0]))\n",
        "            printSupport(ovs_labels)\n",
        "    return ovs_features, ovs_labels, smote_strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy9WiiwEpHXf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "prefix = \"CrossValidated\" if USE_CROSS_VALIDATION else \"Single\"\n",
        "RUN_NAME = prefix + \"Run_\"+strftime(\"%Y%m%d-%H%M%S\")\n",
        "OUTPUT_DIR = os.path.join(MODELFOLDER,RUN_NAME)\n",
        "os.makedirs(OUTPUT_DIR,exist_ok = True) \n",
        "\n",
        "\n",
        "if PREVIEW_MODEL:\n",
        "    model_prev = define_model_architecture(len(CLASSES))\n",
        "    model_prev.summary()\n",
        "    with open(os.path.join(OUTPUT_DIR, 'model_summary.txt'), 'w') as f:\n",
        "        model_prev.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "\n",
        "if CUSTOM_PLAYER_K_FOLD:\n",
        "    print('Warning! Using custom K-Fold, which splits the dataset in the data from different players.')\n",
        "    cv = CustomPlayerFold(dataset_metadata,dataset_features,dataset_labels,_val_split_size=VAL_SPLIT_SIZE_TESTPERC)\n",
        "    K_SPLITS = cv.get_k_splits()\n",
        "    print('as a result, the number of folds has been OVERWRITTEN and is now ',K_SPLITS)\n",
        "\n",
        "else:\n",
        "    cv = StratifiedKFold(n_splits=K_SPLITS,shuffle=True,random_state=random_state)\n",
        "\n",
        "def main_routine(X,y,train_idx=None,test_idx=None,val_idx=None,_val_split_size=None,aug_data=None,foldcount=None,is_k_fold=False, eval_metrics=None, quantized_eval_metrics=None):\n",
        "    assert np.equal(np.sort(CLASSES),np.unique(y)).all(), 'np.sort(CLASSES) != np.unique(y)  ('+np.sort(CLASSES)+'!='+np.unique(y)+')'\n",
        "\n",
        "    np.random.shuffle(train_idx)   #TODO: remove if this turns out to be useless\n",
        "    np.random.shuffle(test_idx)    #TODO: remove if this turns out to be useless\n",
        "    np.random.shuffle(val_idx) #TODO: remove if this turns out to be useless\n",
        "    \n",
        "    if eval_metrics is None:\n",
        "        raise ValueError(\"provide a eval_metrics dict\")\n",
        "\n",
        "    current_dir = MODELFOLDER + \"/\" + RUN_NAME\n",
        "    # %mkdir -p \"$current_dir\"\n",
        "    os.makedirs(current_dir,exist_ok = True) \n",
        "    if is_k_fold:\n",
        "        fold_dir = current_dir + '/Fold_' + str(foldcount)\n",
        "        # %mkdir -p \"$fold_dir\"\n",
        "        os.makedirs(fold_dir,exist_ok = True)\n",
        "\n",
        "    ### PRINT INFO\n",
        "    if is_k_fold:\n",
        "        print(\"\\nFold [\"+str(foldcount)+\"/\"+str(K_SPLITS)+\"]\")\n",
        "        #### SPLIT DATA\n",
        "        print(\"Selecting Split Data...\")\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_test, y_test = X[test_idx], y[test_idx]\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "    else:\n",
        "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_split_size,random_state=random_state, shuffle=True, stratify = y)\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "\n",
        "    if val_idx is None:\n",
        "        print('Performing generic validation/train split from training split')    \n",
        "        X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=_val_split_size,random_state=random_state, shuffle=True, stratify = y_train)\n",
        "        # printSupport(y_train)                                                       # Verify that the split is STRATIFIED\n",
        "    else:\n",
        "        print('Taking validation split from provided indexes')    \n",
        "        X_valid, y_valid = X[val_idx], y[val_idx]\n",
        "        # printSupport(y_valid)                                                       # Verify that the split is STRATIFIED\n",
        "\n",
        "    ### NORMALIZE DATA\n",
        "    if DO_NORMALIZE_DATA:\n",
        "        print(\"Normalizing Data...\")\n",
        "        scaler = SCALER_TO_USE\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test =  scaler.transform(X_test)\n",
        "        X_valid = scaler.transform(X_valid)\n",
        "        scaler.fit([[0]]) # \"reset\" scaler\n",
        "        print(\"Done.\")\n",
        "\n",
        "\n",
        "\n",
        "    ### OVERSAMPLE\n",
        "    if VERBOSE_OVERSAMPLING:\n",
        "        if DO_OVERSAMPLING:\n",
        "            print(\"Oversampling...\")\n",
        "        else:\n",
        "            print('NOT performing oversampling')\n",
        "    if DO_OVERSAMPLING:\n",
        "        # VERBOSE_OVERSAMPLING = True\n",
        "        # with warnings.catch_warnings():\n",
        "        #     warnings.simplefilter(\"ignore\")\n",
        "        #     if VERBOSE_OVERSAMPLING:\n",
        "        #         prev_len = y_train.shape[0]\n",
        "        #     X_train, y_train = SMOTE(smote_mask).fit_sample(X_train, y_train)\n",
        "        #     if VERBOSE_OVERSAMPLING:\n",
        "        #         print(\"Increased training samples from \" + str(prev_len) + \" to \" + str(y_train.shape[0]))\n",
        "        #         printSupport(y_train)\n",
        "\n",
        "        X_train, y_train, smote_strategy = oversample(X_train, y_train, aggressiveness = OVERSAMPLING_AGGRESSIVENESS, verbose = VERBOSE_OVERSAMPLING)\n",
        "\n",
        "    ### ADD AUGMENTED DATA\n",
        "    assert not USE_AUGMENTED_DATA or (aug_data != None), 'USE_AUGMENTED_DATA should imply that aug_data is not None, however this is not the case'\n",
        "\n",
        "    if USE_AUGMENTED_DATA:\n",
        "        (metadata_aug, features_aug, labels_aug, train_aug_idx) = aug_data\n",
        "        X_train_aug, y_train_aug = features_aug.to_numpy()[train_aug_idx], labels_aug.to_numpy()[train_aug_idx]\n",
        "        print('Adding augmented data to training set...')\n",
        "        print('Length of training set before augmentation: ',len(X_train))\n",
        "        X_train = np.concatenate((X_train,X_train_aug),axis=0)\n",
        "        y_train = np.concatenate((y_train,y_train_aug),axis=0)\n",
        "        print('Length of training set after augmentation: ',len(X_train))\n",
        "        \n",
        "\n",
        "    ### DEFINE MODEL\n",
        "    model = define_model_architecture(len(CLASSES),_verbose = True)\n",
        "    \n",
        "    ### DEFINE LOSS\n",
        "    loss_fn = get_loss()\n",
        "\n",
        "    ### PREPARE DATA IN CASE OF A FIRST CONV LAYER IN THE NET\n",
        "    if type(model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "        X_train = np.expand_dims(X_train,axis = 2) # Adapt data for Conv1d ([batch_shape, steps, input_dim] -> in our case indim = 1, steps = features, batchshape = train datset size)\n",
        "        X_valid= np.expand_dims(X_valid,axis = 2)  # Adapt data for Conv1d\n",
        "        X_test= np.expand_dims(X_test,axis = 2)    # Adapt data for Conv1d\n",
        "\n",
        "    ### PERFORM TEST (**OPTIONAL)\n",
        "    if DO_TEST:\n",
        "        predictions = model(X_test[:1].astype('float32')).numpy()\n",
        "        print(\"Predictions: \" + str(predictions) + \"\\nWith Softmax: \" + str(tf.nn.softmax(predictions).numpy()) + \"\\nLoss: \" + str(loss_fn(y_test[:1], predictions).numpy()))\n",
        "\n",
        "    ### COMPILE MODEL\n",
        "    compile_model(model,optimizer,loss_fn,_verbose = True)\n",
        "\n",
        "    ### SETUP TENSORBOARD\n",
        "    tensorboard_callback = start_tensorboard(tb_dir,\"Fold_\"+str(foldcount) if is_k_fold else None)\n",
        "    callbacks=[tensorboard_callback,]\n",
        "    \n",
        "    ### SETUP EARLY STOPPING (only if not in K-fold mode)\n",
        "    if is_k_fold is False and use_early_stopping:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200))\n",
        "\n",
        "    # * FIT MODEL *\n",
        "    history = model.fit(X_train, y_train, epochs=args['epochs'], validation_data = (X_valid,y_valid),\n",
        "                        callbacks=callbacks,\n",
        "                        batch_size=args['batchsize'])\n",
        "    # Plot history\n",
        "    plot_folder = fold_dir if is_k_fold else current_dir\n",
        "    getval = lambda metric: history.history[metric]\n",
        "    plot_history(getval('accuracy'),getval('val_accuracy'), \"Training and validation accuracy\",\"Epochs\",\"Accuracy\", filename = plot_folder + \"/AccuracyPlot\")\n",
        "    plot_history(getval('loss'),    getval('val_loss'),     \"Training and validation loss\",\"Epochs\",\"Accuracy\",     filename = plot_folder + \"/LossPlot\")\n",
        "    plt.close()\n",
        "    plt.ioff()\n",
        "\n",
        "    # * TEST MODEL *\n",
        "    # keras_test_loss, keras_test_accuracy = model.evaluate(X_test,  y_test, verbose=2) # Keras solution, might not be needed\n",
        "    y_true = np.squeeze(y_test)\n",
        "    y_pred = np.argmax(model(X_test),axis=1)\n",
        "    cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, \\\n",
        "                                                                                                 y_pred, \\\n",
        "                                                                                                 _verbose=True)\n",
        "    eval_metrics[\"accuracy\"].append(cm_acc)\n",
        "    eval_metrics[\"f1_weightedmacroavg\"].append(f1mw)\n",
        "    eval_metrics[\"confusion_matrix\"].append(cm_conf_matrix)\n",
        "    eval_metrics[\"classification_report\"].append(cm_classf_report)\n",
        "    eval_metrics[\"printable_classification_report\"].append(cm_printable_classf_report)\n",
        "\n",
        "    SAVED_MODEL_PATH = None\n",
        "    if is_k_fold:\n",
        "        # Save fold history\n",
        "        with open(fold_dir+\"/history_fold_\"+str(foldcount)+\".pickle\",'wb') as picklefile:\n",
        "            pickle.dump(history.history,picklefile)\n",
        "\n",
        "        # Save the fold models only if we want them, or need them to test (in the second case they will be deleted after test)        \n",
        "        if TEST_QUANTIZATION or DO_SAVE_FOLD_MODELS:\n",
        "            model.save(fold_dir)\n",
        "            SAVED_MODEL_PATH = fold_dir\n",
        "    else:\n",
        "        assert len(eval_metrics['accuracy']) == 1\n",
        "        \n",
        "        # Save the entire model as a SavedModel.\n",
        "        if TEST_QUANTIZATION or DO_SAVE_FOLD_MODELS:\n",
        "            model.save(current_dir)\n",
        "            SAVED_MODEL_PATH = current_dir\n",
        "        with open(current_dir+\"/history.pickle\",'wb') as picklefile:\n",
        "            pickle.dump(history.history,picklefile)\n",
        "\n",
        "    if TEST_QUANTIZATION:\n",
        "        assert quantized_eval_metrics is not None\n",
        "        model_filename = 'partially_quantized_test_model'\n",
        "        # Convert and save lite model\n",
        "        convert2tflite(SAVED_MODEL_PATH,model_name=model_filename,quantization=\"dynamic\")\n",
        "        # Load and Test lite model\n",
        "        y_quant_pred = test_tflite_model(SAVED_MODEL_PATH+'/'+model_filename+'.tflite',X_test,y_test,type(model.layers[0]) == tf.keras.layers.Conv1D,verbose_test = False)\n",
        "        # Compute Test Metrics\n",
        "        cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, \\\n",
        "                                                                                                     y_quant_pred, \\\n",
        "                                                                                                     _verbose=True)\n",
        "        quantized_eval_metrics[\"accuracy\"].append(cm_acc)\n",
        "        quantized_eval_metrics[\"f1_weightedmacroavg\"].append(f1mw)\n",
        "        quantized_eval_metrics[\"confusion_matrix\"].append(cm_conf_matrix)\n",
        "        quantized_eval_metrics[\"classification_report\"].append(cm_classf_report)\n",
        "        quantized_eval_metrics[\"printable_classification_report\"].append(cm_printable_classf_report)\n",
        "\n",
        "    if not DO_SAVE_FOLD_MODELS:\n",
        "        if os.path.exists(os.path.join(fold_dir,'assets')):\n",
        "            shutil.rmtree(os.path.join(fold_dir,'assets'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'variables')):\n",
        "            shutil.rmtree(os.path.join(fold_dir,'variables'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'savedmodel.pb')):\n",
        "            os.remove(os.path.join(fold_dir,'savedmodel.pb'))\n",
        "        if os.path.exists(os.path.join(fold_dir,'keras_metadata.pb')):\n",
        "            os.remove(os.path.join(fold_dir,'keras_metadata.pb'))\n",
        "        if TEST_QUANTIZATION and os.path.exists(os.path.join(fold_dir,'partially_quantized_test_model.tflite')):\n",
        "            os.remove(os.path.join(fold_dir,'partially_quantized_test_model.tflite'))\n",
        "\n",
        "    # Now that all the tests are performed, all the info can be saved\n",
        "\n",
        "    if is_k_fold:\n",
        "        metrics_to_save = {}\n",
        "        metrics_to_save.update({'def_model_'+key:value for (key,value) in eval_metrics.items()})\n",
        "        if TEST_QUANTIZATION:\n",
        "            metrics_to_save.update({'quant_model_'+key:value for (key,value) in quantized_eval_metrics.items()})\n",
        "\n",
        "        # save_fold_info(model,optimizer,foldcount,K_SPLITS,X_test.shape[0],eval_metrics,list(dataset_features.columns),fold_dir)\n",
        "        save_model_info(model,\n",
        "                        optimizer,\n",
        "                        False,K_SPLITS,\n",
        "                        metrics_to_save,\n",
        "                        fold_dir,\n",
        "                        fold_zerobased=foldcount-1,\n",
        "                        smote_strategy = smote_strategy)\n",
        "\n",
        "    SMOTE_STRATEGY = smote_strategy\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwKI-vRUdDs0"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for split_idx, (train_idx, test_idx, val_idx) in enumerate(cv.split(dataset_features.to_numpy(), dataset_labels.to_numpy())):\n",
        "#     if USE_AUGMENTED_DATA:\n",
        "#         # Here we want to select the augmented data that corresponds to the current training split\n",
        "#         training_metadata = metadata.iloc[train_idx]\n",
        "#         training_aug_indices = []\n",
        "        \n",
        "#         sources_and_times_aug = metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].values\n",
        "#         training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "\n",
        "#         for idx, (source_aug, time_aug) in enumerate(sources_and_times_aug):\n",
        "#             if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "#                 training_aug_indices.append(idx)\n",
        "                \n",
        "# Faster code\n",
        "# for split_idx, (train_idx, test_idx, val_idx) in enumerate(cv.split(dataset_features.to_numpy(), dataset_labels.to_numpy())):\n",
        "#     training_aug_indices = []\n",
        "#     if USE_AUGMENTED_DATA:\n",
        "#         # Here we want to select the augmented data that corresponds to the current training split\n",
        "#         training_metadata = metadata.iloc[train_idx]\n",
        "#         training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "\n",
        "#         for idx,source_aug,time_aug in  metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].itertuples():\n",
        "#             if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "#                 training_aug_indices.append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orQZbq4df5jK"
      },
      "outputs": [],
      "source": [
        "'''Call the main routine for each fold'''\n",
        "result_model = []\n",
        "train_aug_indexes = {}\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    metric_names = ['accuracy','f1_weightedmacroavg','confusion_matrix','classification_report','printable_classification_report']\n",
        "    evaluation_metrics, quantized_model_evaluation_metrics = {k:[] for k in metric_names}, {k:[] for k in metric_names}\n",
        "    \n",
        "    X,y = dataset_features.to_numpy(), dataset_labels.to_numpy()\n",
        "\n",
        "    if USE_CROSS_VALIDATION:\n",
        "        for foldidx,split_folds in enumerate(cv.split(X, y)): \n",
        "            if CUSTOM_PLAYER_K_FOLD:\n",
        "                train_idx, test_idx, val_idx  = split_folds\n",
        "            else:\n",
        "                train_idx, test_idx = split_folds\n",
        "                val_idx = None\n",
        "\n",
        "            train_aug_idx = []  # Indexes of the augmented data that correspond to the current training split\n",
        "            if USE_AUGMENTED_DATA: # Here we want to select the augmented data that corresponds to the current training split\n",
        "                print('Selecting augmented data for fold',foldidx,'...')\n",
        "                training_metadata = metadata.iloc[train_idx]\n",
        "                training_sources_and_times = {tuple(k):True for k in training_metadata[['meta_audiofilePath','meta_onsetGroundTruthLabelTime']].values}\n",
        "                \n",
        "                for idx,source_aug,time_aug in  metadata_aug[['meta_augmentation_source','meta_onsetGroundTruthLabelTime']].itertuples():\n",
        "                    if((source_aug,time_aug) in training_sources_and_times.keys()):\n",
        "                        train_aug_idx.append(idx)\n",
        "                assert len(train_aug_idx) > 0, \"No augmented data found for the current training split\"\n",
        "                print('Found',len(train_aug_idx),'augmented samples for fold',foldidx)\n",
        "            train_aug_indexes[foldidx] = train_aug_idx\n",
        "            # TODO: wait there might be a bug here, we are not using the augmented data \n",
        "            result_model.append(main_routine(X, y,\n",
        "                                             train_idx = train_idx,\n",
        "                                             test_idx  = test_idx,\n",
        "                                             val_idx   = val_idx,\n",
        "                                             aug_data = None if not USE_AUGMENTED_DATA else (metadata_aug, features_aug, labels_aug, train_aug_idx),\n",
        "                                             foldcount = foldidx+1,\n",
        "                                             is_k_fold = USE_CROSS_VALIDATION,\n",
        "                                             eval_metrics           = evaluation_metrics,\n",
        "                                             quantized_eval_metrics = quantized_model_evaluation_metrics))        \n",
        "    else:\n",
        "        result_model = main_routine(X, y,\n",
        "                                    eval_metrics           = evaluation_metrics,\n",
        "                                    quantized_eval_metrics = quantized_model_evaluation_metrics,\n",
        "                                    _val_split_size        = VAL_SPLIT_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLTufCRmlpIW"
      },
      "source": [
        "# Cross Validation average results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqw9k1wlWZC"
      },
      "source": [
        "## Utilities for reports and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvpD2bFcYcCi"
      },
      "outputs": [],
      "source": [
        "def report_average(reports):\n",
        "    mean_dict = dict()\n",
        "    for label in reports[0].keys():\n",
        "        dictionary = dict()\n",
        "\n",
        "        if label in 'accuracy':\n",
        "            mean_dict[label] = sum(d[label] for d in reports) / len(reports)\n",
        "            continue\n",
        "\n",
        "        for key in reports[0][label].keys():\n",
        "            dictionary[key] = sum(d[label][key] for d in reports) / len(reports)\n",
        "        mean_dict[label] = dictionary\n",
        "\n",
        "    return mean_dict\n",
        "\n",
        "def classification_report_dict2print(report):\n",
        "    ret = \"\"\n",
        "    classes = list(report.keys())[0:-3]\n",
        "    summary_metrics = list(report.keys())[-3:]\n",
        "    longest_1st_column_name = max([len(key) for key in report.keys()])\n",
        "    ret = ' ' * longest_1st_column_name\n",
        "    ret += '  precision    recall  f1-score   support\\n\\n'\n",
        "\n",
        "    METRIC_DECIMAL_DIGITS = 4\n",
        "    metric_digits = METRIC_DECIMAL_DIGITS + 2 # add 0 and dot\n",
        "\n",
        "    header_spacing = 1\n",
        "    metrics = list(report[classes[0]].keys())\n",
        "    longest_1st_row_name = max([len(key) for key in report[classes[0]].keys()]) + header_spacing\n",
        "\n",
        "    for classname in classes:\n",
        "        ret += (' '*(longest_1st_column_name-len(classname))) + classname + ' '\n",
        "        for metric in metrics:\n",
        "            if metric != \"support\":\n",
        "                ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "                ret += \"%.4f\" % round(report[classname][metric],METRIC_DECIMAL_DIGITS)\n",
        "            else:\n",
        "                current_support_digits = len(str(int(report[classname][metric])))\n",
        "                ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "                ret += \"%d\" % round(report[classname][metric],0)\n",
        "        ret += '\\n'\n",
        "    ret += '\\n'\n",
        "\n",
        "    # Accuracy\n",
        "    ret += (' '*(longest_1st_column_name-len(summary_metrics[0]))) + summary_metrics[0] + ' '\n",
        "    ret += 2* (' '*longest_1st_row_name)\n",
        "    ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "    ret += \"%.4f\" % round(report[\"accuracy\"],METRIC_DECIMAL_DIGITS)\n",
        "    current_support_digits = len(str(int(report[summary_metrics[-1]]['support'])))\n",
        "    ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "    ret += \"%d\" % round(report[summary_metrics[-1]]['support'],0)\n",
        "    ret += '\\n'\n",
        "  \n",
        "  \n",
        "    for classname in summary_metrics[1:]:\n",
        "        ret += (' '*(longest_1st_column_name-len(classname))) + classname + ' '\n",
        "        for metric in metrics:\n",
        "            if metric != \"support\":\n",
        "                ret += (' '*(longest_1st_row_name-metric_digits))\n",
        "                ret += \"%.4f\" % round(report[classname][metric],METRIC_DECIMAL_DIGITS)\n",
        "            else:\n",
        "                current_support_digits = len(str(int(report[classname][metric])))\n",
        "                ret += (' '*(longest_1st_row_name-current_support_digits))\n",
        "                ret += \"%d\" % round(report[classname][metric],0)\n",
        "        ret += '\\n'\n",
        "    ret += '\\n'\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vw1nmcpmApM"
      },
      "source": [
        "## Compute average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-xQAAnQu6BO"
      },
      "outputs": [],
      "source": [
        "if USE_CROSS_VALIDATION:\n",
        "    assert len(evaluation_metrics['accuracy']) == K_SPLITS, \"The number of accuracy values does not match the number of folds ({} != {})\".format(len(evaluation_metrics['accuracy']),K_SPLITS)\n",
        "    \n",
        "    averaged_classification_reports = report_average(evaluation_metrics[\"classification_report\"])\n",
        "    macro_avg_f1_score = averaged_classification_reports[\"macro avg\"][\"f1-score\"]\n",
        "    average_fold_accuracy = averaged_classification_reports[\"accuracy\"]\n",
        "    printable_avg_report = classification_report_dict2print(averaged_classification_reports)\n",
        "    qm_printable_avg_report = \"Not performed\"\n",
        "    if TEST_QUANTIZATION:\n",
        "        qm_printable_avg_report = classification_report_dict2print(report_average(quantized_model_evaluation_metrics[\"classification_report\"]))\n",
        "    metrics_to_save = {'macro avg f1-score' : macro_avg_f1_score,\\\n",
        "                       'average_fold_accuracy' : average_fold_accuracy,\\\n",
        "                       'avg_classification_report' : printable_avg_report,\\\n",
        "                       'avg_classification_report_for_quantized_model' : qm_printable_avg_report}\n",
        "else:\n",
        "    assert len(evaluation_metrics['accuracy']) == 1\n",
        "    metrics_to_save = {}\n",
        "    for metric in evaluation_metrics.keys():\n",
        "        metrics_to_save[metric] = evaluation_metrics[metric][0]\n",
        "    for metric in quantized_model_evaluation_metrics.keys():\n",
        "        metrics_to_save['quantizedmod_'+str(metric)] = quantized_model_evaluation_metrics[metric][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sYtK9DZu64h"
      },
      "source": [
        "# Save Model Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNDoHM4gXw5x"
      },
      "outputs": [],
      "source": [
        "if SAVE_MODEL_INFO:\n",
        "    current_dir = MODELFOLDER + \"/\" + RUN_NAME\n",
        "    # %mkdir -p \"$current_dir\"\n",
        "    os.makedirs(current_dir,exist_ok = True)\n",
        "\n",
        "    save_model_info(result_model[0] if type(result_model) == list else result_model,\n",
        "                    optimizer,\n",
        "                    USE_CROSS_VALIDATION,K_SPLITS,\n",
        "                    metrics_to_save,\n",
        "                    current_dir)\n",
        "else:\n",
        "    print(\"RESULTS\\n\\n\" + metrics_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7ZdTjKVcE3"
      },
      "source": [
        "# Train final model on the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_dir = os.path.join(MODELFOLDER,RUN_NAME)\n",
        "assert os.path.exists(run_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT6_bSsfVbiH"
      },
      "outputs": [],
      "source": [
        "if TRAIN_FINAL_MODEL:\n",
        "    use_early_stopping = False\n",
        "\n",
        "    ### DEFINE MODEL\n",
        "    final_model = define_model_architecture(len(CLASSES),_verbose = True)\n",
        "    loss_fn = get_loss()\n",
        "\n",
        "    ### Normalize data if needed\n",
        "    if DO_NORMALIZE_DATA:\n",
        "        scaler = SCALER_TO_USE\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        # Save final scaler and parameters\n",
        "        with open(os.path.join(run_dir,'scaler.pickle'),'wb') as sf:\n",
        "            pickle.dump(scaler,sf)\n",
        "        with open(os.path.join(run_dir,'info.txt'),'a') as infof:\n",
        "            if type(SCALER_TO_USE) == MinMaxScaler:\n",
        "                infof.write(\"The scaler used was sklearn.preprocessing MinMaxScaler\\nScaler parameters:\\n\")\n",
        "\n",
        "                infof.write('MinMaxScaler().data_min_: '        + str(SCALER_TO_USE.data_min_)+'\\n')\n",
        "                infof.write('MinMaxScaler().data_max_: '        + str(SCALER_TO_USE.data_max_)+'\\n')\n",
        "                infof.write('MinMaxScaler().data_range_: '      + str(SCALER_TO_USE.data_range_)+'\\n')\n",
        "                infof.write('MinMaxScaler().scale_: '           + str(SCALER_TO_USE.scale_)+'\\n')\n",
        "                infof.write('MinMaxScaler().n_samples_seen_: '  + str(SCALER_TO_USE.n_samples_seen_)+'\\n')\n",
        "            elif type(SCALER_TO_USE) == StandardScaler:\n",
        "                infof.write(\"The scaler used was sklearn.preprocessing StandardScaler\\nScaler parameters:\\n\")\n",
        "                infof.write('StandardScaler().mean_: '          + str(SCALER_TO_USE.mean_)+'\\n')\n",
        "                infof.write('StandardScaler().var_: '           + str(SCALER_TO_USE.var_)+'\\n')\n",
        "                infof.write('StandardScaler().scale_: '         + str(SCALER_TO_USE.scale_)+'\\n')\n",
        "                infof.write('StandardScaler().n_samples_seen_: '+ str(SCALER_TO_USE.n_samples_seen_)+'\\n')\n",
        "            else:\n",
        "                raise Exception(\"\\\"%s\\\" scaler not supported\"%(SCALER_TO_USE))\n",
        "\n",
        "    ### PREPARE DATA IN CASE OF A FIRST CONV LAYER IN THE NET\n",
        "    if type(final_model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "        X_all = np.expand_dims(X,axis = 2) # Adapt data for Conv1d ([batch_shape, steps, input_dim] -> in our case indim = 1, steps = features, batchshape = train datset size)\n",
        "    else:\n",
        "        X_all = X\n",
        "\n",
        "    ### COMPILE MODEL\n",
        "    compile_model(final_model,optimizer,loss_fn,_verbose = True)\n",
        "\n",
        "    ### SETUP TENSORBOARD\n",
        "    tensorboard_callback = start_tensorboard(tb_dir,None)\n",
        "    callbacks=[tensorboard_callback,]\n",
        "\n",
        "    ### SETUP EARLY STOPPING (only if not in K-fold mode)\n",
        "    if use_early_stopping:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='min', verbose=1, patience=200))\n",
        "\n",
        "    # * FIT MODEL *\n",
        "    final_model.fit(X_all, y, epochs=args['epochs'],\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=args['batchsize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXJ9wBMbb4sp"
      },
      "outputs": [],
      "source": [
        "final_model_dir = MODELFOLDER + \"/\" + RUN_NAME + \"/finalModel\"\n",
        "# %mkdir -p \"$final_model_dir\"\n",
        "os.makedirs(final_model_dir,exist_ok = True)\n",
        "\n",
        "final_model.save(final_model_dir)\n",
        "\n",
        "# Convert and save lite model (Non quantized)\n",
        "convert2tflite(final_model_dir,model_name='final_model',quantization=None)\n",
        "# Convert and save lite model (Dynamically quantized)\n",
        "convert2tflite(final_model_dir,model_name='final_model_dynquant',quantization=\"dynamic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pBrSDphxyxL"
      },
      "source": [
        "# Save the model for TF Lite\n",
        "## *(Only if not a Cross Validated run)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ScdRpC-m_J"
      },
      "outputs": [],
      "source": [
        "# if USE_CROSS_VALIDATION is False:\n",
        "#     model_path = MODELFOLDER + \"/\" + RUN_NAME\n",
        "#     convert2tflite(model_path)                                                # standard TFLITE model\n",
        "#     convert2tflite(model_path,model_name=\"model_partially_quantized\",quantization=\"dynamic\")   # Partial quantization  https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization\n",
        "    \n",
        "#     quantization_dataset = X\n",
        "#     if type(result_model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "#         quantization_dataset = np.expand_dims(X,axis = 2) # Adapt data for Conv1d\n",
        "    \n",
        "#     convert2tflite(model_path,model_name=\"model_float_fallback\",quantization=\"float-fallback\",dataset=quantization_dataset) # https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_float_fallback_quantization\n",
        "#     convert2tflite(model_path,model_name=\"model_fully_quantized\",quantization=\"full\",dataset=quantization_dataset)          # FULL uint8 quantization https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnPqZ8k4box0"
      },
      "outputs": [],
      "source": [
        "# first_layer_is_conv = (type(result_model.layers[0]) == tf.keras.layers.Conv1D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QdCLmkgzonI"
      },
      "outputs": [],
      "source": [
        "# TEST_SAVED_MODEL = None\n",
        "# # TEST_SAVED_MODEL = 'model.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_partially_quantized.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_float_fallback.tflite'\n",
        "# # TEST_SAVED_MODEL = 'model_fully_quantized.tflite'\n",
        "# # TEST_SAVED_MODEL = 'quant_aware_model.tflite'\n",
        "# # TEST_SAVED_MODEL = 'saved_model.pb'\n",
        "# verbose_test = False\n",
        "\n",
        "# def test_generic_model(model_filename,model_path,X_test,Y_test,first_layer_is_conv,verbose_test = False):\n",
        "#     if model_filename.split('.')[-1] == 'tflite':\n",
        "#         y_pred = test_tflite_model(model_path+'/'+model_filename,X_test,y_test,first_layer_is_conv,verbose_test = verbose_test)\n",
        "#         correct = np.count_nonzero((np.array(y_pred) == np.ravel(y_test)).astype(int))\n",
        "#         total = np.shape(y_test)[0]\n",
        "#         accuracy = round(correct/total,4)\n",
        "#     elif model_filename.split('.')[-1] == 'pb':\n",
        "#         accuracy = test_regulartf_model(model_path,X_test,y_test,first_layer_is_conv,verbose_test = verbose_test)\n",
        "#     else:\n",
        "#         raise ValueError(\"\")\n",
        "\n",
        "#     return accuracy\n",
        "\n",
        "# if USE_CROSS_VALIDATION is False and TEST_SAVED_MODEL is not None:\n",
        "#     assert np.max([len(ev_metric) for ev_metric in evaluation_metrics]) == K_SPLITS\n",
        "\n",
        "#     target_accuracy = evaluation_metrics['accuracy'][0]\n",
        "#     accuracy = test_generic_model(TEST_SAVED_MODEL,model_path,X_test,Y_test,first_layer_is_conv)\n",
        "\n",
        "#     epsilon = 1e-4\n",
        "#     EQUAL_ACCURACY = abs(target_accuracy - accuracy) < epsilon\n",
        "\n",
        "#     print(\"accuracy: \" + str(accuracy))\n",
        "\n",
        "#     if EQUAL_ACCURACY:\n",
        "#         print(\"Accuracy of the original model and the saved TF model correspond(on same test set)\")\n",
        "#     else:\n",
        "#         raise ValueError('Accuracy does not match target (Target: '+str(target_accuracy)+' but got '+str(accuracy)+' instead)')\n",
        "# else:\n",
        "#     print(\"TF model testing is disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERYDqiDVXUqX"
      },
      "source": [
        "# Quantization aware fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2QJCdx9HOfW"
      },
      "outputs": [],
      "source": [
        "# #################################################\n",
        "# PERFORM_QUANZATION_AWARE_TRAINING = False       #\n",
        "# #################################################\n",
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     pip_install('tensorflow_model_optimization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5W8KuwyXTsX"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     imported_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "#     import tensorflow_model_optimization as tfmot\n",
        "\n",
        "#     quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "#     # q_aware stands for for quantization aware.\n",
        "#     q_aware_model = None\n",
        "#     q_aware_model = quantize_model(imported_model)\n",
        "\n",
        "#     # `quantize_model` requires a recompile.\n",
        "#     _,loss_fn = define_model_architecture(len(CLASSES),_verbose = True)  # Get only the loss function\n",
        "#     compile_model(q_aware_model,optimizer,loss_fn,_verbose = True)  # Recompile the quantization aware model\n",
        "\n",
        "#     q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pb9pAqTA0h5"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     tb_dir = \"logs2/fit/\"\n",
        "#     %tensorboard --logdir $tb_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQPy1nbXtWto"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     finetuning_epochs = 50\n",
        "#     tensorboard_callback = start_tensorboard(tb_dir,None)\n",
        "\n",
        "#     q_history = q_aware_model.fit(X_train, y_train, epochs=finetuning_epochs, validation_data = (X_valid,y_valid),\n",
        "#                                 callbacks=[tensorboard_callback],\n",
        "#                                 batch_size=args['batchsize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIbbjlPRx1bx"
      },
      "outputs": [],
      "source": [
        "# if PERFORM_QUANZATION_AWARE_TRAINING:\n",
        "#     quant_model_path = MODELFOLDER + \"/\" + RUN_NAME + \"/quant_aware_model.tflite\"\n",
        "\n",
        "#     converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "#     converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "#     quantized_tflite_model = converter.convert()\n",
        "\n",
        "#     with tf.io.gfile.GFile(quant_model_path, 'wb') as f:\n",
        "#         f.write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVw_zjlf6uFi"
      },
      "source": [
        "## Rename current output folder by prefixing the accuracy value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    window_folder = '100ms'\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    window_folder = '14.67ms'\n",
        "else:\n",
        "    raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "# make window folder\n",
        "window_folder = os.path.join(os.path.dirname(run_dir),window_folder)\n",
        "if not os.path.exists(window_folder):\n",
        "    os.mkdir(window_folder)\n",
        "problem_folder = os.path.join(window_folder,classification_task.value[1])\n",
        "if not os.path.exists(problem_folder):\n",
        "    os.mkdir(problem_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzJLnKEZ6uFi"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(os.path.join(run_dir,'info.txt')):\n",
        "    metrics_prefix = 'c_' if 'CrossValidated' in os.path.basename(run_dir) else ''\n",
        "\n",
        "    metrics_prefix += 'maf1_%.4f_acc_%.4f_'%(round(metrics_to_save['macro avg f1-score'],4),round(metrics_to_save['average_fold_accuracy'],4))\n",
        "\n",
        "    window_prefix = ''\n",
        "    if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "        window_prefix = 'z100ms_'\n",
        "    elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)\n",
        "\n",
        "\n",
        "    problem_prefix = classification_task.value[1]+'_'\n",
        "        \n",
        "    newfoldername = os.path.join(problem_folder,window_prefix+problem_prefix+metrics_prefix+os.path.basename(run_dir))\n",
        "    # print('Renaming \"'+run_dir+'\" to \"'+newfoldername+'\"')\n",
        "    os.rename(run_dir,newfoldername)\n",
        "    run_dir = newfoldername\n",
        "else:\n",
        "    errfoldername = os.path.join(os.path.dirname(run_dir),'ERR_'+os.path.basename(run_dir))\n",
        "    os.rename(run_dir,errfoldername)\n",
        "    run_dir = errfoldername"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAgndl_b6uFi"
      },
      "source": [
        "## Testing with extra test data\n",
        "This data was extracted from extra recordings, made to test the system in a real life scenario.  \n",
        "Here we test only to veryfy that everything is working here, before making a shift to the real life test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if FEATURE_WINDOW_SIZE == FeatureWindowSize.s4800_SAMPLES_100ms:\n",
        "    TEST_WITH_EXTRA_DATA = False\n",
        "elif FEATURE_WINDOW_SIZE == FeatureWindowSize.s704_Samples_14ms:\n",
        "    TEST_WITH_EXTRA_DATA = True # Heep in mind that TRAIN_FINAL_MODEL should be True too\n",
        "else:\n",
        "    raise ValueError('Invalid FeatureWindowSize \"%s\"'%FeatureWindowSize.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdhiK3rD6uFj"
      },
      "outputs": [],
      "source": [
        "if TRAIN_FINAL_MODEL and TEST_WITH_EXTRA_DATA:\n",
        "    \"\"\" Load the test data \"\"\"\n",
        "\n",
        "    TEST_DATA_FILE_PATH = os.path.join(DATAFOLDER,'20221011_110715_test_onlycorrectdetections.pickle')\n",
        "    print(\"Loading test data from pickle...\")\n",
        "    with open(TEST_DATA_FILE_PATH,'rb') as pf:\n",
        "        testdataset = pickle.load(pf)\n",
        "    testdataset.sort_values(['meta_expressive_technique_id','meta_audiofilePath'],inplace = True)\n",
        "    print('Done.')\n",
        "    # If this fails, the dataset has changed from the last time the program was run successfully (CHECK THE DATA!!!)\n",
        "    assert testdataset.shape == (754,507)\n",
        "    # display(testdataset)\n",
        "\n",
        "\n",
        "    \"\"\" Drop unused features (like the train/test dataset) \"\"\"\n",
        "\n",
        "    drop_unused_features(testdataset,inplace=True)\n",
        "    assert testdataset.shape == (754,504)\n",
        "\n",
        "\n",
        "    \"\"\" Divide the test data into metadata, features and labels (like the train/test dataset) \"\"\"\n",
        "\n",
        "    test_metadata, test_features, test_labels = divide_dataset(testdataset)\n",
        "    assert test_metadata.shape[0] == test_features.shape[0] == test_labels.shape[0] == 754\n",
        "    assert test_metadata.shape[1] == 9\n",
        "    assert test_features.shape[1] == 495\n",
        "\n",
        "    \"\"\" Filter the dataset according to the task \"\"\"\n",
        "    # This might mean removing samples or renaming classes\n",
        "    test_features,test_labels,test_metadata = filter_dataset(test_features.copy(),test_labels.copy(),test_metadata.copy(),classification_task)\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" Apply the feature selection computed for the train/test set (like the train/test dataset) \"\"\"\n",
        "\n",
        "    test_features = test_features.copy().loc[:,selected_features]\n",
        "\n",
        "    if len(selected_features) != AUTO_FEATURE_NUMBER:\n",
        "        raise Exception('The number of selected_features ('+str(len(selected_features))+') is not the same as AUTO_FEATURE_NUMBER ('+str(AUTO_FEATURE_NUMBER)+'). Check the code.')\n",
        "\n",
        "    if test_features.shape[1] != AUTO_FEATURE_NUMBER:\n",
        "        raise Exception('The number of features in the test dataset ('+str(test_features.shape[1])+') is different from the number of features in the train/test dataset ('+str(AUTO_FEATURE_NUMBER)+')')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XskezstrklRC"
      },
      "outputs": [],
      "source": [
        "if TRAIN_FINAL_MODEL and TEST_WITH_EXTRA_DATA:\n",
        "    extra_test_x = test_features.to_numpy()\n",
        "    extra_test_y = test_labels.to_numpy()\n",
        "\n",
        "    \n",
        "\n",
        "    if DO_NORMALIZE_DATA:\n",
        "        # This scaler was \"learned\" a few cells above on the whole train/test dataset (obviously exluding the etra-test data)\n",
        "        extra_test_x = scaler.transform(extra_test_x)\n",
        "\n",
        "    if type(final_model.layers[0]) == tf.keras.layers.Conv1D:\n",
        "        extra_test_x = np.expand_dims(extra_test_x,axis = 2) # Adapt data for Conv1d ([batch_shape, steps, input_dim] -> in our case indim = 1, steps = features, batchshape = train datset size)\n",
        "\n",
        "    y_true = np.squeeze(extra_test_y)\n",
        "    y_pred = np.argmax(final_model(extra_test_x),axis=1)\n",
        "    cm_acc, f1mw, cm_conf_matrix, cm_classf_report, cm_printable_classf_report = compute_metrics(y_true, y_pred, _verbose=False)\n",
        "\n",
        "    with open(os.path.join(run_dir,'info.txt'),'a') as infof:\n",
        "        infof.write('______________________________________________________________________________________________________________________________________________________\\n\\n\\n')\n",
        "        infof.write('+----------------------------------------------------------------+\\n')\n",
        "        infof.write('| Results obtained on extra test recordings with the FINAL MODEL |\\n')\n",
        "        infof.write('+----------------------------------------------------------------+\\n\\n')\n",
        "        infof.write('Extra-test-Accuracy: '+str(cm_acc)+'\\n\\n')\n",
        "        infof.write('Extra-test-F1 Score (weighted average): '+str(f1mw)+'\\n\\n')\n",
        "        infof.write('Extra-test-ConfusionMatrix: \\n'+str(cm_conf_matrix)+'\\n\\n')\n",
        "        infof.write('Extra-test-Report: \\n'+str(cm_printable_classf_report)+'\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp5iv_0pklRC"
      },
      "outputs": [],
      "source": [
        "print('*--* Training successfully completed. *--*')\n",
        "print(\"Data at\",run_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def send_to_telegram(message):\n",
        "    apiToken = 'apiKeyHere'\n",
        "    chatID = 'chatIDHere'\n",
        "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
        "\n",
        "    try:\n",
        "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
        "        # print(response.text)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# send_to_telegram('Training completed in folder ' + run_dir)\n",
        "\n",
        "\n",
        "# check if 'best_notified_accyracy_yet.txt' exists\n",
        "# if it does, read the value, compare to the current and notify via telegram if the current is better\n",
        "# if it doesn't, create it and write the current value\n",
        "\n",
        "def save_and_notify_if_greater(newvalue, metricfilepath, metric_name):\n",
        "    BA_FILE = metricfilepath\n",
        "    if os.path.exists(BA_FILE):\n",
        "        with open(BA_FILE) as f:\n",
        "            best_notified_metric_yet = float(f.readline())\n",
        "    else:\n",
        "        best_notified_metric_yet = 0.0\n",
        "\n",
        "    if newvalue > best_notified_metric_yet:\n",
        "        message = str(round(newvalue,4)) + '\\nfrom run in folder ' + run_dir\n",
        "        with open(BA_FILE,'w') as f:\n",
        "            f.write(message)\n",
        "        send_to_telegram('New best '+str(metric_name)+': '+message)\n",
        "\n",
        "save_and_notify_if_greater(metrics_to_save['average_fold_accuracy'], os.path.join(MODELFOLDER,'best_accuracy_yet_notified.txt'), 'Accuracy')\n",
        "save_and_notify_if_greater(metrics_to_save['macro avg f1-score'], os.path.join(MODELFOLDER,'best_maf1_yet_notified.txt'), 'Macro Average F1-Score')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uLTufCRmlpIW",
        "4pBrSDphxyxL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-CLONE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "959cb910f4c92fed33bcbb2d615fe10bd0473a33e22a9b4e00ebf3acf1e03643"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
